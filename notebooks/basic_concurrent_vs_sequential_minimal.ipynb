{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ae533a",
   "metadata": {},
   "source": [
    "# WHISP: Concurrent vs Sequential Processing Test\n",
    "\n",
    "Minimal test notebook comparing concurrent (high-volume endpoint) vs sequential (standard endpoint) processing.\n",
    "\n",
    "## Quick Start\n",
    "1. Initialize Earth Engine\n",
    "2. Generate test data\n",
    "3. Run concurrent processing\n",
    "4. Run sequential processing\n",
    "5. Compare results\n",
    "6. Export results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a026280",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import gc\n",
    "import logging\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "\n",
    "# Reset and initialize Earth Engine\n",
    "ee.Reset()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "\n",
    "print(\"[OK] Earth Engine initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51603a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WHISP\n",
    "import openforis_whisp as whisp\n",
    "from openforis_whisp.advanced_stats import (\n",
    "    whisp_formatted_stats_geojson_to_df_concurrent,\n",
    "    whisp_formatted_stats_geojson_to_df_sequential,\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(\"whisp-concurrent\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "print(\"[OK] WHISP imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11457b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test parameters\n",
    "iso2_codes = ['br', 'co', 'ci']\n",
    "num_polygons = 200\n",
    "min_area_ha = 5\n",
    "max_area_ha = 10\n",
    "min_number_vert = 50\n",
    "max_number_vert = 100\n",
    "\n",
    "print(f\"[OK] Test parameters configured\")\n",
    "print(f\"     Polygons: {num_polygons}\")\n",
    "print(f\"     Area: {min_area_ha}-{max_area_ha} ha\")\n",
    "print(f\"     Vertices: {min_number_vert}-{max_number_vert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea04d9",
   "metadata": {},
   "source": [
    "## Part 2: Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b445446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test region (Brazil Amazon)\n",
    "state_geom = (ee.FeatureCollection(\"projects/sat-io/open-datasets/FAO/GAUL/GAUL_2024_L1\")\n",
    "    .filter(ee.Filter.inList('gaul1_name', ['Amazonas', 'Mato Grosso', 'Rondônia', 'Pará'])))\n",
    "bounds = state_geom.geometry().bounds()\n",
    "\n",
    "# Generate test GeoJSON\n",
    "with redirect_stdout(io.StringIO()):\n",
    "    random_geojson = whisp.generate_test_polygons(\n",
    "        bounds=bounds,\n",
    "        num_polygons=num_polygons,\n",
    "        min_area_ha=min_area_ha,\n",
    "        max_area_ha=max_area_ha,\n",
    "        min_number_vert=min_number_vert,\n",
    "        max_number_vert=max_number_vert\n",
    "    )\n",
    "\n",
    "# Save to temporary file\n",
    "temp_fd, test_geojson_path = tempfile.mkstemp(suffix='.geojson', text=True)\n",
    "os.close(temp_fd)\n",
    "with open(test_geojson_path, 'w') as f:\n",
    "    json.dump(random_geojson, f)\n",
    "\n",
    "print(f\"[OK] Generated test GeoJSON\")\n",
    "print(f\"     Features: {len(random_geojson['features'])}\")\n",
    "print(f\"     File: {test_geojson_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d01d29",
   "metadata": {},
   "source": [
    "## Part 3: Concurrent Processing (High-Volume Endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to high-volume endpoint\n",
    "ee.Reset()\n",
    "ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "\n",
    "api_url = str(ee.data._cloud_api_base_url)\n",
    "if 'highvolume' in api_url:\n",
    "    print(\"[OK] Using HIGH-VOLUME endpoint\")\n",
    "else:\n",
    "    print(\"[WARNING] Not using high-volume endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concurrent processing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 1: CONCURRENT PROCESSING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    df_concurrent = whisp_formatted_stats_geojson_to_df_concurrent(\n",
    "        input_geojson_filepath=test_geojson_path,\n",
    "        national_codes=iso2_codes,\n",
    "        batch_size=10,\n",
    "        max_concurrent=30,\n",
    "        validate_geometries=False,\n",
    "        add_metadata_server=False,\n",
    "        logger=logger,\n",
    "    )\n",
    "    \n",
    "    print(f\"[OK] Concurrent processing complete!\")\n",
    "    print(f\"     Rows: {df_concurrent.shape[0]}\")\n",
    "    print(f\"     Columns: {df_concurrent.shape[1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb26395",
   "metadata": {},
   "source": [
    "## Part 4: Sequential Processing (Standard Endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f83b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to standard endpoint\n",
    "ee.Reset()\n",
    "try:\n",
    "    ee.Initialize(opt_url='https://earthengine.googleapis.com')\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(opt_url='https://earthengine.googleapis.com')\n",
    "\n",
    "print(\"[OK] Using STANDARD endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ec060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequential processing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: SEQUENTIAL PROCESSING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    df_sequential = whisp_formatted_stats_geojson_to_df_sequential(\n",
    "        input_geojson_filepath=test_geojson_path,\n",
    "        national_codes=iso2_codes,\n",
    "        add_metadata_client_side=True,\n",
    "        logger=logger,\n",
    "    )\n",
    "    \n",
    "    print(f\"[OK] Sequential processing complete!\")\n",
    "    print(f\"     Rows: {df_sequential.shape[0]}\")\n",
    "    print(f\"     Columns: {df_sequential.shape[1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1f4f3",
   "metadata": {},
   "source": [
    "## Part 5: Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb126c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: Concurrent vs Sequential\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "if 'df_concurrent' in locals() and 'df_sequential' in locals():\n",
    "    print(f\"Concurrent shape:  {df_concurrent.shape}\")\n",
    "    print(f\"Sequential shape:  {df_sequential.shape}\")\n",
    "    \n",
    "    # Check columns match\n",
    "    if set(df_concurrent.columns) == set(df_sequential.columns):\n",
    "        print(\"\\n[OK] Column names match!\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] Column names differ\")\n",
    "        concurrent_only = set(df_concurrent.columns) - set(df_sequential.columns)\n",
    "        sequential_only = set(df_sequential.columns) - set(df_concurrent.columns)\n",
    "        if concurrent_only:\n",
    "            print(f\"  Only in concurrent: {concurrent_only}\")\n",
    "        if sequential_only:\n",
    "            print(f\"  Only in sequential: {sequential_only}\")\n",
    "else:\n",
    "    print(\"[ERROR] Both dataframes needed for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44736f4d",
   "metadata": {},
   "source": [
    "## Part 6: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directory\n",
    "out_directory = Path.home() / 'downloads' / 'whisp_results'\n",
    "out_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {out_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export concurrent results\n",
    "if 'df_concurrent' in locals():\n",
    "    csv_file = out_directory / 'concurrent_results.csv'\n",
    "    df_concurrent.to_csv(csv_file, index=False)\n",
    "    print(f\"[OK] Concurrent results exported: {csv_file}\")\n",
    "    print(f\"     Rows: {len(df_concurrent)}\")\n",
    "else:\n",
    "    print(\"[SKIP] Concurrent results not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export sequential results\n",
    "if 'df_sequential' in locals():\n",
    "    csv_file = out_directory / 'sequential_results.csv'\n",
    "    df_sequential.to_csv(csv_file, index=False)\n",
    "    print(f\"[OK] Sequential results exported: {csv_file}\")\n",
    "    print(f\"     Rows: {len(df_sequential)}\")\n",
    "else:\n",
    "    print(\"[SKIP] Sequential results not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as GeoJSON (if geometry column exists)\n",
    "if 'df_concurrent' in locals() and 'geo' in df_concurrent.columns:\n",
    "    geojson_file = out_directory / 'concurrent_results.geojson'\n",
    "    try:\n",
    "        whisp.convert_df_to_geojson(df_concurrent, geojson_file)\n",
    "        print(f\"[OK] Concurrent GeoJSON exported: {geojson_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Could not export GeoJSON: {e}\")\n",
    "else:\n",
    "    print(\"[INFO] No geometry column found (GeoJSON export skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30839df5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Concurrent**: Processes multiple batches in parallel using high-volume endpoint\n",
    "- **Sequential**: Processes one batch at a time using standard endpoint\n",
    "- **Results**: Both methods should produce identical data (same rows and columns)\n",
    "- **Use case**: Concurrent is faster for large datasets; sequential is useful for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c6e5e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
