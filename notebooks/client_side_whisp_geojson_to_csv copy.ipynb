{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250b53cc",
   "metadata": {},
   "source": [
    "### Whisp a feature collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed47318",
   "metadata": {},
   "source": [
    "Setup\n",
    "- NB use a virtual environment to avoid altering your python environment (https://docs.python.org/3/tutorial/venv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Engine and Common Libraries\n",
    "import ee\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    ee.Initialize(project='ee-andyarnellgee', opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project='ee-andyarnellgee', opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install openforis-whisp (uncomment line if not already installed)\n",
    "# !pip install --pre openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e60218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d9f78",
   "metadata": {},
   "source": [
    "Get a feature collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOJSON_EXAMPLE_FILEPATH = whisp.get_example_data_path(\"geojson_example.geojson\")\n",
    "\n",
    "print (GEOJSON_EXAMPLE_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "def setup_logging(log_level=logging.INFO):\n",
    "    \"\"\"Set up logging with consistent formatting.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    return logging.getLogger('whisp_processor')\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# //step 1: Load GeoJSON features and add unique IDs\n",
    "def convert_geojson_to_ee_bbox(geojson_filepath) -> ee.FeatureCollection:\n",
    "    \"\"\"\n",
    "    Reads a GeoJSON file, creates bounding boxes for each feature,\n",
    "    and converts to Earth Engine FeatureCollection.\n",
    "    \n",
    "    Args:\n",
    "        geojson_filepath (Any): The filepath to the GeoJSON file.\n",
    "        \n",
    "    Returns:\n",
    "        ee.FeatureCollection: Earth Engine FeatureCollection of bounding boxes.\n",
    "    \"\"\"\n",
    "    # import os\n",
    "    # from pathlib import Path\n",
    "    # import geopandas as gpd\n",
    "    # import ee\n",
    "    \n",
    "    # Read the GeoJSON file using geopandas\n",
    "    if isinstance(geojson_filepath, (str, Path)):\n",
    "        file_path = os.path.abspath(geojson_filepath)\n",
    "        print(f\"Reading GeoJSON file from: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load GeoJSON directly with geopandas\n",
    "            gdf = gpd.read_file(file_path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading GeoJSON file: {str(e)}\")\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a file path (str or Path)\")\n",
    "    \n",
    "    # Check if GeoDataFrame is empty\n",
    "    if len(gdf) == 0:\n",
    "        raise ValueError(\"GeoJSON contains no features\")\n",
    "    \n",
    "    # Add internal_id if not present\n",
    "    if 'internal_id' not in gdf.columns:\n",
    "        gdf['internal_id'] = range(1, len(gdf) + 1)\n",
    "    \n",
    "    # Create a new GeoDataFrame with bounding boxes\n",
    "    bbox_features = []\n",
    "    for idx, row in gdf.iterrows():\n",
    "        try:\n",
    "            # Get the bounds of the geometry (minx, miny, maxx, maxy)\n",
    "            minx, miny, maxx, maxy = row.geometry.bounds\n",
    "            \n",
    "            # Create an Earth Engine Rectangle geometry\n",
    "            ee_geometry = ee.Geometry.Rectangle([minx, miny, maxx, maxy])\n",
    "            \n",
    "            # Copy properties from the original feature\n",
    "            properties = {col: row[col] for col in gdf.columns if col != 'geometry'}\n",
    "            \n",
    "            # Convert numpy types to native Python types for proper serialization\n",
    "            for key, value in properties.items():\n",
    "                if hasattr(value, 'item'):  # Check if it's a numpy type\n",
    "                    properties[key] = value.item()  # Convert to Python native type\n",
    "                elif pd.isna(value):\n",
    "                    properties[key] = None\n",
    "            \n",
    "            # Create an Earth Engine feature with the bbox geometry\n",
    "            ee_feature = ee.Feature(ee_geometry, properties)\n",
    "            bbox_features.append(ee_feature)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature {idx}: {str(e)}\")\n",
    "    \n",
    "    # Check if any features were created\n",
    "    if not bbox_features:\n",
    "        raise ValueError(\"No valid features found in GeoJSON\")\n",
    "    \n",
    "    # Create the Earth Engine FeatureCollection\n",
    "    feature_collection = ee.FeatureCollection(bbox_features)\n",
    "    print(f\"Created Earth Engine FeatureCollection with {len(bbox_features)} bounding box features\")\n",
    "    \n",
    "    return feature_collection\n",
    "\n",
    "# Download GeoTIFF for feature using Earth Engine\n",
    "def download_geotiff_for_feature(ee_feature, image, output_dir, scale=10, max_retries=3, retry_delay=5):\n",
    "    \"\"\"\n",
    "    Download a GeoTIFF for a specific Earth Engine feature by clipping the image.\n",
    "    \n",
    "    Args:\n",
    "        ee_feature: Earth Engine feature to clip the image to\n",
    "        image: Earth Engine image to download (e.g., whisp.combine_datasets())\n",
    "        output_dir: Directory to save the GeoTIFF\n",
    "        scale: Resolution in meters (default 10m)\n",
    "        max_retries: Maximum number of retry attempts for download\n",
    "        retry_delay: Seconds to wait between retries\n",
    "        \n",
    "    Returns:\n",
    "        output_path: Path to the downloaded GeoTIFF file\n",
    "    \"\"\"\n",
    "    # Get the feature ID\n",
    "    try:\n",
    "        internal_id = ee_feature.get('internal_id').getInfo()\n",
    "        logger.info(f\"Downloading GeoTIFF for feature {internal_id}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting internal_id from feature: {str(e)}\")\n",
    "        internal_id = f\"unknown_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Create a unique filename\n",
    "    filename = f\"feature_{internal_id}.tif\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    # If file already exists, don't re-download\n",
    "    if output_path.exists():\n",
    "        logger.info(f\"File {filename} already exists, skipping download\")\n",
    "        return output_path\n",
    "    \n",
    "    # Track retries\n",
    "    retries = 0\n",
    "    \n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Clip the image to the feature\n",
    "            clipped_image = image.clip(ee_feature.geometry())\n",
    "            \n",
    "            # Generate the download URL with timeout handling\n",
    "            logger.debug(f\"Generating download URL for feature {internal_id}\")\n",
    "            start_time = time.time()\n",
    "            download_url = clipped_image.getDownloadURL({\n",
    "                'format': 'GeoTIFF',  # Note: Earth Engine accepts 'GeoTIFF' \n",
    "                'region': ee_feature.geometry(),\n",
    "                'scale': scale,\n",
    "                'crs': 'EPSG:4326'\n",
    "            })\n",
    "            url_time = time.time() - start_time\n",
    "            logger.debug(f\"URL generated in {url_time:.2f}s: {download_url[:80]}...\")\n",
    "            \n",
    "            # Download the image with timeout\n",
    "            logger.info(f\"Downloading to {output_path}\")\n",
    "            response = requests.get(download_url, timeout=300)  # 5-minute timeout\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Check if the response is actually a GeoTIFF\n",
    "                content_type = response.headers.get('Content-Type', '')\n",
    "                if 'tiff' in content_type.lower() or 'zip' in content_type.lower():\n",
    "                    with open(output_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    logger.info(f\"Successfully downloaded {filename}\")\n",
    "                    return output_path\n",
    "                else:\n",
    "                    # Log error if the response isn't a GeoTIFF\n",
    "                    logger.error(f\"Download returned non-TIFF content: {content_type}\")\n",
    "                    # Save the response for debugging\n",
    "                    error_file = output_dir / f\"error_{internal_id}.txt\"\n",
    "                    with open(error_file, 'wb') as f:\n",
    "                        f.write(response.content[:2000])  # Save first part for debugging\n",
    "                    logger.error(f\"Saved error content to {error_file}\")\n",
    "                    retries += 1\n",
    "            else:\n",
    "                logger.error(f\"Failed to download (status {response.status_code}): {response.text[:200]}\")\n",
    "                retries += 1\n",
    "                \n",
    "            # Wait before retrying\n",
    "            if retries < max_retries:\n",
    "                sleep_time = retry_delay * (2 ** retries)  # Exponential backoff\n",
    "                logger.info(f\"Retrying in {sleep_time} seconds (attempt {retries+1}/{max_retries})\")\n",
    "                time.sleep(sleep_time)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading feature {internal_id}: {str(e)}\", exc_info=True)\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                logger.info(f\"Retrying in {retry_delay} seconds (attempt {retries+1}/{max_retries})\")\n",
    "                time.sleep(retry_delay)\n",
    "    \n",
    "    logger.error(f\"Maximum retries reached for feature {internal_id}\")\n",
    "    return None\n",
    "\n",
    "def download_geotiffs_for_feature_collection(feature_collection, image, output_dir=None, \n",
    "                                            scale=10, max_features=None, max_workers=None,\n",
    "                                            max_retries=3, retry_delay=5):\n",
    "    \"\"\"\n",
    "    Download GeoTIFFs for an entire Earth Engine FeatureCollection, with parallel processing option.\n",
    "    \n",
    "    Args:\n",
    "        feature_collection: Earth Engine FeatureCollection to process\n",
    "        image: Earth Engine image to clip and download\n",
    "        output_dir: Directory to save the GeoTIFFs (default: ~/Downloads/whisp_features)\n",
    "        scale: Resolution in meters (default 10m)\n",
    "        max_features: Maximum number of features to process (default: all)\n",
    "        max_workers: Maximum number of parallel workers (default: None, sequential processing)\n",
    "        max_retries: Maximum number of retry attempts for each download\n",
    "        retry_delay: Base delay in seconds between retries (uses exponential backoff)\n",
    "        \n",
    "    Returns:\n",
    "        List of paths to successfully downloaded GeoTIFF files\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import concurrent.futures\n",
    "    from pathlib import Path\n",
    "    import ee\n",
    "    \n",
    "    logger = logging.getLogger('whisp_processor')\n",
    "    \n",
    "    # Set default output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = Path.home() / 'Downloads' / 'whisp_features'\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Get collection size and limit if needed\n",
    "    collection_size = feature_collection.size().getInfo()\n",
    "    logger.info(f\"Processing Earth Engine FeatureCollection with {collection_size} features\")\n",
    "    \n",
    "    if max_features and max_features < collection_size:\n",
    "        feature_collection = feature_collection.limit(max_features)\n",
    "        collection_size = max_features\n",
    "        logger.info(f\"Limited to processing first {max_features} features\")\n",
    "    \n",
    "    # Get features as a list\n",
    "    features = feature_collection.toList(collection_size)\n",
    "    \n",
    "    # Create a function to download a single feature given its index\n",
    "    def download_feature(index):\n",
    "        try:\n",
    "            ee_feature = ee.Feature(features.get(index))\n",
    "            return download_geotiff_for_feature(\n",
    "                ee_feature=ee_feature,\n",
    "                image=image,\n",
    "                output_dir=output_dir,\n",
    "                scale=scale,\n",
    "                max_retries=max_retries,\n",
    "                retry_delay=retry_delay\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing feature at index {index}: {str(e)}\", exc_info=True)\n",
    "            return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Parallel processing if max_workers is specified and > 1\n",
    "    if max_workers and max_workers > 1:\n",
    "        logger.info(f\"Using parallel processing with {max_workers} workers\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_index = {\n",
    "                executor.submit(download_feature, i): i \n",
    "                for i in range(collection_size)\n",
    "            }\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in concurrent.futures.as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    path = future.result()\n",
    "                    if path:\n",
    "                        results.append(path)\n",
    "                        logger.info(f\"Completed feature {index+1}/{collection_size}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Failed to download feature {index+1}/{collection_size}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Exception occurred while processing feature {index+1}: {str(e)}\")\n",
    "    else:\n",
    "        # Sequential processing\n",
    "        logger.info(\"Processing features sequentially\")\n",
    "        for i in range(collection_size):\n",
    "            logger.info(f\"Processing feature {i+1}/{collection_size}\")\n",
    "            path = download_feature(i)\n",
    "            if path:\n",
    "                results.append(path)\n",
    "    \n",
    "    logger.info(f\"Completed downloading {len(results)}/{collection_size} features successfully\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f86b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_bbox_collection = convert_geojson_to_ee_bbox(GEOJSON_EXAMPLE_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625dd78",
   "metadata": {},
   "source": [
    "Functions for further obscuring ee features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbc5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extend_bbox(minx, miny, maxx, maxy, extension_distance=None, extension_range=None):\n",
    "    \"\"\"\n",
    "    Extends a bounding box by a fixed distance or a random distance within a range.\n",
    "    \n",
    "    Args:\n",
    "        minx, miny, maxx, maxy: The original bounding box coordinates\n",
    "        extension_distance: Fixed distance to extend in all directions\n",
    "        extension_range: List [min_dist, max_dist] for random extension\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (minx, miny, maxx, maxy) for the extended bounding box\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    if extension_distance is None and extension_range is None:\n",
    "        return minx, miny, maxx, maxy\n",
    "    \n",
    "    # Determine the extension distance\n",
    "    if extension_range is not None:\n",
    "        min_dist, max_dist = extension_range\n",
    "        dist = random.uniform(min_dist, max_dist)\n",
    "    else:\n",
    "        dist = extension_distance\n",
    "    \n",
    "    # Extend the bounding box\n",
    "    extended_minx = minx - dist\n",
    "    extended_miny = miny - dist\n",
    "    extended_maxx = maxx + dist\n",
    "    extended_maxy = maxy + dist\n",
    "    \n",
    "    return extended_minx, extended_miny, extended_maxx, extended_maxy\n",
    "\n",
    "\n",
    "def shift_bbox(minx, miny, maxx, maxy, max_shift_distance, pixel_length=0.0001):\n",
    "    \"\"\"\n",
    "    Shifts a bounding box in a random direction within max_shift_distance.\n",
    "    \n",
    "    Args:\n",
    "        minx, miny, maxx, maxy: The bounding box coordinates\n",
    "        max_shift_distance: Maximum distance to shift\n",
    "        pixel_length: Length of a pixel to avoid accuracy loss\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (minx, miny, maxx, maxy) for the shifted bounding box\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import math\n",
    "    \n",
    "    if max_shift_distance <= 0:\n",
    "        return minx, miny, maxx, maxy\n",
    "    \n",
    "    # Calculate the effective max shift (max_shift - pixel_length)\n",
    "    effective_max_shift = max(0, max_shift_distance - pixel_length)\n",
    "    \n",
    "    # Random shift distance (less than effective_max_shift)\n",
    "    shift_distance = random.uniform(0, effective_max_shift)\n",
    "    \n",
    "    # Random angle in radians\n",
    "    angle = random.uniform(0, 2 * math.pi)\n",
    "    \n",
    "    # Calculate shift components\n",
    "    dx = shift_distance * math.cos(angle)\n",
    "    dy = shift_distance * math.sin(angle)\n",
    "    \n",
    "    # Apply shift\n",
    "    shifted_minx = minx + dx\n",
    "    shifted_miny = miny + dy\n",
    "    shifted_maxx = maxx + dx\n",
    "    shifted_maxy = maxy + dy\n",
    "    \n",
    "    return shifted_minx, shifted_miny, shifted_maxx, shifted_maxy\n",
    "\n",
    "def generate_random_geometries(gdf, max_distance, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Generates random geometries near the original features in a GeoDataFrame.\n",
    "    Each random geometry is placed within the specified distance of a randomly selected\n",
    "    existing feature, rather than anywhere within the overall extent.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame with the original features\n",
    "        max_distance: Maximum distance from original features\n",
    "        proportion: Proportion of extra geometries to create (relative to original count)\n",
    "        \n",
    "    Returns:\n",
    "        List of Earth Engine features with random geometries\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import ee\n",
    "    import pandas as pd\n",
    "    import math\n",
    "    \n",
    "    if proportion <= 0 or max_distance <= 0 or len(gdf) == 0:\n",
    "        return []\n",
    "    \n",
    "    random_features = []\n",
    "    \n",
    "    # Get the dimensions and centroids from original features\n",
    "    feature_info = []\n",
    "    \n",
    "    for idx, row in gdf.iterrows():\n",
    "        minx, miny, maxx, maxy = row.geometry.bounds\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        centroid_x = (minx + maxx) / 2\n",
    "        centroid_y = (miny + maxy) / 2\n",
    "        feature_info.append({\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'center_x': centroid_x,\n",
    "            'center_y': centroid_y,\n",
    "            'bounds': (minx, miny, maxx, maxy)\n",
    "        })\n",
    "    \n",
    "    # Calculate number of random features to create\n",
    "    num_random_features = max(1, int(len(gdf) * proportion))\n",
    "    \n",
    "    # Generate random features\n",
    "    for i in range(num_random_features):\n",
    "        # Select a random original feature to be near\n",
    "        random_feature_idx = random.randint(0, len(feature_info)-1)\n",
    "        selected_feature = feature_info[random_feature_idx]\n",
    "        \n",
    "        # Get the original feature's dimensions\n",
    "        width = selected_feature['width']\n",
    "        height = selected_feature['height']\n",
    "        orig_x = selected_feature['center_x']\n",
    "        orig_y = selected_feature['center_y']\n",
    "        \n",
    "        # Add some variation to dimensions (± 20%)\n",
    "        width_variation = random.uniform(0.8, 1.2)\n",
    "        height_variation = random.uniform(0.8, 1.2)\n",
    "        width *= width_variation\n",
    "        height *= height_variation\n",
    "        \n",
    "        # Generate a random position within max_distance of the selected feature\n",
    "        # First, select a random angle\n",
    "        angle = random.uniform(0, 2 * math.pi)\n",
    "        \n",
    "        # Then, select a random distance (within max_distance)\n",
    "        distance = random.uniform(0, max_distance)\n",
    "        \n",
    "        # Calculate the new center point\n",
    "        center_x = orig_x + (distance * math.cos(angle))\n",
    "        center_y = orig_y + (distance * math.sin(angle))\n",
    "        \n",
    "        # Calculate corners for the random rectangle\n",
    "        r_minx = center_x - (width / 2)\n",
    "        r_miny = center_y - (height / 2)\n",
    "        r_maxx = center_x + (width / 2)\n",
    "        r_maxy = center_y + (height / 2)\n",
    "        \n",
    "        # Create Earth Engine Rectangle geometry\n",
    "        ee_geometry = ee.Geometry.Rectangle([r_minx, r_miny, r_maxx, r_maxy])\n",
    "        \n",
    "        # Create random properties\n",
    "        properties = {\n",
    "            'random_feature': True, \n",
    "            'internal_id': f'random_{i + 1000}',  # Use high numbers to avoid conflicts\n",
    "            'obscured': True,\n",
    "            'near_feature_id': random_feature_idx + 1  # Store which feature it's near\n",
    "        }\n",
    "        \n",
    "        # Create an Earth Engine feature\n",
    "        ee_feature = ee.Feature(ee_geometry, properties)\n",
    "        random_features.append(ee_feature)\n",
    "    \n",
    "    return random_features\n",
    "\n",
    "def convert_geojson_to_ee_bbox_obscured(\n",
    "    geojson_filepath, \n",
    "    extension_distance=None, \n",
    "    extension_range=None,\n",
    "    shift_geometries=False,\n",
    "    shift_proportion=0.5,  # NEW: Control how much of extension is used for shifting\n",
    "    pixel_length=0.0001,\n",
    "    add_random_features=False,\n",
    "    max_distance=0.1,\n",
    "    random_proportion=0.5\n",
    ") -> ee.FeatureCollection:\n",
    "    \"\"\"\n",
    "    Reads a GeoJSON file, creates bounding boxes for each feature,\n",
    "    and converts to Earth Engine FeatureCollection with options to obscure locations.\n",
    "    \n",
    "    Args:\n",
    "        geojson_filepath (str or Path): The filepath to the GeoJSON file\n",
    "        extension_distance (float): Fixed distance to extend bounding boxes\n",
    "        extension_range (list): [min_dist, max_dist] for random extension\n",
    "        shift_geometries (bool): Whether to shift bounding boxes randomly\n",
    "        shift_proportion (float): How much of extension can be used for shifting (0-1, default 0.5)\n",
    "        pixel_length (float): Length of a pixel to avoid accuracy loss\n",
    "        add_random_features (bool): Whether to add random decoy features\n",
    "        max_distance (float): Maximum distance for random features\n",
    "        random_proportion (float): Proportion of random features to add\n",
    "        \n",
    "    Returns:\n",
    "        ee.FeatureCollection: Earth Engine FeatureCollection of bounding boxes\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import geopandas as gpd\n",
    "    import ee\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Read the GeoJSON file using geopandas\n",
    "    if isinstance(geojson_filepath, (str, Path)):\n",
    "        file_path = os.path.abspath(geojson_filepath)\n",
    "        print(f\"Reading GeoJSON file from: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load GeoJSON directly with geopandas\n",
    "            gdf = gpd.read_file(file_path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading GeoJSON file: {str(e)}\")\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a file path (str or Path)\")\n",
    "    \n",
    "    # Check if GeoDataFrame is empty\n",
    "    if len(gdf) == 0:\n",
    "        raise ValueError(\"GeoJSON contains no features\")\n",
    "    \n",
    "    # Add internal_id if not present\n",
    "    if 'internal_id' not in gdf.columns:\n",
    "        gdf['internal_id'] = range(1, len(gdf) + 1)\n",
    "    \n",
    "    # Create a new list with bounding boxes\n",
    "    bbox_features = []\n",
    "    \n",
    "    # Validate shift_proportion to be between 0 and 1\n",
    "    shift_proportion = max(0, min(1, shift_proportion))\n",
    "    \n",
    "    for idx, row in gdf.iterrows():\n",
    "        try:\n",
    "            # Get the bounds of the geometry (minx, miny, maxx, maxy)\n",
    "            minx, miny, maxx, maxy = row.geometry.bounds\n",
    "            \n",
    "            # Apply bounding box extension if requested\n",
    "            if extension_distance is not None or extension_range is not None:\n",
    "                minx, miny, maxx, maxy = extend_bbox(\n",
    "                    minx, miny, maxx, maxy, \n",
    "                    extension_distance=extension_distance,\n",
    "                    extension_range=extension_range\n",
    "                )\n",
    "            \n",
    "            # Apply random shift if requested\n",
    "            if shift_geometries:\n",
    "                # Determine max shift distance - limit by shift_proportion\n",
    "                max_shift = 0\n",
    "                if extension_distance is not None:\n",
    "                    max_shift = extension_distance * shift_proportion\n",
    "                elif extension_range is not None:\n",
    "                    max_shift = extension_range[1] * shift_proportion  # Use max of range\n",
    "                \n",
    "                if max_shift > 0:\n",
    "                    minx, miny, maxx, maxy = shift_bbox(\n",
    "                        minx, miny, maxx, maxy, \n",
    "                        max_shift, pixel_length\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"Warning: No shifting applied to feature {idx} due to missing extension parameters\")\n",
    "            \n",
    "            # Create an Earth Engine Rectangle geometry\n",
    "            ee_geometry = ee.Geometry.Rectangle([minx, miny, maxx, maxy])\n",
    "            \n",
    "            # Copy properties from the original feature\n",
    "            properties = {col: row[col] for col in gdf.columns if col != 'geometry'}\n",
    "            \n",
    "            # Convert numpy types to native Python types for proper serialization\n",
    "            for key, value in properties.items():\n",
    "                if hasattr(value, 'item'):  # Check if it's a numpy type\n",
    "                    properties[key] = value.item()  # Convert to Python native type\n",
    "                elif pd.isna(value):\n",
    "                    properties[key] = None\n",
    "            \n",
    "            # Create an Earth Engine feature with the bbox geometry\n",
    "            ee_feature = ee.Feature(ee_geometry, properties)\n",
    "            bbox_features.append(ee_feature)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature {idx}: {str(e)}\")\n",
    "    \n",
    "    # Check if any features were created\n",
    "    if not bbox_features:\n",
    "        raise ValueError(\"No valid features found in GeoJSON\")\n",
    "    \n",
    "    # Add random decoy features if requested\n",
    "    if add_random_features:\n",
    "        random_features = generate_random_geometries(\n",
    "            gdf, max_distance, random_proportion\n",
    "        )\n",
    "        \n",
    "        if random_features:\n",
    "            bbox_features.extend(random_features)\n",
    "            print(f\"Added {len(random_features)} random decoy features to obscure real locations\")\n",
    "    \n",
    "    # Create the Earth Engine FeatureCollection\n",
    "    feature_collection = ee.FeatureCollection(bbox_features)\n",
    "    print(f\"Created Earth Engine FeatureCollection with {len(bbox_features)} bounding box features\")\n",
    "    \n",
    "    return feature_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829fd41",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_collection = whisp.convert_geojson_to_ee(\n",
    " GEOJSON_EXAMPLE_FILEPATH\n",
    ")\n",
    "\n",
    "# Example 1: Just extend bounding boxes by fixed distance\n",
    "extended_collection = convert_geojson_to_ee_bbox_obscured(\n",
    "    GEOJSON_EXAMPLE_FILEPATH,\n",
    "    extension_distance=0.001  # where 0.001 degrees is ~100m at equator\n",
    ")\n",
    "\n",
    "# Example 2: Use random extension distances for each feature\n",
    "random_extended_collection = convert_geojson_to_ee_bbox_obscured(\n",
    "    GEOJSON_EXAMPLE_FILEPATH,\n",
    "    extension_range=[0.0005, 0.001]  # Random extension (0.001 degrees is ~100m at equator)\n",
    ")\n",
    "\n",
    "# Example 3: Extend and shift geometries\n",
    "shifted_collection = convert_geojson_to_ee_bbox_obscured(\n",
    "    GEOJSON_EXAMPLE_FILEPATH,\n",
    "    extension_distance=0.001,\n",
    "    shift_geometries=True,\n",
    "    shift_proportion=0.5,  # Use X% of the extension distance for shifting\n",
    "    pixel_length=0.0001  # ~10m at equator\n",
    ")\n",
    "\n",
    "# Example 4: add random features\n",
    "random_extras_collection = convert_geojson_to_ee_bbox_obscured(\n",
    "    GEOJSON_EXAMPLE_FILEPATH,\n",
    "    add_random_features=True,\n",
    "    random_proportion=0.1  # Add X% more features as decoys\n",
    ")\n",
    "\n",
    "# Example 5: Full obscuration - extend, shift, and add random features\n",
    "fully_obscured_collection = convert_geojson_to_ee_bbox_obscured(\n",
    "    GEOJSON_EXAMPLE_FILEPATH,\n",
    "    extension_range=[0.002, 0.003],\n",
    "    shift_geometries=True,\n",
    "    shift_proportion=0.9,\n",
    "    pixel_length=0.0001,  # ~10m at equator\n",
    "    add_random_features=True,\n",
    "    max_distance=0.05,  # xkm at equator\n",
    "    random_proportion= 1  # Add X more features as decoys\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6282e",
   "metadata": {},
   "source": [
    "View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "\n",
    "map = geemap.Map()\n",
    "# map.addLayer(extended_collection, {}, \"Extended Collection\")\n",
    "# map.addLayer(random_extended_collection, {}, \"Random Extended Collection\")\n",
    "# map.addLayer(shifted_collection, {}, \"Shifted Collection\")\n",
    "# map.addLayer(random_extras_collection, {}, \"Random Extras Collection\")\n",
    "map.addLayer(fully_obscured_collection, {}, \"Fully Obscured Collection\")\n",
    "# map.addLayer (ee_bbox_collection, {}, \"Original bbox Collection\")\n",
    "map.addLayer (ee_collection, {}, \"Original Collection\")\n",
    "map.centerObject(ee_bbox_collection.first(), 10)\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequential processing\n",
    "# geotiff_paths = download_geotiffs_for_feature_collection(\n",
    "#     feature_collection=ee_bbox_collection,\n",
    "#     image=whisp.combine_datasets(),\n",
    "#     output_dir=Path.home() / 'whisp_outputs',\n",
    "#     max_features=10\n",
    "# )\n",
    "\n",
    "# # Parallel processing (faster for many features)\n",
    "# geotiff_paths = download_geotiffs_for_feature_collection(\n",
    "#     feature_collection=ee_bbox_collection,\n",
    "#     # feature_collection=fully_obscured_collection,\n",
    "#     image=whisp.combine_datasets(),\n",
    "#     max_features=100,\n",
    "#     max_workers=40  # Process X features concurrently\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cb4e1",
   "metadata": {},
   "source": [
    "Sequential stats (client side using exact extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geotiff_stats_by_feature_id(\n",
    "    geojson_path, \n",
    "    tiff_dir=None, \n",
    "    output_csv=None, \n",
    "    tiff_id_pattern=r'feature_(\\d+)\\.tif', \n",
    "    id_column='internal_id',\n",
    "    ops=['sum'],\n",
    "    max_features=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Process GeoTIFF files that match feature IDs in a GeoJSON, run exactextract,\n",
    "    and save the results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        geojson_path (str or Path): Path to the GeoJSON file with features\n",
    "        tiff_dir (str or Path): Directory containing GeoTIFF files (default: ~/Downloads/whisp_features)\n",
    "        output_csv (str or Path): Path to save the output CSV (default: uses timestamp)\n",
    "        tiff_id_pattern (str): Regex pattern to extract ID from GeoTIFF filename\n",
    "        id_column (str): Column name in GeoJSON containing feature IDs\n",
    "        ops (list): List of operations to perform with exactextract\n",
    "        max_features (int): Maximum number of features to process\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined results DataFrame\n",
    "        str: Path to the output CSV file\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    import logging\n",
    "    from exactextract import exact_extract\n",
    "    \n",
    "    logger = logging.getLogger('whisp_processor')\n",
    "    \n",
    "    # Set default directory if not specified\n",
    "    if tiff_dir is None:\n",
    "        tiff_dir = Path.home() / 'Downloads' / 'whisp_features'\n",
    "    else:\n",
    "        tiff_dir = Path(tiff_dir)\n",
    "    \n",
    "    # Load the GeoJSON\n",
    "    logger.info(f\"Loading GeoJSON from {geojson_path}\")\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    \n",
    "    # Ensure ID column exists\n",
    "    if id_column not in gdf.columns:\n",
    "        logger.warning(f\"ID column '{id_column}' not found in GeoJSON. Adding sequential IDs.\")\n",
    "        gdf[id_column] = range(1, len(gdf) + 1)\n",
    "    \n",
    "    # Apply max_features if specified\n",
    "    if max_features and max_features < len(gdf):\n",
    "        logger.info(f\"Limiting to first {max_features} features\")\n",
    "        gdf = gdf.iloc[:max_features]\n",
    "    \n",
    "    # Find all GeoTIFF files in the directory\n",
    "    tiff_files = []\n",
    "    for file in os.listdir(tiff_dir):\n",
    "        if file.endswith('.tif') or file.endswith('.tiff'):\n",
    "            tiff_files.append(file)\n",
    "    \n",
    "    logger.info(f\"Found {len(tiff_files)} GeoTIFF files in {tiff_dir}\")\n",
    "    \n",
    "    # Set up output CSV\n",
    "    if output_csv is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_csv = Path(tiff_dir) / f\"feature_stats_{timestamp}.csv\"\n",
    "    else:\n",
    "        output_csv = Path(output_csv)\n",
    "    \n",
    "    # Create empty results DataFrame\n",
    "    all_results = pd.DataFrame()\n",
    "    id_pattern = re.compile(tiff_id_pattern)\n",
    "    \n",
    "    # Track processed features for reporting\n",
    "    processed_count = 0\n",
    "    matched_count = 0\n",
    "    \n",
    "    # Process each GeoTIFF file\n",
    "    for tiff_file in tiff_files:\n",
    "        # Extract ID from filename using regex\n",
    "        match = id_pattern.search(tiff_file)\n",
    "        if not match:\n",
    "            logger.debug(f\"Could not extract ID from filename: {tiff_file}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        tiff_id = int(match.group(1))\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Find matching feature in GeoJSON\n",
    "        matching_feature = gdf[gdf[id_column] == tiff_id]\n",
    "        if len(matching_feature) == 0:\n",
    "            logger.debug(f\"No matching feature found for ID {tiff_id}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        matched_count += 1\n",
    "        logger.info(f\"Processing feature ID: {tiff_id} ({matched_count} of {processed_count} matched)\")\n",
    "        \n",
    "        # Full path to GeoTIFF file\n",
    "        tiff_path = tiff_dir / tiff_file\n",
    "        \n",
    "        try:\n",
    "            # Run exactextract\n",
    "            logger.debug(f\"Running exactextract on {tiff_file}\")\n",
    "            stats = exact_extract(\n",
    "                rast=str(tiff_path),\n",
    "                vec=matching_feature,\n",
    "                ops=ops,\n",
    "                output='pandas',\n",
    "                include_cols=[id_column]\n",
    "            )\n",
    "            \n",
    "            # Add the geometry column to the results\n",
    "            stats['geometry'] = matching_feature.iloc[0].geometry\n",
    "            \n",
    "            # Append to results\n",
    "            if all_results.empty:\n",
    "                all_results = stats\n",
    "                # Write header to CSV\n",
    "                stats.to_csv(output_csv, index=False)\n",
    "            else:\n",
    "                all_results = pd.concat([all_results, stats], ignore_index=True)\n",
    "                # Append to CSV without header\n",
    "                stats.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "            \n",
    "            logger.info(f\"Feature {tiff_id} processed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing feature {tiff_id}: {str(e)}\")\n",
    "    \n",
    "    # Convert to GeoDataFrame for spatial analysis\n",
    "    if not all_results.empty:\n",
    "        try:\n",
    "            result_gdf = gpd.GeoDataFrame(all_results, geometry='geometry')\n",
    "            if gdf.crs:\n",
    "                result_gdf = result_gdf.set_crs(gdf.crs)\n",
    "                \n",
    "            logger.info(f\"Processed {matched_count}/{processed_count} GeoTIFF files with matching features\")\n",
    "            logger.info(f\"Results saved to {output_csv}\")\n",
    "            return result_gdf, str(output_csv)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating GeoDataFrame from results: {str(e)}\")\n",
    "    \n",
    "    if all_results.empty:\n",
    "        logger.warning(\"No results generated\")\n",
    "    \n",
    "    return all_results, str(output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf26224",
   "metadata": {},
   "source": [
    "simple approach for single raster or list of rasters and all features in geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea430a",
   "metadata": {},
   "source": [
    "list tif files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c90d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T115601Z-001'\n",
    "\n",
    "# Using Path from pathlib (more modern approach)\n",
    "print(\"\\n=== TIFF files using pathlib ===\")\n",
    "folder = Path(folder_path)\n",
    "tif_files = list(folder.glob('*.tif')) + list(folder.glob('*.tiff'))\n",
    "\n",
    "if tif_files:\n",
    "    for i, file_path in enumerate(tif_files, 1):\n",
    "        file_size = file_path.stat().st_size / (1024 * 1024)  # Convert to MB\n",
    "        print(f\"{i}. {file_path.name} - {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"No TIFF files found in the directory\")\n",
    "\n",
    "print(f\"\\nTotal: {len(tif_files)} TIFF files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b1e92",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import os\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    import logging\n",
    "    from exactextract import exact_extract\n",
    "    import concurrent.futures\n",
    "    import threading\n",
    "    import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a22bd",
   "metadata": {},
   "source": [
    "try avoiding gdal using rio_vrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fcef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rio-vrt\n",
    "from rio_vrt import build_vrt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a25a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the folder containing your GeoTIFFs\n",
    "folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T115601Z-001_2'\n",
    "folder = Path(folder_path)\n",
    "\n",
    "# Get list of TIFF files\n",
    "tif_files = list(folder.glob('*.tif')) + list(folder.glob('*.tiff'))\n",
    "tif_file_paths = [str(file) for file in tif_files]\n",
    "\n",
    "# Output VRT path\n",
    "output_vrt = str(folder / \"combined_rasters.vrt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the VRT file\n",
    "# vrt_file = build_vrt(output_vrt, tif_file_paths)\n",
    "\n",
    "# print(f\"VRT file created at: {output_vrt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842493fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_to_16bit(input_path, output_path, signed=True):\n",
    "    \"\"\"\n",
    "    Convert a raster to 16-bit (int16 or uint16)\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input raster\n",
    "        output_path: Path to save the output raster\n",
    "        signed: Whether to use signed (int16) or unsigned (uint16) data type\n",
    "    \"\"\"\n",
    "    dtype = 'int16' if signed else 'uint16'\n",
    "    \n",
    "    with rasterio.open(input_path) as src:\n",
    "        # Read source metadata\n",
    "        meta = src.meta.copy()\n",
    "        \n",
    "        # Read data\n",
    "        data = src.read()\n",
    "        \n",
    "        # Determine scaling if needed (depends on your data values)\n",
    "        if data.min() < 0 and not signed:\n",
    "            print(\"Warning: Negative values found but converting to unsigned int16\")\n",
    "            # You might need to add an offset or rescale\n",
    "        \n",
    "        # Update metadata with new data type\n",
    "        meta.update({\n",
    "            'dtype': dtype,\n",
    "            'driver': 'GTiff',\n",
    "            'compress': 'lzw'  # Optional compression\n",
    "        })\n",
    "        \n",
    "        # Convert and write data\n",
    "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "            # Convert data to new dtype (with appropriate scaling if needed)\n",
    "            dst.write(data.astype(dtype))\n",
    "            \n",
    "    print(f\"Converted {input_path} to 16-bit ({dtype}) at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8139e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_convert_to_16bit(folder_path, output_folder=None, signed=True):\n",
    "    \"\"\"\n",
    "    Convert all rasters in a folder to 16-bit\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path containing raster files\n",
    "        output_folder: Where to save the outputs (defaults to subfolder \"16bit\")\n",
    "        signed: Whether to use signed (int16) or unsigned (uint16) data type\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Set output folder\n",
    "    if output_folder is None:\n",
    "        output_folder = folder / \"16bit\"\n",
    "    else:\n",
    "        output_folder = Path(output_folder)\n",
    "        \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_folder.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Find all raster files\n",
    "    tif_files = list(folder.glob('*.tif')) + list(folder.glob('*.tiff'))\n",
    "    \n",
    "    print(f\"Found {len(tif_files)} raster files to convert\")\n",
    "    \n",
    "    for i, file_path in enumerate(tif_files, 1):\n",
    "        output_path = output_folder / file_path.name\n",
    "        print(f\"Converting ({i}/{len(tif_files)}): {file_path.name}\")\n",
    "        try:\n",
    "            convert_to_16bit(file_path, output_path, signed=signed)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_path.name}: {e}\")\n",
    "            \n",
    "    print(f\"Conversion complete. Output files saved to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_convert_to_16bit(folder_path, output_folder=folder_path+'_2', signed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce525ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# from rasterio.vrt import WarpedVRT\n",
    "# import glob\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Define the folder path containing your GeoTIFFs\n",
    "# folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T115601Z-001'\n",
    "# folder = Path(folder_path)\n",
    "\n",
    "# # Get list of TIFF files\n",
    "# tif_files = list(folder.glob('*.tif')) + list(folder.glob('*.tiff'))\n",
    "# raster_files = [\"example.tif\", \"example2.tif\", \"...\", \"examplen.tif\"]\n",
    "\n",
    "# tif_file_paths = [str(file) for file in tif_files]\n",
    "\n",
    "# # Create a mosaic using rasterio\n",
    "# output_vrt = str(folder / \"combined_rasters.vrt\")\n",
    "\n",
    "# # Build a simple VRT manually\n",
    "# with open(output_vrt, 'w') as f:\n",
    "#     f.write('<VRTDataset rasterXSize=\"10000\" rasterYSize=\"10000\">\\n')\n",
    "#     for idx, tif in enumerate(tif_file_paths):\n",
    "#         f.write(f'  <VRTRasterBand dataType=\"Float32\" band=\"{idx+1}\">\\n')\n",
    "#         f.write(f'    <SimpleSource>\\n')\n",
    "#         f.write(f'      <SourceFilename relativeToVRT=\"0\">{tif}</SourceFilename>\\n')\n",
    "#         f.write(f'      <SourceBand>1</SourceBand>\\n')\n",
    "#         f.write(f'    </SimpleSource>\\n')\n",
    "#         f.write(f'  </VRTRasterBand>\\n')\n",
    "#     f.write('</VRTDataset>')\n",
    "\n",
    "# print(f\"Virtual raster created manually: {output_vrt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c02ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69469275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f049980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# import rasterio\n",
    "# from rasterio.vrt import WarpedVRT\n",
    "\n",
    "# # Define the folder path containing your GeoTIFFs\n",
    "# folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T115601Z-001'\n",
    "# folder = Path(folder_path)\n",
    "\n",
    "# # Get list of TIFF files\n",
    "# tif_files = list(folder.glob('*.tif')) + list(folder.glob('*.tiff'))\n",
    "# tif_file_paths = [str(file) for file in tif_files]\n",
    "\n",
    "# # Output VRT path\n",
    "# output_vrt = str(folder / \"combined_rasters.vrt\")\n",
    "\n",
    "# # Get information from the first raster to determine VRT parameters\n",
    "# with rasterio.open(tif_file_paths[0]) as src:\n",
    "#     crs = src.crs\n",
    "#     nodata = src.nodata\n",
    "#     dtype = src.dtypes[0]\n",
    "\n",
    "# # Create VRT with rasterio's build_vrt function\n",
    "# with rasterio.Env():\n",
    "#     # Create a temporary text file with the list of rasters\n",
    "#     list_file = str(folder / \"raster_list.txt\")\n",
    "#     with open(list_file, 'w') as f:\n",
    "#         for tif in tif_file_paths:\n",
    "#             f.write(f\"{tif}\\n\")\n",
    "    \n",
    "#     # Use gdalbuildvrt through subprocess\n",
    "#     import subprocess\n",
    "#     cmd = ['gdalbuildvrt', '-separate', '-overwrite', output_vrt, '-input_file_list', list_file]\n",
    "#     subprocess.run(cmd)\n",
    "    \n",
    "#     # Remove temporary file\n",
    "#     os.remove(list_file)\n",
    "\n",
    "# print(f\"Virtual raster created: {output_vrt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eff0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# import subprocess\n",
    "\n",
    "# folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T115601Z-001'\n",
    "# folder = Path(folder_path)\n",
    "# tif_files = list(folder.glob('*.tif')) + list(folder.glob('*.tiff'))\n",
    "# tif_file_paths = [str(f) for f in tif_files]\n",
    "\n",
    "# # Write list of TIFFs to a text file for gdalbuildvrt\n",
    "# list_file = folder / \"raster_list.txt\"\n",
    "# with open(list_file, 'w') as f:\n",
    "#     for tif in tif_file_paths:\n",
    "#         f.write(f\"{tif}\\n\")\n",
    "\n",
    "# # Build the VRT using gdalbuildvrt\n",
    "# output_vrt = folder / \"combined_rasters.vrt\"\n",
    "# cmd = [\n",
    "#     'gdalbuildvrt', '-separate', '-overwrite',\n",
    "#     str(output_vrt),\n",
    "#     '-input_file_list', str(list_file)\n",
    "# ]\n",
    "# subprocess.run(cmd, check=True)\n",
    "\n",
    "# # Remove the temporary list file\n",
    "# os.remove(list_file)\n",
    "\n",
    "# print(f\"VRT created at: {output_vrt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9efd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# from rasterio.merge import merge\n",
    "# from rasterio.vrt import WarpedVRT\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Folder with TIFFs\n",
    "# folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T115601Z-001'\n",
    "# folder = Path(folder_path)\n",
    "\n",
    "# # Find all tiffs\n",
    "# tif_files = list(folder.glob('*.tif')) + list(folder.glob('*.tiff'))\n",
    "\n",
    "# # Open the datasets\n",
    "# datasets = [rasterio.open(str(tif)) for tif in tif_files]\n",
    "\n",
    "# # Merge them into a single mosaic (in-memory)\n",
    "# mosaic, out_trans = merge(datasets)\n",
    "\n",
    "# # Save a proper VRT\n",
    "# vrt_options = {\n",
    "#     'resampling': rasterio.enums.Resampling.nearest\n",
    "# }\n",
    "\n",
    "# # Save as a VRT file\n",
    "# vrt_path = str(folder / \"combined_rasters.vrt\")\n",
    "\n",
    "# with rasterio.open(\n",
    "#     vrt_path, 'w',\n",
    "#     driver='VRT',\n",
    "#     width=mosaic.shape[2],\n",
    "#     height=mosaic.shape[1],\n",
    "#     count=1,\n",
    "#     dtype=mosaic.dtype.name,\n",
    "#     transform=out_trans,\n",
    "#     crs=datasets[0].crs\n",
    "# ) as dst:\n",
    "#     dst.write(mosaic[0], 1)\n",
    "\n",
    "# print(f\"Proper VRT saved: {vrt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Define the folder path\n",
    "# folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T115601Z-001'\n",
    "# folder = Path(folder_path)\n",
    "\n",
    "# # Get list of TIFF files\n",
    "# tif_files = list(folder.glob('*.tif')) + list(folder.glob('*.tiff'))\n",
    "# tif_file_paths = [str(file) for file in tif_files]\n",
    "\n",
    "# # Output VRT path\n",
    "# output_vrt = str(folder / \"combined_rasters.vrt\")\n",
    "\n",
    "# # Create XML content for VRT file\n",
    "# with open(output_vrt, 'w') as f:\n",
    "#     # Open the first file to get dimensions\n",
    "#     with rasterio.open(tif_file_paths[0]) as src:\n",
    "#         width = src.width\n",
    "#         height = src.height\n",
    "    \n",
    "#     # Write VRT header\n",
    "#     f.write(f'<VRTDataset rasterXSize=\"{width}\" rasterYSize=\"{height}\">\\n')\n",
    "    \n",
    "#     # Add each input file as a separate band\n",
    "#     for i, tif_path in enumerate(tif_file_paths, 1):\n",
    "#         f.write(f'  <VRTRasterBand dataType=\"Int8\" band=\"{i}\">\\n')\n",
    "#         f.write('    <SimpleSource>\\n')\n",
    "#         f.write(f'      <SourceFilename relativeToVRT=\"0\">{tif_path}</SourceFilename>\\n')\n",
    "#         f.write('      <SourceBand>1</SourceBand>\\n')\n",
    "#         f.write('    </SimpleSource>\\n')\n",
    "#         f.write('  </VRTRasterBand>\\n')\n",
    "    \n",
    "#     # Close VRT\n",
    "#     f.write('</VRTDataset>')\n",
    "\n",
    "# print(f\"VRT file created at: {output_vrt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import math\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.validation import make_valid\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from functools import partial\n",
    "\n",
    "def generate_random_polygon(min_lon, min_lat, max_lon, max_lat, min_area_ha=1, max_area_ha=10, vertex_count=6):\n",
    "    \"\"\"\n",
    "    Generate a random polygon within bounds with area in specified range (hectares)\n",
    "    \n",
    "    Args:\n",
    "        min_lon, min_lat, max_lon, max_lat: Boundary coordinates\n",
    "        min_area_ha: Minimum area in hectares\n",
    "        max_area_ha: Maximum area in hectares\n",
    "        vertex_count: Number of vertices for the polygon\n",
    "    \"\"\"\n",
    "    # Helper function to calculate area in hectares\n",
    "    def calculate_area_ha(polygon, lon, lat):\n",
    "        # Create a projection from WGS84 to UTM for the specific location\n",
    "        proj_string = f\"+proj=utm +zone={int((lon + 180) / 6) + 1} +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n",
    "        project = partial(\n",
    "            pyproj.transform,\n",
    "            pyproj.Proj('EPSG:4326'),  # source coordinate system (WGS84)\n",
    "            pyproj.Proj(proj_string)   # target coordinate system (UTM)\n",
    "        )\n",
    "        \n",
    "        # Transform the polygon to UTM to calculate area in square meters\n",
    "        polygon_utm = transform(project, polygon)\n",
    "        # Convert square meters to hectares (1 hectare = 10,000 sq meters)\n",
    "        return polygon_utm.area / 10000\n",
    "    \n",
    "    # Initial parameters\n",
    "    target_area_ha = random.uniform(min_area_ha, max_area_ha)\n",
    "    max_attempts = 10\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        # Start with a random center point\n",
    "        center_lon = random.uniform(min_lon, max_lon)\n",
    "        center_lat = random.uniform(min_lat, max_lat)\n",
    "        \n",
    "        # Dynamic size adjustment - start with a reasonable guess\n",
    "        # This is a very rough approximation - about 0.01 degrees ~ 1000m at equator\n",
    "        size_estimate = math.sqrt(target_area_ha / 100) * 0.01\n",
    "        \n",
    "        # Create vertices around the center\n",
    "        vertices = []\n",
    "        for i in range(vertex_count):\n",
    "            angle = 2 * math.pi * i / vertex_count\n",
    "            # Add some irregularity\n",
    "            angle += random.uniform(-0.3, 0.3)\n",
    "            # Random distance from center (to make irregular polygon)\n",
    "            distance = random.uniform(0.8, 1.2) * size_estimate\n",
    "            \n",
    "            # Calculate vertex position\n",
    "            lon = center_lon + distance * math.cos(angle)\n",
    "            lat = center_lat + distance * math.sin(angle)\n",
    "            \n",
    "            # Ensure within bounds\n",
    "            lon = min(max(lon, min_lon), max_lon)\n",
    "            lat = min(max(lat, min_lat), max_lat)\n",
    "            \n",
    "            vertices.append((lon, lat))\n",
    "        \n",
    "        # Close the polygon\n",
    "        vertices.append(vertices[0])\n",
    "        \n",
    "        # Create and validate the polygon\n",
    "        poly = Polygon(vertices)\n",
    "        if not poly.is_valid:\n",
    "            poly = make_valid(poly)\n",
    "            if poly.geom_type != 'Polygon':  # Skip if make_valid doesn't return a simple polygon\n",
    "                continue\n",
    "        \n",
    "        # Calculate actual area\n",
    "        actual_area_ha = calculate_area_ha(poly, center_lon, center_lat)\n",
    "        \n",
    "        # Check if within target range (+/- 20%)\n",
    "        if min_area_ha * 0.8 <= actual_area_ha <= max_area_ha * 1.2:\n",
    "            return poly, actual_area_ha\n",
    "        \n",
    "        # If we're close, try to scale the polygon\n",
    "        if attempt < max_attempts - 1:\n",
    "            scale_factor = math.sqrt(target_area_ha / actual_area_ha)\n",
    "            # Apply scale factor for next attempt's size estimate\n",
    "            size_estimate *= scale_factor\n",
    "    \n",
    "    # If we reach here, return the last attempt's polygon anyway\n",
    "    return poly, actual_area_ha\n",
    "\n",
    "def generate_random_properties(area_ha):\n",
    "    \"\"\"Generate random properties for features including the actual area\"\"\"\n",
    "    land_uses = [\"forest\", \"agriculture\", \"settlement\", \"water\", \"grassland\"]\n",
    "    risk_levels = [\"low\", \"medium\", \"high\"]\n",
    "    \n",
    "    return {\n",
    "        \"id\": random.randint(1000, 9999),\n",
    "        \"area_ha\": round(area_ha, 2),\n",
    "        \"land_use\": random.choice(land_uses),\n",
    "        \"risk\": random.choice(risk_levels),\n",
    "    }\n",
    "\n",
    "def create_geojson(bounds, num_polygons=25, min_area_ha=1, max_area_ha=10):\n",
    "    \"\"\"Create a GeoJSON file with random polygons within area range\"\"\"\n",
    "    min_lon, min_lat, max_lon, max_lat = bounds\n",
    "    \n",
    "    features = []\n",
    "    for i in range(num_polygons):\n",
    "        # Random vertex count between 4 and 8\n",
    "        vertices = random.randint(4, 8)\n",
    "        \n",
    "        # Generate polygon with area control\n",
    "        polygon, actual_area = generate_random_polygon(\n",
    "            min_lon, min_lat, max_lon, max_lat, \n",
    "            min_area_ha=min_area_ha,\n",
    "            max_area_ha=max_area_ha,\n",
    "            vertex_count=vertices\n",
    "        )\n",
    "        \n",
    "        # Create GeoJSON feature with actual area\n",
    "        properties = generate_random_properties(actual_area)\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": properties,\n",
    "            \"geometry\": mapping(polygon)\n",
    "        }\n",
    "        \n",
    "        features.append(feature)\n",
    "    \n",
    "    # Create the GeoJSON feature collection\n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": features\n",
    "    }\n",
    "    \n",
    "    return geojson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8872fce",
   "metadata": {},
   "source": [
    "make random geometries within bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T194155Z-002_COGS' #COGS\n",
    "\n",
    "# Define bounds from the provided Earth Engine geometry\n",
    "bounds = [\n",
    "    -3.04548260909834,  # min_lon\n",
    "    5.253961384163733,  # min_lat\n",
    "    -1.0179939534016594,  # max_lon\n",
    "    7.48307210714245    # max_lat\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6601b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the GeoJSON\n",
    "random_polygons = create_geojson(bounds, num_polygons=1000, min_area_ha=.5, max_area_ha=1)\n",
    "# Save to file\n",
    "with open(folder_path+\"/random_polygons.geojson\", \"w\") as f:\n",
    "    json.dump(random_polygons, f, indent=2)\n",
    "\n",
    "print(\"Created random_polygons.geojson with random polygons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = ['sum']\n",
    "# tiff_path = r'C:\\Users\\Arnell\\Downloads\\whisp_image_clip_v0.tif'\n",
    "\n",
    "# folder_path = r'C:\\Users\\Arnell\\Downloads\\drive-download-20250427T115601Z-001_2' #goetiffs (16bit)\n",
    "list_of_tiffs = glob.glob(folder_path + '/*.tif') \n",
    "list_of_tiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da111bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats = exact_extract(\n",
    "    # rast=str(tiff_path),\n",
    "    rast = list_of_tiffs,\n",
    "    # rast = folder_path+'/combined_rasters.vrt',# slow with normal tiffs \n",
    "    # vec=GEOJSON_EXAMPLE_FILEPATH,\n",
    "    vec=folder_path+'/random_polygons.geojson',\n",
    "    # strategy=\"raster-sequential\",#\"feature-sequential\n",
    "    ops=ops,\n",
    "    output='pandas',\n",
    "    # include_cols=[id_column]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420334b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.to_csv(folder_path+'/combined_rasters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83542f",
   "metadata": {},
   "source": [
    "STORING AI answer to canb i run exact extract on a cloud bucket:\n",
    " Use GDAL's virtual file system (advanced)\n",
    "For Cloud-Optimized GeoTIFFs, you can use GDAL's virtual file system with the /vsigs/ prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a14e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gdal\n",
    "# from exactextract import exact_extract\n",
    "# import geopandas as gpd\n",
    "\n",
    "# # Set GCS credentials environment variable\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/your/credentials.json'\n",
    "\n",
    "# # GCS path using GDAL's virtual file system\n",
    "# gcs_path = \"/vsigs/your-bucket-name/path/to/file.tif\"\n",
    "\n",
    "# vector_file = \"your_vector_data.geojson\"\n",
    "# gdf = gpd.read_file(vector_file)\n",
    "\n",
    "# # Try with exactextract\n",
    "# try:\n",
    "#     stats = exact_extract(\n",
    "#         rast=gcs_path, \n",
    "#         vec=gdf,\n",
    "#         ops=[\"mean\"],\n",
    "#         output='pandas'\n",
    "#     )\n",
    "#     print(stats)\n",
    "# except Exception as e:\n",
    "#     print(f\"Direct access failed: {e}\")\n",
    "#     print(\"You may need to download the file first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e531c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic usage with default settings\n",
    "# results_df, csv_path = geotiff_stats_by_feature_id(\n",
    "#     geojson_path=GEOJSON_EXAMPLE_FILEPATH\n",
    "# )\n",
    "\n",
    "# # Custom directory and operations \n",
    "# results_df, csv_path = geotiff_stats_by_feature_id(\n",
    "#     geojson_path=GEOJSON_EXAMPLE_FILEPATH,\n",
    "#     tiff_dir=Path.home() / 'my_geotiffs',\n",
    "#     ops=['sum', 'mean', 'count'],\n",
    "#     max_features=5\n",
    "# )\n",
    "\n",
    "# # Custom ID pattern for different filename format (e.g., 'parcel_123_ndvi.tif')\n",
    "# results_df, csv_path = geotiff_stats_by_feature_id(\n",
    "#     geojson_path=GEOJSON_EXAMPLE_FILEPATH,\n",
    "#     tiff_id_pattern=r'parcel_(\\d+)_ndvi\\.tif',\n",
    "#     id_column='parcel_id'\n",
    "# )\n",
    "\n",
    "# # Specify output CSV location\n",
    "# results_df, csv_path = geotiff_stats_by_feature_id(\n",
    "#     geojson_path=GEOJSON_EXAMPLE_FILEPATH,\n",
    "#     output_csv=Path.home() / 'analysis' / 'whisp_results.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af18196",
   "metadata": {},
   "source": [
    "Stats for fc - parallel processing in batches (client side) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa900f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geotiff_stats_by_feature_id_threaded(\n",
    "    geojson_path, \n",
    "    tiff_dir=None, \n",
    "    output_csv=None, \n",
    "    tiff_id_pattern=r'feature_(\\d+)\\.tif', \n",
    "    id_column='internal_id',\n",
    "    ops=['sum'],\n",
    "    max_features=None,\n",
    "    max_workers=4,\n",
    "    batch_size=5  # Process files in batches for better performance\n",
    "):\n",
    "    \"\"\"\n",
    "    Process GeoTIFF files that match feature IDs in a GeoJSON using thread-based\n",
    "    parallelism, which avoids the serialization issues of multiprocessing.\n",
    "    \n",
    "    Args:\n",
    "        geojson_path (str or Path): Path to the GeoJSON file with features\n",
    "        tiff_dir (str or Path): Directory containing GeoTIFF files (default: ~/Downloads/whisp_features)\n",
    "        output_csv (str or Path): Path to save the output CSV (default: uses timestamp)\n",
    "        tiff_id_pattern (str): Regex pattern to extract ID from GeoTIFF filename\n",
    "        id_column (str): Column name in GeoJSON containing feature IDs\n",
    "        ops (list): List of operations to perform with exactextract\n",
    "        max_features (int): Maximum number of features to process\n",
    "        max_workers (int): Maximum number of concurrent workers (default: 4)\n",
    "        batch_size (int): Number of files to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined results DataFrame\n",
    "        str: Path to the output CSV file\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    logger = logging.getLogger('whisp_processor')\n",
    "    \n",
    "    # Set default directory if not specified\n",
    "    if tiff_dir is None:\n",
    "        tiff_dir = Path.home() / 'Downloads' / 'whisp_features'\n",
    "    else:\n",
    "        tiff_dir = Path(tiff_dir)\n",
    "    \n",
    "    # Load the GeoJSON\n",
    "    logger.info(f\"Loading GeoJSON from {geojson_path}\")\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    \n",
    "    # Ensure ID column exists\n",
    "    if id_column not in gdf.columns:\n",
    "        logger.warning(f\"ID column '{id_column}' not found in GeoJSON. Adding sequential IDs.\")\n",
    "        gdf[id_column] = range(1, len(gdf) + 1)\n",
    "    \n",
    "    # Apply max_features if specified\n",
    "    if max_features and max_features < len(gdf):\n",
    "        logger.info(f\"Limiting to first {max_features} features\")\n",
    "        gdf = gdf.iloc[:max_features]\n",
    "    \n",
    "    # Create feature lookup dictionary for faster access\n",
    "    feature_dict = {}\n",
    "    for idx, row in gdf.iterrows():\n",
    "        feature_id = row[id_column]\n",
    "        feature_dict[feature_id] = idx\n",
    "    \n",
    "    # Find matching GeoTIFF files\n",
    "    tiff_files = []\n",
    "    id_pattern = re.compile(tiff_id_pattern)\n",
    "    \n",
    "    for file in os.listdir(tiff_dir):\n",
    "        if file.endswith('.tif') or file.endswith('.tiff'):\n",
    "            match = id_pattern.search(file)\n",
    "            if match:\n",
    "                tiff_id = int(match.group(1))\n",
    "                if tiff_id in feature_dict:\n",
    "                    tiff_files.append(file)\n",
    "    \n",
    "    logger.info(f\"Found {len(tiff_files)} matching GeoTIFF files in {tiff_dir}\")\n",
    "    \n",
    "    # Set up output CSV\n",
    "    if output_csv is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_csv = Path(tiff_dir) / f\"feature_stats_{timestamp}.csv\"\n",
    "    else:\n",
    "        output_csv = Path(output_csv)\n",
    "    \n",
    "    # CSV writing synchronization\n",
    "    csv_created = False\n",
    "    csv_lock = threading.Lock()\n",
    "    \n",
    "    # Create batches of files for processing\n",
    "    batches = []\n",
    "    for i in range(0, len(tiff_files), batch_size):\n",
    "        batches.append(tiff_files[i:i+batch_size])\n",
    "    \n",
    "    logger.info(f\"Created {len(batches)} batches with up to {batch_size} files each\")\n",
    "    \n",
    "    # Function to process a batch of files\n",
    "    def process_batch(batch_files):\n",
    "        batch_results = []\n",
    "        \n",
    "        for tiff_file in batch_files:\n",
    "            try:\n",
    "                # Extract ID from filename\n",
    "                match = id_pattern.search(tiff_file)\n",
    "                if not match:\n",
    "                    logger.debug(f\"Could not extract ID from filename: {tiff_file}\")\n",
    "                    continue\n",
    "                    \n",
    "                tiff_id = int(match.group(1))\n",
    "                \n",
    "                # Find the corresponding feature in the GeoDataFrame\n",
    "                if tiff_id not in feature_dict:\n",
    "                    logger.debug(f\"No matching feature found for ID {tiff_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get the feature from the dataframe\n",
    "                feature_idx = feature_dict[tiff_id]\n",
    "                feature = gdf.iloc[[feature_idx]]\n",
    "                \n",
    "                # Full path to GeoTIFF file\n",
    "                tiff_path = tiff_dir / tiff_file\n",
    "                \n",
    "                # Execute exactextract\n",
    "                logger.info(f\"Processing feature ID: {tiff_id}\")\n",
    "                stats = exact_extract(\n",
    "                    rast=str(tiff_path),\n",
    "                    vec=feature,\n",
    "                    ops=ops,\n",
    "                    output='pandas',\n",
    "                    include_cols=[id_column]\n",
    "                )\n",
    "                \n",
    "                # Add the geometry column to the results\n",
    "                stats['geometry'] = feature.iloc[0].geometry\n",
    "                \n",
    "                # Add to batch results\n",
    "                batch_results.append(stats)\n",
    "                logger.info(f\"Feature {tiff_id} processed successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {tiff_file}: {str(e)}\")\n",
    "        \n",
    "        return batch_results\n",
    "    \n",
    "    # Process batches (in parallel if max_workers > 1)\n",
    "    all_results = []\n",
    "    \n",
    "    if max_workers > 1:\n",
    "        logger.info(f\"Processing batches in parallel with {max_workers} threads\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit batch tasks\n",
    "            future_to_batch = {executor.submit(process_batch, batch): i for i, batch in enumerate(batches)}\n",
    "            \n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(future_to_batch):\n",
    "                batch_idx = future_to_batch[future]\n",
    "                try:\n",
    "                    batch_results = future.result()\n",
    "                    for result in batch_results:\n",
    "                        # Write to CSV with proper synchronization\n",
    "                        with csv_lock:\n",
    "                            if not csv_created:\n",
    "                                result.to_csv(output_csv, index=False)\n",
    "                                csv_created = True\n",
    "                            else:\n",
    "                                result.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "                        # Collect results\n",
    "                        all_results.append(result)\n",
    "                    \n",
    "                    logger.info(f\"Completed batch {batch_idx+1}/{len(batches)}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Exception in batch {batch_idx}: {str(e)}\")\n",
    "    else:\n",
    "        # Process sequentially\n",
    "        logger.info(\"Processing batches sequentially\")\n",
    "        for i, batch in enumerate(batches):\n",
    "            batch_results = process_batch(batch)\n",
    "            for result in batch_results:\n",
    "                # Write to CSV\n",
    "                if not csv_created:\n",
    "                    result.to_csv(output_csv, index=False)\n",
    "                    csv_created = True\n",
    "                else:\n",
    "                    result.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "                # Collect results\n",
    "                all_results.append(result)\n",
    "            \n",
    "            logger.info(f\"Completed batch {i+1}/{len(batches)}\")\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Convert to GeoDataFrame\n",
    "        try:\n",
    "            result_gdf = gpd.GeoDataFrame(all_results_df, geometry='geometry')\n",
    "            if gdf.crs:\n",
    "                result_gdf = result_gdf.set_crs(gdf.crs)\n",
    "            \n",
    "            logger.info(f\"Total processing time: {time.time() - start_time:.2f}s\")\n",
    "            logger.info(f\"Results saved to {output_csv}\")\n",
    "            return result_gdf, str(output_csv)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating GeoDataFrame from results: {str(e)}\")\n",
    "    \n",
    "    if not all_results:\n",
    "        logger.warning(\"No results generated\")\n",
    "    \n",
    "    logger.info(f\"Total processing time: {time.time() - start_time:.2f}s\")\n",
    "    return pd.DataFrame(), str(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fe630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use thread-based parallelism (more reliable than processes for GIS operations)\n",
    "# results_df, csv_path = geotiff_stats_by_feature_id_threaded(\n",
    "#     geojson_path=GEOJSON_EXAMPLE_FILEPATH,\n",
    "#     max_workers=10,  # Adjust based on your machine's capabilities\n",
    "#     batch_size=10    # Smaller batch size for better load balancing\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df07af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# import os\n",
    "from exactextract import exact_extract\n",
    "\n",
    "def safely_extract_stats(tiff_path, feature, ops=['sum'], id_column=None):\n",
    "    \"\"\"\n",
    "    A safer wrapper around exactextract that ensures resources are properly released.\n",
    "    \n",
    "    This function isolates the exactextract call and ensures cleanup even if exceptions occur.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        # Process in its own scope\n",
    "        result = exact_extract(\n",
    "            rast=tiff_path,\n",
    "            vec=feature,\n",
    "            ops=ops,\n",
    "            output='pandas',\n",
    "            include_cols=[id_column] if id_column else None\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {tiff_path}: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # Explicit cleanup to help release the file\n",
    "        gc.collect()\n",
    "        \n",
    "        # On Windows, add a brief delay which sometimes helps release file locks\n",
    "        try:\n",
    "            import time\n",
    "            time.sleep(0.1)  # Short delay\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45260477",
   "metadata": {},
   "source": [
    "Chain with exact extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_stats_for_collection(\n",
    "    feature_collection, \n",
    "    image, \n",
    "    geojson_path=None,\n",
    "    output_dir=None,\n",
    "    output_csv=None,\n",
    "    scale=10, \n",
    "    max_features=None, \n",
    "    max_workers=None,\n",
    "    max_retries=3, \n",
    "    retry_delay=3,\n",
    "    ops=['sum'],\n",
    "    id_column='internal_id',\n",
    "    keep_geotiffs=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Combined function that downloads GeoTIFFs for features in a collection and\n",
    "    immediately runs exactextract to calculate statistics with proper band names.\n",
    "    \n",
    "    Args:\n",
    "        feature_collection: Earth Engine FeatureCollection to process\n",
    "        image: Earth Engine image to clip and download\n",
    "        geojson_path: Path to matching GeoJSON file (optional, for more precise polygon extraction)\n",
    "        output_dir: Directory to save the GeoTIFFs (default: ~/Downloads/whisp_features)\n",
    "        output_csv: Path to save the output CSV (default: uses timestamp)\n",
    "        scale: Resolution in meters (default 10m)\n",
    "        max_features: Maximum number of features to process (default: all)\n",
    "        max_workers: Maximum number of parallel workers (default: None, sequential)\n",
    "        max_retries: Maximum number of retry attempts for each download\n",
    "        retry_delay: Base delay in seconds between retries\n",
    "        ops: List of operations to perform with exactextract\n",
    "        id_column: Column name in GeoJSON containing feature IDs\n",
    "        keep_geotiffs: Whether to keep the downloaded GeoTIFF files (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        gdf: GeoDataFrame with extracted statistics\n",
    "        csv_path: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    import ee\n",
    "    import os\n",
    "    import time\n",
    "    import logging\n",
    "    import concurrent.futures\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import requests\n",
    "    import rasterio\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    import threading\n",
    "    from exactextract import exact_extract\n",
    "    \n",
    "    # Set up logging\n",
    "    logger = logging.getLogger('whisp_processor')\n",
    "    \n",
    "    # Get band names from the Earth Engine image\n",
    "    try:\n",
    "        # Get band information from the Earth Engine image\n",
    "        band_names = image.bandNames().getInfo()\n",
    "        logger.info(f\"Retrieved band names from image: {band_names}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to get band names from image: {str(e)}\")\n",
    "        band_names = None\n",
    "    \n",
    "    # Set default output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = Path.home() / 'Downloads' / 'whisp_features'\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Set up output CSV\n",
    "    if output_csv is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_csv = output_dir / f\"feature_stats_{timestamp}.csv\"\n",
    "    else:\n",
    "        output_csv = Path(output_csv)\n",
    "    \n",
    "    # Load GeoJSON if provided (for more accurate extraction)\n",
    "    if geojson_path:\n",
    "        logger.info(f\"Loading GeoJSON from {geojson_path}\")\n",
    "        source_gdf = gpd.read_file(geojson_path)\n",
    "        \n",
    "        # Ensure ID column exists\n",
    "        if id_column not in source_gdf.columns:\n",
    "            logger.warning(f\"ID column '{id_column}' not found in GeoJSON. Adding sequential IDs.\")\n",
    "            source_gdf[id_column] = range(1, len(source_gdf) + 1)\n",
    "            \n",
    "        # Create feature lookup dictionary for faster access\n",
    "        feature_dict = {}\n",
    "        for idx, row in source_gdf.iterrows():\n",
    "            feature_id = row[id_column]\n",
    "            feature_dict[feature_id] = idx\n",
    "    else:\n",
    "        source_gdf = None\n",
    "        feature_dict = None\n",
    "    \n",
    "    # Get collection size and limit if needed\n",
    "    collection_size = feature_collection.size().getInfo()\n",
    "    logger.info(f\"Processing Earth Engine FeatureCollection with {collection_size} features\")\n",
    "    \n",
    "    if max_features and max_features < collection_size:\n",
    "        feature_collection = feature_collection.limit(max_features)\n",
    "        collection_size = max_features\n",
    "        logger.info(f\"Limited to processing first {max_features} features\")\n",
    "    \n",
    "    # Get features as a list\n",
    "    features = feature_collection.toList(collection_size)\n",
    "    \n",
    "    # CSV writing synchronization\n",
    "    csv_created = False\n",
    "    csv_lock = threading.Lock()\n",
    "    all_results = []\n",
    "    \n",
    "    \n",
    "    def rename_band_columns(df, band_names, ops=['sum']):\n",
    "        \"\"\"\n",
    "        Rename generic band index columns to meaningful band names in a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with stats columns to rename\n",
    "            band_names (list): List of band names from Earth Engine image\n",
    "            ops (list): List of operations (e.g., ['sum', 'mean', 'count'])\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with renamed columns\n",
    "        \"\"\"\n",
    "        # Create a copy of the input DataFrame to avoid modifying the original\n",
    "        renamed_df = df.copy()\n",
    "        \n",
    "        # Create a mapping from generic band names to actual band names\n",
    "        column_mapping = {}\n",
    "        for op in ops:\n",
    "            for i, band_name in enumerate(band_names):\n",
    "                # Check both possible formats\n",
    "                format1 = f\"{op}_{i+1}\"      # e.g., sum_1 (standard exactextract format)\n",
    "                format2 = f\"band_{i+1}_{op}\" # e.g., band_1_sum (alternative format)\n",
    "                \n",
    "                if format1 in renamed_df.columns:\n",
    "                    column_mapping[format1] = band_name\n",
    "                elif format2 in renamed_df.columns:\n",
    "                    column_mapping[format2] = band_name\n",
    "        \n",
    "        # Apply the renaming and return\n",
    "        return renamed_df.rename(columns=column_mapping)\n",
    "\n",
    "\n",
    "    def download_and_process_feature(index):\n",
    "        try:\n",
    "            # Get the feature\n",
    "            ee_feature = ee.Feature(features.get(index))\n",
    "            \n",
    "            # Get the feature ID\n",
    "            try:\n",
    "                internal_id = ee_feature.get(id_column).getInfo()\n",
    "                logger.info(f\"Processing feature {internal_id} ({index+1}/{collection_size})\")\n",
    "            except Exception:\n",
    "                internal_id = f\"unknown_{index}\"\n",
    "                logger.warning(f\"Could not get ID for feature {index}, using {internal_id}\")\n",
    "            \n",
    "            # Create a unique filename\n",
    "            filename = f\"feature_{internal_id}.tif\"\n",
    "            output_path = output_dir / filename\n",
    "            \n",
    "            # Skip download if file exists\n",
    "            if output_path.exists():\n",
    "                logger.info(f\"File {filename} already exists, skipping download\")\n",
    "                # Always use process_downloaded_feature for consistency\n",
    "                return process_downloaded_feature(ee_feature, str(output_path), internal_id)\n",
    "            \n",
    "            # Download the file\n",
    "            retries = 0\n",
    "            while retries < max_retries:\n",
    "                try:\n",
    "                    # Clip the image to the feature\n",
    "                    clipped_image = image.clip(ee_feature.geometry())\n",
    "                    \n",
    "                    # Generate the download URL\n",
    "                    logger.debug(f\"Generating download URL for feature {internal_id}\")\n",
    "                    download_url = clipped_image.getDownloadURL({\n",
    "                        'format': 'GeoTIFF',\n",
    "                        'region': ee_feature.geometry(),\n",
    "                        'scale': scale,\n",
    "                        'crs': 'EPSG:4326'\n",
    "                    })\n",
    "                    \n",
    "                    # Download the image with timeout\n",
    "                    logger.info(f\"Downloading to {output_path}\")\n",
    "                    response = requests.get(download_url, timeout=300)\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        # Check if the response is actually a GeoTIFF\n",
    "                        content_type = response.headers.get('Content-Type', '')\n",
    "                        if 'tiff' in content_type.lower() or 'zip' in content_type.lower():\n",
    "                            with open(output_path, 'wb') as f:\n",
    "                                f.write(response.content)\n",
    "                            logger.info(f\"Successfully downloaded {filename}\")\n",
    "                            \n",
    "                            # Process the downloaded file\n",
    "                            return process_downloaded_feature(ee_feature, str(output_path), internal_id)\n",
    "                        else:\n",
    "                            logger.error(f\"Download returned non-TIFF content: {content_type}\")\n",
    "                            error_file = output_dir / f\"error_{internal_id}.txt\"\n",
    "                            with open(error_file, 'wb') as f:\n",
    "                                f.write(response.content[:2000])\n",
    "                            retries += 1\n",
    "                    else:\n",
    "                        logger.error(f\"Failed to download (status {response.status_code})\")\n",
    "                        retries += 1\n",
    "                    \n",
    "                    # Wait before retrying\n",
    "                    if retries < max_retries:\n",
    "                        sleep_time = retry_delay * (2 ** retries)\n",
    "                        logger.info(f\"Retrying in {sleep_time} seconds (attempt {retries+1}/{max_retries})\")\n",
    "                        time.sleep(sleep_time)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error downloading feature {internal_id}: {str(e)}\", exc_info=True)\n",
    "                    retries += 1\n",
    "                    if retries < max_retries:\n",
    "                        logger.info(f\"Retrying in {retry_delay} seconds (attempt {retries+1}/{max_retries})\")\n",
    "                        time.sleep(retry_delay)\n",
    "            \n",
    "            logger.error(f\"Maximum retries reached for feature {internal_id}\")\n",
    "            return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing feature at index {index}: {str(e)}\", exc_info=True)\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    # Helper function to process a downloaded GeoTIFF file\n",
    "    def process_downloaded_feature(ee_feature, tiff_path, feature_id):\n",
    "\n",
    "        def get_band_names(tif_path):\n",
    "            \"\"\"Extract band names from a GeoTIFF file.\"\"\"\n",
    "            with rasterio.open(tif_path) as src:\n",
    "                if src.descriptions and all(src.descriptions):\n",
    "                    return list(src.descriptions)\n",
    "                else:\n",
    "                    return [f\"Band {i+1}\" for i in range(src.count)]\n",
    "        try:\n",
    "            # Ensure we have a plain string path, not a Path object\n",
    "            tiff_path_str = str(tiff_path)\n",
    "            \n",
    "            logger.info(f\"Processing downloaded file: {tiff_path_str}\")\n",
    "\n",
    "            # # Get band names from the TIFF file if not already available\n",
    "            \n",
    "            local_band_names = band_names\n",
    "\n",
    "            if local_band_names is None:\n",
    "                print(f\"Band names not available from image, trying to read from file: {tiff_path_str}\")\n",
    "                try:\n",
    "                    local_band_names = get_band_names(tiff_path_str)\n",
    "                    local_band_names = image.bandNames().getInfo()\n",
    "\n",
    "                    with rasterio.open(tiff_path_str) as src:\n",
    "                        # If raster has descriptions, use them as band names\n",
    "                        if src.descriptions and all(src.descriptions):\n",
    "                            local_band_names = list(src.descriptions)\n",
    "                            logger.info(f\"Using band descriptions from GeoTIFF: {local_band_names}\")\n",
    "                        else:\n",
    "                            # Otherwise create generic names\n",
    "                            local_band_names = [f\"band_{i+1}\" for i in range(src.count)]\n",
    "                            logger.info(f\"Using generic band names: {local_band_names}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to read band names from GeoTIFF: {str(e)}\")\n",
    "                    # Fallback to generic names if reading fails\n",
    "                    local_band_names = image.bandNames().getInfo()\n",
    "            \n",
    "\n",
    "\n",
    "            # Find the corresponding feature in source GeoJSON if available\n",
    "            if source_gdf is not None and feature_id in feature_dict:\n",
    "                feature_idx = feature_dict[feature_id]\n",
    "                feature = source_gdf.iloc[[feature_idx]]\n",
    "            else:\n",
    "                logger.warning(f\"Feature ID {feature_id} not found in GeoJSON\")\n",
    "                # # Use the EE feature's geometry (less precise but works)\n",
    "                # feature_geom = gpd.GeoDataFrame(\n",
    "                #     {'internal_id': [feature_id]},\n",
    "                #     geometry=[gpd.GeoSeries.from_wkt([ee_feature.geometry().toWkt().getInfo()])[0]]\n",
    "                # )\n",
    "                # feature = feature_geom\n",
    "            \n",
    "            # Use exactextract with plain string path\n",
    "            logger.debug(f\"Running exactextract on {tiff_path_str}\")\n",
    "            stats = exact_extract(\n",
    "                rast=tiff_path_str,\n",
    "                vec=feature,\n",
    "                ops=ops,\n",
    "                output='pandas',\n",
    "                include_cols=[id_column]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Rename columns to use actual band names if available\n",
    "            if local_band_names:\n",
    "                stats = rename_band_columns(stats, local_band_names, ops=ops)\n",
    "                logger.debug(f\"Renamed columns using band names: {local_band_names}\")\n",
    "            \n",
    "\n",
    "            \n",
    "            # Add the geometry column\n",
    "            stats['geometry'] = feature.iloc[0].geometry\n",
    "            \n",
    "            # Delete the GeoTIFF if not keeping\n",
    "            if not keep_geotiffs:\n",
    "                try:\n",
    "                    os.remove(tiff_path_str)\n",
    "                    logger.debug(f\"Deleted temporary file {tiff_path_str}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to delete temporary file {tiff_path_str}: {str(e)}\")\n",
    "            \n",
    "            return stats\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting stats for feature {feature_id}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Process features (parallel or sequential)\n",
    "    if max_workers and max_workers > 1:\n",
    "        logger.info(f\"Processing features in parallel with {max_workers} workers\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_index = {\n",
    "                executor.submit(download_and_process_feature, i): i \n",
    "                for i in range(collection_size)\n",
    "            }\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in concurrent.futures.as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    stats = future.result()\n",
    "                    if stats is not None:\n",
    "                        # Write to CSV with proper synchronization\n",
    "                        with csv_lock:\n",
    "                            if not csv_created:\n",
    "                                stats.to_csv(output_csv, index=False)\n",
    "                                csv_created = True\n",
    "                            else:\n",
    "                                stats.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "                        # Add to results\n",
    "                        all_results.append(stats)\n",
    "                        logger.info(f\"Completed feature {index+1}/{collection_size}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Failed to process feature {index+1}/{collection_size}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Exception occurred while processing feature {index+1}: {str(e)}\")\n",
    "    else:\n",
    "        # Sequential processing\n",
    "        logger.info(\"Processing features sequentially\")\n",
    "        for i in range(collection_size):\n",
    "            logger.info(f\"Processing feature {i+1}/{collection_size}\")\n",
    "            stats = download_and_process_feature(i)\n",
    "            if stats is not None:\n",
    "                # Write to CSV\n",
    "                if not csv_created:\n",
    "                    stats.to_csv(output_csv, index=False)\n",
    "                    csv_created = True\n",
    "                else:\n",
    "                    stats.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "                # Add to results\n",
    "                all_results.append(stats)\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        try:\n",
    "            all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "            result_gdf = gpd.GeoDataFrame(all_results_df, geometry='geometry')\n",
    "            \n",
    "            if source_gdf is not None and source_gdf.crs:\n",
    "                result_gdf = result_gdf.set_crs(source_gdf.crs)\n",
    "            \n",
    "            logger.info(f\"Completed processing {len(all_results)}/{collection_size} features successfully\")\n",
    "            logger.info(f\"Results saved to {output_csv}\")                    \n",
    "                \n",
    "            return result_gdf, str(output_csv)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating final GeoDataFrame: {str(e)}\")\n",
    "    \n",
    "    if not all_results:\n",
    "        logger.warning(\"No results generated\")\n",
    "    return None, str(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e22aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic usage with defaults\n",
    "# results_df, csv_path = download_and_extract_stats_for_collection(\n",
    "#     feature_collection=ee_bbox_collection,\n",
    "#     image=whisp.combine_datasets(),\n",
    "#     geojson_path=GEOJSON_EXAMPLE_FILEPATH\n",
    "# )\n",
    "\n",
    "GEOJSON_EXAMPLE_FILEPATH = folder_path+\"/random_polygons.geojson\"\n",
    "\n",
    "# ee_bbox_collection = convert_geojson_to_ee_bbox(GEOJSON_EXAMPLE_FILEPATH)\n",
    "\n",
    "# Example 5: Full obscuration - extend, shift, and add random features\n",
    "fully_obscured_collection = convert_geojson_to_ee_bbox_obscured(\n",
    "    GEOJSON_EXAMPLE_FILEPATH,\n",
    "    # extension_range=[0.002, 0.003],\n",
    "    # shift_geometries=True,\n",
    "    # shift_proportion=0.9,\n",
    "    # pixel_length=0.0001,  # ~10m at equator\n",
    "    # add_random_features=False,\n",
    "    # max_distance=0.05,  # xkm at equator\n",
    "    # random_proportion= 0.1  # Add X more features as decoys\n",
    ")\n",
    "\n",
    "# Advanced usage\n",
    "results_df, csv_path = download_and_extract_stats_for_collection(\n",
    "    feature_collection=fully_obscured_collection,\n",
    "    image=whisp.combine_datasets(),\n",
    "    geojson_path=GEOJSON_EXAMPLE_FILEPATH,\n",
    "    output_dir=folder_path+ \"/\"+'whisp_on_the_fly_v4',\n",
    "    # output_csv=Path.home() / 'whisp_analysis' / 'results.csv',\n",
    "    ops=['sum'],# 'mean', 'count'],\n",
    "    max_features=1000,\n",
    "    max_workers=30,\n",
    "    keep_geotiffs=True  # Delete GeoTIFFs after processing to save space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# tif_path = r\"C:\\Users\\Arnell\\Downloads\\whisp_features\\feature_9.tif\"\n",
    "# def get_band_names(tif_path):\n",
    "#     \"\"\"Extract band names from a GeoTIFF file.\"\"\"\n",
    "#     with rasterio.open(tif_path) as src:\n",
    "#         if src.descriptions and all(src.descriptions):\n",
    "#             return list(src.descriptions)\n",
    "#         else:\n",
    "#             return [f\"Band {i+1}\" for i in range(src.count)]\n",
    "            \n",
    "# # Check the updated band names\n",
    "# bands = get_band_names(tif_path)\n",
    "# print(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c304ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rename_band_columns(df, band_names, ops=['sum']):\n",
    "#     \"\"\"\n",
    "#     Rename generic band index columns to meaningful band names in a DataFrame.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): DataFrame with stats columns to rename\n",
    "#         band_names (list): List of band names from Earth Engine image\n",
    "#         ops (list): List of operations (e.g., ['sum', 'mean', 'count'])\n",
    "        \n",
    "#     Returns:\n",
    "#         pd.DataFrame: DataFrame with renamed columns\n",
    "#     \"\"\"\n",
    "#     # Create a copy of the input DataFrame to avoid modifying the original\n",
    "#     renamed_df = df.copy()\n",
    "    \n",
    "#     # Create a mapping from generic band names to actual band names\n",
    "#     column_mapping = {}\n",
    "#     for op in ops:\n",
    "#         for i, band_name in enumerate(band_names):\n",
    "#             # Check both possible formats\n",
    "#             format1 = f\"{op}_{i+1}\"      # e.g., sum_1 (standard exactextract format)\n",
    "#             format2 = f\"band_{i+1}_{op}\" # e.g., band_1_sum (alternative format)\n",
    "            \n",
    "#             if format1 in renamed_df.columns:\n",
    "#                 column_mapping[format1] = band_name\n",
    "#             elif format2 in renamed_df.columns:\n",
    "#                 column_mapping[format2] = band_name\n",
    "    \n",
    "#     # Apply the renaming and return\n",
    "#     return renamed_df.rename(columns=column_mapping)\n",
    "\n",
    "\n",
    "\n",
    "# def rename_stats_columns_with_band_names(csv_path, band_names, ops=['sum']):\n",
    "#     \"\"\"\n",
    "#     Rename statistics columns in a CSV file by replacing generic band indices \n",
    "#     with actual band names from Earth Engine image bands.\n",
    "    \n",
    "#     Args:\n",
    "#         csv_path (str): Path to the CSV file with statistics\n",
    "#         band_names (list): List of band names from image.bandNames().getInfo()\n",
    "#         ops (list): List of operations (e.g., ['sum', 'mean', 'count'])\n",
    "        \n",
    "#     Returns:\n",
    "#         pd.DataFrame: DataFrame with renamed columns\n",
    "#     \"\"\"\n",
    "#     # import pandas as pd\n",
    "    \n",
    "#     # Load the CSV\n",
    "#     stats = pd.read_csv(csv_path)\n",
    "    \n",
    "#     # Print current columns to help diagnose format\n",
    "#     print(\"Current columns:\", stats.columns.tolist())\n",
    "#     rename_band_columns\n",
    "#     # Create a mapping from generic band names to actual band names\n",
    "#     column_mapping = {}\n",
    "#     for op in ops:\n",
    "#         for i, band_name in enumerate(band_names):\n",
    "#             # Check both possible formats\n",
    "#             format1 = f\"{op}_{i+1}\"      # e.g., sum_1 (standard exactextract format)\n",
    "#             format2 = f\"band_{i+1}_{op}\" # e.g., band_1_sum (alternative format)\n",
    "            \n",
    "#             if format1 in stats.columns:\n",
    "#                 column_mapping[format1] = band_name\n",
    "#             elif format2 in stats.columns:\n",
    "#                 column_mapping[format2] = band_name\n",
    "    \n",
    "#     # Print mapping for verification\n",
    "#     print(\"Column mapping:\", column_mapping)\n",
    "    \n",
    "#     # Apply the renaming\n",
    "#     stats = stats.rename(columns=column_mapping)\n",
    "    \n",
    "#     return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = r\"C:\\Users\\Arnell\\Downloads\\whisp_features\\feature_stats_20250424_190341.csv\"\n",
    "# stats = pd.read_csv(csv_path)\n",
    "# ops = ['sum']\n",
    "# band_names = whisp.combine_datasets().bandNames().getInfo()\n",
    "# stats = rename_band_columns(stats, band_names, ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d57d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "\n",
    "# def get_band_names(tif_path):\n",
    "#     \"\"\"\n",
    "#     Extract band names from a GeoTIFF file.\n",
    "    \n",
    "#     Args:\n",
    "#         tif_path (str): Path to the GeoTIFF file\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of band names/descriptions\n",
    "#     \"\"\"\n",
    "#     with rasterio.open(tif_path) as src:\n",
    "#         # Try to get band descriptions (often contain band names)\n",
    "#         band_descriptions = src.descriptions\n",
    "        \n",
    "#         # If descriptions are available and not empty, use them\n",
    "#         if band_descriptions and all(band_descriptions):\n",
    "#             print(f\"Found {len(band_descriptions)} bands with descriptions\")\n",
    "#             return list(band_descriptions)\n",
    "            \n",
    "#         # Check for band metadata that might contain names\n",
    "#         band_names = []\n",
    "#         for i in range(1, src.count + 1):\n",
    "#             band_meta = src.tags(i)\n",
    "#             if band_meta and 'name' in band_meta:\n",
    "#                 band_names.append(band_meta['name'])\n",
    "#             else:\n",
    "#                 # Fall back to generic naming\n",
    "#                 band_names.append(f\"Band {i}\")\n",
    "        \n",
    "#         print(f\"Found {src.count} bands\")\n",
    "#         return band_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2481f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# Replace with your file path\n",
    "tif_path = r\"C:\\Users\\Arnell\\Downloads\\whisp_features\\feature_1.tif\"\n",
    "band_names = get_band_names(tif_path)\n",
    "for i, name in enumerate(band_names):\n",
    "    print(f\"Band {i+1}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707653ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_open_files():\n",
    "    \"\"\"Check for open TIFF files and return their paths\"\"\"\n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    open_files = process.open_files()\n",
    "    tiff_files = [f.path for f in open_files if f.path.endswith(('.tif', '.tiff'))]\n",
    "    print(f\"Open TIFF files: {tiff_files}\")\n",
    "    return tiff_files  # Return the list of file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6285ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_open_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_release_tiff_files():\n",
    "    \"\"\"Force release of stubborn TIFF file locks using multiple approaches\"\"\"\n",
    "    import gc\n",
    "    import os\n",
    "    import psutil\n",
    "    import sys\n",
    "    \n",
    "    print(\"Starting aggressive TIFF cleanup...\")\n",
    "    \n",
    "    # 1. First garbage collection pass\n",
    "    gc.collect()\n",
    "    \n",
    "    # 2. Try to identify what's holding the files\n",
    "    process = psutil.Process()\n",
    "    tiff_files = [f for f in process.open_files() if f.path.endswith(('.tif', '.tiff'))]\n",
    "    for file in tiff_files:\n",
    "        print(f\"Locked file: {file}\")\n",
    "    \n",
    "    # 3. Try to reset libraries that commonly lock files\n",
    "    try:\n",
    "        # Reset GDAL\n",
    "        from osgeo import gdal\n",
    "        gdal.UseExceptions()  # Make GDAL throw exceptions\n",
    "        print(\"Resetting GDAL cache...\")\n",
    "        gdal.SetConfigOption('GDAL_MAX_DATASET_POOL_SIZE', '0')  # Disable dataset pooling\n",
    "        gdal.SetCacheMax(0)  # Clear caches\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"GDAL not directly imported\")\n",
    "    \n",
    "    # 4. Reset exactextract if it's loaded\n",
    "    if 'exactextract' in sys.modules:\n",
    "        print(\"Removing exactextract from sys.modules...\")\n",
    "        del sys.modules['exactextract']\n",
    "    \n",
    "    # 5. If rasterio is being used\n",
    "    try:\n",
    "        import rasterio\n",
    "        from rasterio.errors import RasterioIOError\n",
    "        \n",
    "        print(\"Cleaning rasterio environment...\")\n",
    "        rasterio.env.GDALEnv(CPL_DEBUG=True)  # Create a new environment with debug on\n",
    "        \n",
    "        # Try to deliberately close the files\n",
    "        for file in tiff_files:\n",
    "            try:\n",
    "                # This might raise an error, but sometimes forces release\n",
    "                with rasterio.open(file.path, 'r') as src:\n",
    "                    pass  # Just open and close it to reset\n",
    "            except RasterioIOError:\n",
    "                pass  # Ignore errors, we're just trying to force close\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"Rasterio not installed\")\n",
    "    \n",
    "    # 6. More aggressive garbage collection\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "    \n",
    "    # 7. Check what's left\n",
    "    remaining = [f for f in psutil.Process().open_files() if f.path.endswith(('.tif', '.tiff'))]\n",
    "    print(f\"After aggressive cleanup: {len(remaining)} files still locked\")\n",
    "    \n",
    "    return remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56757ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_release_tiff_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a241815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safely_process_with_isolation(tiff_paths):\n",
    "    \"\"\"\n",
    "    Process a list of TIFF files by completely isolating the exactextract module\n",
    "    to prevent file locks from persisting.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import sys\n",
    "    import importlib\n",
    "    \n",
    "    print(f\"Attempting to unlock {len(tiff_paths)} files using isolation method...\")\n",
    "    \n",
    "    # Step 1: Force reset all relevant modules that might hold locks\n",
    "    modules_to_reload = []\n",
    "    for module_name in list(sys.modules.keys()):\n",
    "        if any(keyword in module_name for keyword in ['gdal', 'rasterio', 'exactextract', 'fiona', 'osgeo']):\n",
    "            modules_to_reload.append(module_name)\n",
    "    \n",
    "    # Force unload these modules\n",
    "    for module_name in modules_to_reload:\n",
    "        if module_name in sys.modules:\n",
    "            try:\n",
    "                del sys.modules[module_name]\n",
    "                print(f\"Unloaded: {module_name}\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    # Step 2: Run aggressive garbage collection\n",
    "    print(\"Running multiple garbage collection cycles...\")\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "    \n",
    "    # Step 3: Attempt a direct file copy approach to break locks\n",
    "    import os\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    \n",
    "    for tiff_path in tiff_paths:\n",
    "        try:\n",
    "            path = Path(tiff_path)\n",
    "            # Create a temporary copy with a different name\n",
    "            temp_path = path.with_name(f\"temp_{path.name}\")\n",
    "            \n",
    "            try:\n",
    "                # Copy the file data rather than moving the file handle\n",
    "                shutil.copy2(tiff_path, temp_path)\n",
    "                print(f\"Created temporary copy: {temp_path}\")\n",
    "                \n",
    "                # Remove original (might fail if still locked)\n",
    "                try:\n",
    "                    os.remove(tiff_path)\n",
    "                    # Rename temp back to original\n",
    "                    os.rename(temp_path, tiff_path)\n",
    "                    print(f\"Successfully unlocked: {tiff_path}\")\n",
    "                except:\n",
    "                    print(f\"Original file still locked, will keep temporary copy: {temp_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying file {tiff_path}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {tiff_path}: {str(e)}\")\n",
    "    \n",
    "    print(\"Isolation process complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "safely_process_with_isolation(check_open_files())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076bb45",
   "metadata": {},
   "source": [
    "Chain for downloading and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6eea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# tiff_path = Path(\"C:/Users/Arnell/Downloads/whisp_features/feature_1.tif\")\n",
    "# print(\"Exists:\", tiff_path.exists())\n",
    "# print(\"Is file:\", tiff_path.is_file())\n",
    "# print(\"Absolute path:\", tiff_path.resolve())\n",
    "# print(str(tiff_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba70e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # internal_id =\"1\"        \n",
    "# gdf = gpd.read_file(GEOJSON_EXAMPLE_FILEPATH)\n",
    "# # Find matching feature in the GeoDataFrame\n",
    "# feature = gdf.iloc[0]\n",
    "# # if len(feature) == 0:\n",
    "# #     logger.warning(f\"No matching feature found for ID {internal_id}, skipping\")\n",
    "# #     return []\n",
    "\n",
    "# # Get the geometry from the feature\n",
    "# geom = feature.geometry#.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da86d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact_extract(rast=str(tiff_path),\n",
    "#                vec=gdf,\n",
    "#                ops=['sum'],\n",
    "#                output='pandas',\n",
    "#             #    include_cols=['internal_id']\n",
    "#             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf43f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = whisp.whisp_formatted_stats_ee_to_df(convert_geojson_to_ee_bbox(GEOJSON_EXAMPLE_FILEPATH))\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2f8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81858532",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted_stats "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec34493",
   "metadata": {},
   "source": [
    "Parallel processing test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce13c5",
   "metadata": {},
   "source": [
    "Whisp it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted_stats = whisp.whisp_formatted_stats_geojson_to_df(folder_path+'/random_polygons.geojson')\n",
    "# df_formatted_stats = whisp.whisp_formatted_stats_geojson_to_df(GEOJSON_EXAMPLE_FILEPATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ce34d",
   "metadata": {},
   "source": [
    "Display table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da398c62-7372-426e-9ff4-25d9f72afe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder (if running in Sepal change path to preferred folder) \n",
    "out_directory = Path.home() / 'Downloads'\n",
    "\n",
    "# Define the output file path for CSV\n",
    "csv_output_file = out_directory / 'whisp_output_table_stats.csv'\n",
    "\n",
    "# Save the CSV file\n",
    "df_formatted_stats.to_csv(path_or_buf=csv_output_file, index=False)\n",
    "print(f\"Table with risk columns saved to: {csv_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ecbf7-d86f-425c-9c93-80ce21f3dff2",
   "metadata": {},
   "source": [
    "Calculate risk category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd1b5c-41ac-4d3c-af26-a47db3d48af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add risk columns to end of dataframe\n",
    "df_w_risk = whisp.whisp_risk(df=df_formatted_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15577b-44bb-4792-98ef-95fd6bc1aabb",
   "metadata": {},
   "source": [
    "Display table with risk columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62b10e-7484-47cd-9f73-40f612be5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c0019-585e-4598-8c54-c628625f5f80",
   "metadata": {},
   "source": [
    "Export table to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder \n",
    "# e.g. in running in Sepal this might be: Path.home() / 'module_results/whisp/'\n",
    "out_directory = Path.home() / 'Downloads'\n",
    "\n",
    "# Define the output file path for CSV\n",
    "csv_output_file = out_directory / 'whisp_output_table_w_risk.csv'\n",
    "\n",
    "# Save the CSV file\n",
    "df_w_risk.to_csv(path_or_buf=csv_output_file, index=False)\n",
    "print(f\"Table with risk columns saved to: {csv_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c1765",
   "metadata": {},
   "source": [
    "Export to GeoJSON (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path for GeoJSON\n",
    "geojson_output_file = out_directory / 'whisp_output_table.geojson'\n",
    "\n",
    "# Save the GeoJSON file\n",
    "whisp.convert_df_to_geojson(df_w_risk, geojson_output_file)  # builds a geojson file containing Whisp columns. Uses the geometry column \"geo\" to create the spatial features.\n",
    "print(f\"GeoJSON file saved to: {geojson_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ede603",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49731520",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13aed10e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eaa785e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
