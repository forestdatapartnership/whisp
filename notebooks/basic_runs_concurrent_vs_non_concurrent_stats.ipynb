{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459bb104",
   "metadata": {},
   "source": [
    "# Test: WHISP Concurrent & Sequential Processing\n",
    "\n",
    "Testing new concurrent and sequential stats processing functions with proper logging, progress tracking, and endpoint validation.\n",
    "\n",
    "## Test Structure\n",
    "\n",
    "- Concurrent processing (high-volume endpoint)\n",
    "- Sequential processing (standard endpoint)\n",
    "- Results comparison and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ce0ab",
   "metadata": {},
   "source": [
    "## Part 1: Setup\n",
    "\n",
    "Initialize Earth Engine and configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Reset Earth Engine completely\n",
    "ee.Reset()\n",
    "print(\"✅ Earth Engine reset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ba7e1",
   "metadata": {},
   "source": [
    "## Part 2: CONCURRENT PROCESSING (High-Volume Endpoint)\n",
    "\n",
    "Test concurrent processing with the high-volume endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Initialize and set high-volume endpoint\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Earth Engine reset\")\n",
    "    ee.data.setDebuggingEnabled(False)\n",
    "    print(\"Initialized with high-volume endpoint\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        ee.Authenticate()\n",
    "        ee.Initialize()\n",
    "        print(\"Authenticated and initialized with high-volume endpoint\")\n",
    "    except:\n",
    "        print(\"Using HIGH-VOLUME endpoint\")\n",
    "        if \"high-volume\" not in str(e).lower():\n",
    "            print(\"WARNING: Not using high-volume endpoint!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify endpoint is high-volume\n",
    "api_url = str(ee.data._cloud_api_base_url)\n",
    "if 'highvolume' in api_url:\n",
    "    print(\"✅ Using HIGH-VOLUME endpoint\")\n",
    "else:\n",
    "    print(\"❌ WARNING: Not using high-volume endpoint!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c52f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n",
    "import logging\n",
    "from openforis_whisp.concurrent_stats import (\n",
    "    setup_concurrent_logger,\n",
    "    validate_ee_endpoint,\n",
    "    whisp_stats_geojson_to_df_concurrent,\n",
    "    check_ee_endpoint,\n",
    ")\n",
    "\n",
    "print(\"✅ Imported concurrent stats module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging for concurrent processing\n",
    "logger = setup_concurrent_logger(level=logging.INFO)\n",
    "logger.info(\"Logging configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose if want to include additional custom layers\n",
    "USE_CUSTOM_BANDS = True # set to True if want to add extra ee data to whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose if want to include additional custom layers\n",
    "USE_CUSTOM_BANDS = True # set to True if want to add extra ee data to whisp\n",
    "# =============================================================================\n",
    "# CUSTOM BANDS SETUP (OPTIONAL) - runs only if USE_CUSTOM_BANDS = True above\n",
    "# =============================================================================\n",
    "if USE_CUSTOM_BANDS:\n",
    "\n",
    "    # Step 1: Define custom Earth Engine images (binary values 0 or 1)\n",
    "    custom_images = {\n",
    "        'example_treecover': ee.Image(1),  # ee.Image(\"UMD/hansen/global_forest_change_2024_v1_12\").select(\"treecover2000\").gt(10).selfMask()\n",
    "        'nXX_example_commodity': ee.Image.random(seed=1).gte(.5).reproject(crs='EPSG:4326', scale=10) # ee.ImageCollection(\"projects/forestdatapartnership/assets/cocoa/model_2025a\").filter(ee.Filter.date('2020-01-01', '2021-01-01')).mosaic().gt(.8).selfMask()\n",
    "        # add more images as needed (prefix 'nXX_' = iso2 code for national dataset)\n",
    "    }\n",
    "\n",
    "    # Step 2: Define metadata for each custom band (keys must match above)\n",
    "    # Themes: 'treecover', 'commodities', 'disturbance_before', 'disturbance_after'\n",
    "    # Timber themes: 'primary', 'naturally_reg_2020', 'planted_plantation_2020', etc.\n",
    "    custom_bands_info = {\n",
    "        'example_treecover': {\n",
    "            'ISO2_code': \"\",          # Country code (empty = all countries)\n",
    "            'theme': 'treecover',     # Risk theme\n",
    "            'theme_timber': \"\",       # Timber theme (if applicable)\n",
    "            'use_for_risk': 1,        # Include in risk calculations (1=yes, 0=no)\n",
    "            'use_for_risk_timber': 0  # Include in timber risk (1=yes, 0=no)\n",
    "        },\n",
    "        'nXX_example_commodity': {\n",
    "            'ISO2_code': \"XX\", \n",
    "            'theme': 'commodities', \n",
    "            'theme_timber': \"\",\n",
    "            'use_for_risk': 1, \n",
    "            'use_for_risk_timber': 0\n",
    "        }\n",
    "        # add more band metadata as needed\n",
    "    }\n",
    "\n",
    "    # Step 3: Combine custom bands and extract names\n",
    "    custom_ee_image = whisp.combine_custom_bands(custom_images, custom_bands_info)\n",
    "\n",
    "    custom_bands = list(custom_bands_info.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose additional national datasets to include (currently three countries: 'co', 'ci', 'br').\n",
    "base_iso2_codes = ['co', 'ci', 'br']\n",
    "\n",
    "# automatically add any custom ISO2 codes from custom_bands_info if USE_CUSTOM_BANDS is True\n",
    "iso2_codes_list = base_iso2_codes.copy()\n",
    "if USE_CUSTOM_BANDS:\n",
    "    iso2_codes_list += [code.lower() for code in {v.get('ISO2_code') for v in custom_bands_info.values()} if code and code.lower() not in iso2_codes_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20071d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n",
    "\n",
    "print(\"Imported concurrent stats module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_polygons=1000  # Smaller dataset for testing\n",
    "min_area_ha=10 \n",
    "max_area_ha=10 \n",
    "min_number_vert=10     \n",
    "max_number_vert=10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5704fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data (or use your own GeoJSON)\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "state_geom = (ee.FeatureCollection(\"projects/sat-io/open-datasets/FAO/GAUL/GAUL_2024_L1\")\n",
    "    .filter(ee.Filter.inList('gaul1_name', ['Amazonas', 'Mato Grosso', 'Rondônia', 'Pará'])))\n",
    "bounds = state_geom.geometry().bounds()\n",
    "\n",
    "# Suppress GeoJSON generation messages\n",
    "with redirect_stdout(io.StringIO()):\n",
    "    random_geojson = whisp.generate_test_polygons(\n",
    "        bounds=bounds, \n",
    "        num_polygons=num_polygons,\n",
    "        min_area_ha=min_area_ha, \n",
    "        max_area_ha=max_area_ha, \n",
    "        min_number_vert=min_number_vert,     \n",
    "        max_number_vert=max_number_vert     \n",
    "    )\n",
    "\n",
    "# Save to temporary file\n",
    "temp_fd, concurrent_geojson_path = tempfile.mkstemp(suffix='.geojson', text=True)\n",
    "os.close(temp_fd)\n",
    "with open(concurrent_geojson_path, 'w') as f:\n",
    "    json.dump(random_geojson, f)\n",
    "\n",
    "print(f\"Generated test GeoJSON with {len(random_geojson['features'])} features\")\n",
    "print(f\"   Saved to: {concurrent_geojson_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ec983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to validate GeoJSON file size\n",
    "def validate_geojson_size(geojson_path, max_size_mb=10):\n",
    "    \"\"\"\n",
    "    Check if GeoJSON file size is within acceptable limits.\n",
    "    \n",
    "    Args:\n",
    "        geojson_path: Path to the GeoJSON file\n",
    "        max_size_mb: Maximum allowed size in MB (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_valid, size_mb, message)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    file_size_bytes = os.path.getsize(geojson_path)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    is_valid = file_size_mb <= max_size_mb\n",
    "    \n",
    "    if is_valid:\n",
    "        message = f\"GeoJSON size OK: {file_size_mb:.2f} MB (limit: {max_size_mb} MB)\"\n",
    "    else:\n",
    "        message = f\"GeoJSON TOO LARGE: {file_size_mb:.2f} MB (limit: {max_size_mb} MB)\"\n",
    "    \n",
    "    return is_valid, file_size_mb, message\n",
    "\n",
    "# Test the validation function\n",
    "test_path = concurrent_geojson_path\n",
    "msg = validate_geojson_size(test_path, max_size_mb=10)\n",
    "print(msg[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Whisp image with national codes\n",
    "iso2_codes = ['br', 'co', 'ci']\n",
    "\n",
    "# whisp_image = whisp.combine_datasets(national_codes=iso2_codes)\n",
    "# band_names = whisp_image.bandNames().getInfo()\n",
    "# print(f\"Created Whisp image with {len(band_names)} bands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90379d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concurrent: GeoJSON → DataFrame with automatic formatting\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 1: Concurrent GeoJSON → DataFrame (Formatted)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    df_concurrent = whisp.whisp_formatted_stats_geojson_to_df_concurrent(\n",
    "        input_geojson_filepath=concurrent_geojson_path,\n",
    "        # whisp_image=whisp_image,\n",
    "        # custom_bands=custom_bands if USE_CUSTOM_BANDS else None,\n",
    "        national_codes=iso2_codes,\n",
    "        batch_size=10,\n",
    "        max_concurrent=20,\n",
    "        validate_geometries=False,\n",
    "        add_metadata_server=False,\n",
    "        logger=logger,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ SUCCESS: Concurrent processing complete!\")\n",
    "    print(f\"   Processed: {df_concurrent.shape[0]} features\")\n",
    "    print(f\"   Output columns: {df_concurrent.shape[1]}\")\n",
    "    print(f\"\\n   First row sample:\")\n",
    "    print(df_concurrent.iloc[0, :8])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c2ea1",
   "metadata": {},
   "source": [
    "## Part 3: SEQUENTIAL PROCESSING (For Comparison)\n",
    "\n",
    "Test sequential (standard endpoint) processing as an alternative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13094a04",
   "metadata": {},
   "source": [
    "### Part 3A: Switch to Standard Endpoint\n",
    "\n",
    "Switch from high-volume to standard endpoint for sequential testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Reset Earth Engine completely\n",
    "ee.Reset()\n",
    "print(\"✅ Earth Engine reset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Engine initialization with STANDARD endpoint\n",
    "try:\n",
    "    ee.Initialize(opt_url='https://earthengine.googleapis.com')\n",
    "    print(\"✅ Initialized with standard endpoint\")\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(opt_url='https://earthengine.googleapis.com')\n",
    "    print(\"✅ Authenticated and initialized with standard endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fresh test data for sequential testing (avoid caching)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if True:  # Try generating fresh data\n",
    "    try:\n",
    "        random_geojson_sequential = whisp.generate_test_polygons(\n",
    "            region=geom,\n",
    "            num_polygons=5,\n",
    "            area_ha=5000,  # Larger areas\n",
    "            max_vertices=100,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        temp_fd_sequential, sequential_geojson_path = tempfile.mkstemp(suffix='.geojson', text=True)\n",
    "        \n",
    "        with open(sequential_geojson_path, 'w') as f:\n",
    "            json.dump(random_geojson_sequential, f)\n",
    "        \n",
    "        print(f\"✅ Generated fresh test GeoJSON with {len(random_geojson_sequential['features'])} features\")\n",
    "        print(f\"   Saved to: {sequential_geojson_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Fallback: Using example data ({e})\")\n",
    "        random_geojson_sequential = whisp.get_example_data_path(\"geojson_example.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequential: GeoJSON → DataFrame (Sequential Processing)\n",
    "print(\"\\nTEST 2: Sequential GeoJSON → DataFrame (Sequential)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    df_sequential = whisp.whisp_formatted_stats_geojson_to_df_sequential(\n",
    "        input_geojson_filepath=sequential_geojson_path,\n",
    "        national_codes=['BR'],\n",
    "        add_metadata_client_side=True,\n",
    "        logger=logger,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ SUCCESS: Sequential processing complete!\")\n",
    "    print(f\"   Processed: {df_sequential.shape[0]} features\")\n",
    "    print(f\"   Output columns: {df_sequential.shape[1]}\")\n",
    "    print(\"\\n   First row preview:\")\n",
    "    print(df_sequential.iloc[0, :8])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    df_sequential = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare concurrent vs sequential results\n",
    "print(\"\\nCOMPARISON: Concurrent vs Sequential\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if df_concurrent is not None and df_sequential is not None:\n",
    "    print(f\"\\nConcurrent shape:  {df_concurrent.shape}\")\n",
    "    print(f\"Sequential shape:  {df_sequential.shape}\")\n",
    "    \n",
    "    # Verify they return same columns\n",
    "    if set(df_concurrent.columns) == set(df_sequential.columns):\n",
    "        print(\"\\n✅ Column names match!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  Column names differ\")\n",
    "        print(f\"   Concurrent columns: {set(df_concurrent.columns) - set(df_sequential.columns)}\")\n",
    "        print(f\"   Sequential columns: {set(df_sequential.columns) - set(df_concurrent.columns)}\")\n",
    "    \n",
    "    print(f\"\\n✅ Sequential is simpler and better for debugging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11963e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_concurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f89af",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
