{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c45afc",
   "metadata": {},
   "source": [
    "# BENCHMARK: Concurrent vs Sequential Stats Processing\n",
    "\n",
    "Benchmarking comparing concurrent (batch processing), sequential (standard), and legacy endpoints.\n",
    "\n",
    "## Benchmark Structure\n",
    "\n",
    "- **BENCHMARK 1**: Concurrent processing (high-volume endpoint, batches)\n",
    "- **BENCHMARK 2**: Sequential processing (standard endpoint, sequential)\n",
    "- **BENCHMARK 3**: Legacy endpoint comparison\n",
    "- **ANALYSIS**: Performance comparison, scaling behavior, recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ee\n",
    "\n",
    "# Set up Downloads path for all outputs\n",
    "downloads_path = Path.home() / \"Downloads\" / \"whisp_benchmarks\"\n",
    "downloads_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {downloads_path}\")\n",
    "\n",
    "# Initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Earth Engine initialized\")\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "    print(\"Earth Engine authenticated and initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Reset()\n",
    "ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "print(\"Earth Engine reset and initialized with HIGH-VOLUME endpoint for benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WHISP\n",
    "import openforis_whisp as whisp\n",
    "from openforis_whisp.concurrent_stats import validate_ee_endpoint, check_ee_endpoint\n",
    "print(\"WHISP imported with endpoint validation functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Whisp image (reuse if exists, create once for all benchmarks)\n",
    "print(\"\\nðŸ“¦ Creating WHISP image for benchmarking...\")\n",
    "iso2_codes = ['br', 'co', 'ci']\n",
    "\n",
    "try:\n",
    "    whisp_image_bench = whisp.combine_datasets(national_codes=iso2_codes,auto_recovery=True)\n",
    "    print(f\"âœ… Created WHISP image for benchmarking\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Using existing whisp_image from earlier\")\n",
    "    whisp_image_bench = whisp_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9802c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openforis_whisp.advanced_stats import validate_ee_endpoint, check_ee_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define Brazil Amazon test region\n",
    "test_region_states = [\"Amazonas\", \"Mato Grosso\", \"RondÃ´nia\"]\n",
    "print(f\"Test region: Brazil Amazon ({', '.join(test_region_states)})\")\n",
    "\n",
    "\n",
    "print(\"\\nBenchmark Parameters:\")\n",
    "print(\"  - Concurrent vs Sequential comparison\")\n",
    "print(\"  - Multiple polygon counts and complexity levels\")\n",
    "print(\"  - Multiple repetitions for statistical significance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012000e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "precompiled_image = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    whisp_image\n",
    "    print(\"Using existing whisp_image from earlier\")\n",
    "except NameError:\n",
    "    print(\"Creating WHISP image for benchmarking...\")\n",
    "    try:\n",
    "        whisp_image = whisp.combine_datasets()\n",
    "        print(\"Created WHISP image for benchmarking\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating image: {e}\")\n",
    "        whisp_image = None\n",
    "\n",
    "print(\"Helper function defined for test data generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e028b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to generate test GeoJSON\n",
    "def generate_test_geojson(num_polygons, area_ha, num_vertices, bounds):\n",
    "    \"\"\"Generate random test GeoJSON within bounds\"\"\"\n",
    "    try:\n",
    "        geojson = whisp.generate_test_polygons(\n",
    "            bounds=bounds,\n",
    "            num_polygons=num_polygons,\n",
    "            min_area_ha=area_ha * 0.9,\n",
    "            max_area_ha=area_ha * 1.1,\n",
    "            min_number_vert=num_vertices,\n",
    "            max_number_vert=num_vertices,\n",
    "        )\n",
    "        \n",
    "        # Save to temp file\n",
    "        import tempfile\n",
    "        temp_fd, temp_path = tempfile.mkstemp(suffix='.geojson', text=True)\n",
    "        try:\n",
    "            with os.fdopen(temp_fd, 'w') as f:\n",
    "                json.dump(geojson, f)\n",
    "        except:\n",
    "            os.close(temp_fd)\n",
    "            raise\n",
    "        \n",
    "        return temp_path, geojson\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating test data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Set up logger\n",
    "logger = whisp.setup_concurrent_logger()\n",
    "\n",
    "# Get Brazil geometry for testing\n",
    "try:\n",
    "    fc = (ee.FeatureCollection(\"projects/sat-io/open-datasets/FAO/GAUL/GAUL_2024_L1\")\n",
    "    .filter(ee.Filter.inList('gaul1_name', test_region_states)))\n",
    "    geom = fc.geometry().bounds()\n",
    "    print(\"\\nâœ… Brazil regions geometry loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load Brazil geometry: {e}\")\n",
    "    geom = None\n",
    "\n",
    "print(\"Helper function defined for test data generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRE-BENCHMARK CHECK: Validate High-Volume Endpoint\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRE-BENCHMARK CHECK: Endpoint Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check and display current endpoint\n",
    "api_url = str(ee.data._cloud_api_base_url)\n",
    "current_endpoint = \"HIGH-VOLUME\" if \"highvolume\" in api_url.lower() else \"STANDARD\"\n",
    "print(f\"\\nðŸ“ Current Earth Engine Endpoint: {current_endpoint}\")\n",
    "print(f\"   URL: {api_url}\")\n",
    "\n",
    "# Validate that we're using high-volume endpoint for concurrent\n",
    "try:\n",
    "    validate_ee_endpoint(\"high-volume\", raise_error=True)\n",
    "    print(\"\\nâœ… High-volume endpoint validated - concurrent benchmark can proceed\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nâŒ Endpoint validation FAILED:\")\n",
    "    print(f\"{e}\")\n",
    "    print(\"\\nðŸ”§ Attempting to fix by initializing high-volume endpoint...\")\n",
    "    ee.Reset()\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "    print(\"âœ… High-volume endpoint re-initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 1: CONCURRENT METHOD (High-Volume Endpoint)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK 1: CONCURRENT METHOD (High-Volume Endpoint)\")\n",
    "print(f\"Running {benchmark_params['num_repetitions']} repetitions per test\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "benchmark_results_concurrent = []\n",
    "\n",
    "# Iterate through all parameter combinations with repetitions\n",
    "total_combinations = (len(benchmark_params['areas_ha']) * \n",
    "                      len(benchmark_params['polygon_counts']) * \n",
    "                      len(benchmark_params['vertex_complexity']))\n",
    "total_tests = total_combinations * benchmark_params['num_repetitions']\n",
    "test_num = 0\n",
    "\n",
    "for area_ha in benchmark_params['areas_ha']:\n",
    "    for num_polygons in benchmark_params['polygon_counts']:\n",
    "        for num_vertices in benchmark_params['vertex_complexity']:\n",
    "            # Run each configuration multiple times\n",
    "            for rep in range(benchmark_params['num_repetitions']):\n",
    "                test_num += 1\n",
    "                \n",
    "                # Generate test data\n",
    "                geojson_path, geojson_data = generate_test_geojson(\n",
    "                    num_polygons=num_polygons,\n",
    "                    area_ha=area_ha,\n",
    "                    num_vertices=num_vertices,\n",
    "                    bounds=geom\n",
    "                )\n",
    "                \n",
    "                # Run concurrent processing with timing\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    df_result = whisp.whisp_stats_geojson_to_df_concurrent(\n",
    "                        input_geojson_filepath=geojson_path,\n",
    "                        national_codes=iso2_codes,\n",
    "                        add_metadata_server=False,\n",
    "                        whisp_image=whisp_image_bench if precompiled_image == True else None,\n",
    "                    )\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    status = \"âœ…\"\n",
    "                    error_msg = None\n",
    "                    rows_processed = df_result.shape[0]\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    status = \"âŒ\"\n",
    "                    error_msg = str(e)\n",
    "                    rows_processed = 0\n",
    "                \n",
    "                # Store results\n",
    "                benchmark_results_concurrent.append({\n",
    "                    'method': 'Concurrent',\n",
    "                    'area_ha': area_ha,\n",
    "                    'num_polygons': num_polygons,\n",
    "                    'num_vertices': num_vertices,\n",
    "                    'repetition': rep + 1,\n",
    "                    'total_features': num_polygons,\n",
    "                    'rows_processed': rows_processed,\n",
    "                    'time_seconds': elapsed_time,\n",
    "                    'time_per_polygon': elapsed_time / num_polygons if num_polygons > 0 else 0,\n",
    "                    'status': status,\n",
    "                    'error': error_msg\n",
    "                })\n",
    "                \n",
    "                # Show progress (only show every 3rd rep to reduce clutter)\n",
    "                if rep == benchmark_params['num_repetitions'] - 1:\n",
    "                    print(f\"[{test_num:3d}/{total_tests}] {status} Area: {area_ha:3d}ha | \" + \n",
    "                          f\"Polygons: {num_polygons:3d} | Vertices: {num_vertices:5d} | \" + \n",
    "                          f\"Rep {rep+1}/{benchmark_params['num_repetitions']} | Time: {elapsed_time:7.2f}s\")\n",
    "                \n",
    "                # Cleanup - try multiple times as file may be locked\n",
    "                import time as time_module\n",
    "                time_module.sleep(0.1)  # Give time for file to be released\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        os.remove(geojson_path)\n",
    "                        break\n",
    "                    except (PermissionError, FileNotFoundError):\n",
    "                        if attempt < 2:\n",
    "                            time_module.sleep(0.2)\n",
    "                        elif os.path.exists(geojson_path):\n",
    "                            pass  # File may be locked, skip for now\n",
    "\n",
    "print(f\"\\nâœ… Concurrent benchmarking complete ({test_num} tests with {benchmark_params['num_repetitions']} repetitions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BACKUP: Save Concurrent Results\n",
    "# ============================================================================\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df_concurrent_backup = pd.DataFrame(benchmark_results_concurrent)\n",
    "concurrent_backup_file = downloads_path / f\"benchmark_concurrent_{timestamp}.csv\"\n",
    "df_concurrent_backup.to_csv(concurrent_backup_file, index=False)\n",
    "print(f\"\\nðŸ’¾ Concurrent results backed up: {concurrent_backup_file}\")\n",
    "print(f\"   Rows: {len(df_concurrent_backup)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577763da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SWITCH TO STANDARD ENDPOINT for non-concurrent and legacy tests\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SWITCHING ENDPOINTS: Resetting to Standard for non-concurrent tests\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ee.Reset()\n",
    "print(\"âœ… Earth Engine reset\")\n",
    "\n",
    "try:\n",
    "    ee.Initialize(opt_url='https://earthengine.googleapis.com')\n",
    "    print(\"âœ… Initialized with standard endpoint\")\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(opt_url='https://earthengine.googleapis.com')\n",
    "    print(\"âœ… Authenticated and initialized with standard endpoint\")\n",
    "\n",
    "# Verify endpoint\n",
    "api_url = str(ee.data._cloud_api_base_url)\n",
    "print(f\"ðŸ“ Current endpoint: {'HIGH-VOLUME' if 'highvolume' in api_url else 'STANDARD'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 2: SEQUENTIAL METHOD (Standard Endpoint)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK 2: SEQUENTIAL METHOD (Standard Endpoint)\")\n",
    "print(f\"Running {benchmark_params['num_repetitions']} repetitions per test\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "benchmark_results_sequential = []\n",
    "\n",
    "# Iterate through all parameter combinations with repetitions\n",
    "total_combinations = (len(benchmark_params['areas_ha']) * \n",
    "                      len(benchmark_params['polygon_counts']) * \n",
    "                      len(benchmark_params['vertex_complexity']))\n",
    "total_tests = total_combinations * benchmark_params['num_repetitions']\n",
    "test_num = 0\n",
    "\n",
    "for area_ha in benchmark_params['areas_ha']:\n",
    "    for num_polygons in benchmark_params['polygon_counts']:\n",
    "        for num_vertices in benchmark_params['vertex_complexity']:\n",
    "            # Run each configuration multiple times\n",
    "            for rep in range(benchmark_params['num_repetitions']):\n",
    "                test_num += 1\n",
    "                \n",
    "                # Generate test data\n",
    "                geojson_path, geojson_data = generate_test_geojson(\n",
    "                    num_polygons=num_polygons,\n",
    "                    area_ha=area_ha,\n",
    "                    num_vertices=num_vertices,\n",
    "                    bounds=geom\n",
    "                )\n",
    "                \n",
    "                # Run sequential processing with timing\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    df_result = whisp.whisp_formatted_stats_geojson_to_df_sequential(\n",
    "                        input_geojson_filepath=geojson_path,\n",
    "                        national_codes=iso2_codes,\n",
    "                        add_metadata_client_side=True,\n",
    "                        logger=logger,\n",
    "                    )\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    status = \"âœ…\"\n",
    "                    error_msg = None\n",
    "                    rows_processed = df_result.shape[0]\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    status = \"âŒ\"\n",
    "                    error_msg = str(e)\n",
    "                    rows_processed = 0\n",
    "                \n",
    "                # Store results\n",
    "                benchmark_results_sequential.append({\n",
    "                    'method': 'Sequential',\n",
    "                    'area_ha': area_ha,\n",
    "                    'num_polygons': num_polygons,\n",
    "                    'num_vertices': num_vertices,\n",
    "                    'repetition': rep + 1,\n",
    "                    'total_features': num_polygons,\n",
    "                    'rows_processed': rows_processed,\n",
    "                    'time_seconds': elapsed_time,\n",
    "                    'time_per_polygon': elapsed_time / num_polygons if num_polygons > 0 else 0,\n",
    "                    'status': status,\n",
    "                    'error': error_msg\n",
    "                })\n",
    "                \n",
    "                # Show progress (only show every 3rd rep to reduce clutter)\n",
    "                if rep == benchmark_params['num_repetitions'] - 1:\n",
    "                    print(f\"[{test_num:3d}/{total_tests}] {status} Area: {area_ha:3d}ha | \" + \n",
    "                          f\"Polygons: {num_polygons:3d} | Vertices: {num_vertices:5d} | \" + \n",
    "                          f\"Rep {rep+1}/{benchmark_params['num_repetitions']} | Time: {elapsed_time:7.2f}s\")\n",
    "                \n",
    "                # Cleanup - try multiple times as file may be locked\n",
    "                import time as time_module\n",
    "                time_module.sleep(0.1)  # Give time for file to be released\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        os.remove(geojson_path)\n",
    "                        break\n",
    "                    except (PermissionError, FileNotFoundError):\n",
    "                        if attempt < 2:\n",
    "                            time_module.sleep(0.2)\n",
    "                        elif os.path.exists(geojson_path):\n",
    "                            pass  # File may be locked, skip for now\n",
    "\n",
    "print(f\"\\nâœ… Sequential benchmarking complete ({test_num} tests with {benchmark_params['num_repetitions']} repetitions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BACKUP: Save Sequential Results\n",
    "# ============================================================================\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df_sequential_backup = pd.DataFrame(benchmark_results_sequential)\n",
    "sequential_backup_file = downloads_path / f\"benchmark_sequential_{timestamp}.csv\"\n",
    "df_sequential_backup.to_csv(sequential_backup_file, index=False)\n",
    "print(f\"\\nðŸ’¾ Sequential results backed up: {sequential_backup_file}\")\n",
    "print(f\"   Rows: {len(df_sequential_backup)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BENCHMARK 3: LEGACY METHOD (Standard Endpoint)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK 3: LEGACY METHOD - whisp_formatted_stats_geojson_to_df (Standard Endpoint)\")\n",
    "print(f\"Running {benchmark_params['num_repetitions']} repetitions per test\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "benchmark_results_legacy = []\n",
    "\n",
    "# Iterate through all parameter combinations with repetitions\n",
    "total_combinations = (len(benchmark_params['areas_ha']) * \n",
    "                      len(benchmark_params['polygon_counts']) * \n",
    "                      len(benchmark_params['vertex_complexity']))\n",
    "total_tests = total_combinations * benchmark_params['num_repetitions']\n",
    "test_num = 0\n",
    "\n",
    "for area_ha in benchmark_params['areas_ha']:\n",
    "    for num_polygons in benchmark_params['polygon_counts']:\n",
    "        for num_vertices in benchmark_params['vertex_complexity']:\n",
    "            # Run each configuration multiple times\n",
    "            for rep in range(benchmark_params['num_repetitions']):\n",
    "                test_num += 1\n",
    "                \n",
    "                # Generate test data\n",
    "                geojson_path, geojson_data = generate_test_geojson(\n",
    "                    num_polygons=num_polygons,\n",
    "                    area_ha=area_ha,\n",
    "                    num_vertices=num_vertices,\n",
    "                    bounds=geom\n",
    "                )\n",
    "                \n",
    "                # Run legacy processing with timing\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    df_result = whisp.whisp_formatted_stats_geojson_to_df(\n",
    "                        input_geojson_filepath=geojson_path,\n",
    "                        national_codes=iso2_codes,\n",
    "                        whisp_image=whisp_image_bench if precompiled_image == True else None,\n",
    "                    )\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    status = \"âœ…\"\n",
    "                    error_msg = None\n",
    "                    rows_processed = df_result.shape[0]\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    status = \"âŒ\"\n",
    "                    error_msg = str(e)\n",
    "                    rows_processed = 0\n",
    "                \n",
    "                # Store results\n",
    "                benchmark_results_legacy.append({\n",
    "                    'method': 'Legacy',\n",
    "                    'area_ha': area_ha,\n",
    "                    'num_polygons': num_polygons,\n",
    "                    'num_vertices': num_vertices,\n",
    "                    'repetition': rep + 1,\n",
    "                    'total_features': num_polygons,\n",
    "                    'rows_processed': rows_processed,\n",
    "                    'time_seconds': elapsed_time,\n",
    "                    'time_per_polygon': elapsed_time / num_polygons if num_polygons > 0 else 0,\n",
    "                    'status': status,\n",
    "                    'error': error_msg\n",
    "                })\n",
    "                \n",
    "                # Show progress (only show every 3rd rep to reduce clutter)\n",
    "                if rep == benchmark_params['num_repetitions'] - 1:\n",
    "                    print(f\"[{test_num:3d}/{total_tests}] {status} Area: {area_ha:3d}ha | \" + \n",
    "                          f\"Polygons: {num_polygons:3d} | Vertices: {num_vertices:5d} | \" + \n",
    "                          f\"Rep {rep+1}/{benchmark_params['num_repetitions']} | Time: {elapsed_time:7.2f}s\")\n",
    "                \n",
    "                # Cleanup - try multiple times as file may be locked\n",
    "                import time as time_module\n",
    "                time_module.sleep(0.1)  # Give time for file to be released\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        os.remove(geojson_path)\n",
    "                        break\n",
    "                    except (PermissionError, FileNotFoundError):\n",
    "                        if attempt < 2:\n",
    "                            time_module.sleep(0.2)\n",
    "                        elif os.path.exists(geojson_path):\n",
    "                            pass  # File may be locked, skip for now\n",
    "\n",
    "print(f\"\\nâœ… Legacy benchmarking complete ({test_num} tests with {benchmark_params['num_repetitions']} repetitions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BACKUP: Save Legacy Results\n",
    "# ============================================================================\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df_legacy_backup = pd.DataFrame(benchmark_results_legacy)\n",
    "legacy_backup_file = downloads_path / f\"benchmark_legacy_{timestamp}.csv\"\n",
    "df_legacy_backup.to_csv(legacy_backup_file, index=False)\n",
    "print(f\"\\nðŸ’¾ Legacy results backed up: {legacy_backup_file}\")\n",
    "print(f\"   Rows: {len(df_legacy_backup)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d1e48f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS 1: COMBINED RESULTS SUMMARY (with Statistics)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Detailed Statistics by Method:\n",
      "(Mean Â± Std Dev, with min/max and count)\n",
      "\n",
      "           time_seconds                                time_per_polygon  \\\n",
      "                  count    mean    std     min     max             mean   \n",
      "method                                                                    \n",
      "Concurrent           20  18.664  3.436  14.935  32.116            0.136   \n",
      "Legacy               20  19.830  4.951  11.164  30.128            0.139   \n",
      "Sequential           20  10.530  3.534   6.600  18.483            0.073   \n",
      "\n",
      "                  rows_processed  \n",
      "              std            sum  \n",
      "method                            \n",
      "Concurrent  0.031           2875  \n",
      "Legacy      0.019           2875  \n",
      "Sequential  0.013           2875  \n",
      "\n",
      "\n",
      "ðŸ“Š 95% Confidence Intervals by Method:\n",
      "(Mean Â± CI)\n",
      "\n",
      "Concurrent        :   18.66s Â±   1.61s (n=20, 95% CI)\n",
      "Sequential        :   10.53s Â±   1.65s (n=20, 95% CI)\n",
      "Legacy            :   19.83s Â±   2.32s (n=20, 95% CI)\n",
      "\n",
      "ðŸ“Š Success Rate by Method:\n",
      "status       âœ…\n",
      "method        \n",
      "Concurrent  20\n",
      "Legacy      20\n",
      "Sequential  20\n",
      "\n",
      "Success percentage:\n",
      "  Concurrent        :  100.0%\n",
      "  Sequential        :  100.0%\n",
      "  Legacy            :  100.0%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 1: Combined Results Summary (with Statistics)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 1: COMBINED RESULTS SUMMARY (with Statistics)\")\n",
    "print(\"=\"*80)\n",
    "# Combine all results\n",
    "all_results = (benchmark_results_concurrent + \n",
    "               benchmark_results_sequential \n",
    "               + benchmark_results_legacy\n",
    "            )\n",
    "\n",
    "df_all_results = pd.DataFrame(all_results)\n",
    "# df_all_results.to_csv(\"raw_\")\n",
    "print(\"\\nðŸ“Š Detailed Statistics by Method:\")\n",
    "print(\"(Mean Â± Std Dev, with min/max and count)\\n\")\n",
    "\n",
    "stats_by_method = df_all_results.groupby('method').agg({\n",
    "    'time_seconds': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'time_per_polygon': ['mean', 'std'],\n",
    "    'rows_processed': 'sum'\n",
    "}).round(3)\n",
    "print(stats_by_method)\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "print(\"\\n\\nðŸ“Š 95% Confidence Intervals by Method:\")\n",
    "print(\"(Mean Â± CI)\\n\")\n",
    "\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "for method in df_all_results['method'].unique():\n",
    "    method_data = df_all_results[df_all_results['method'] == method]['time_seconds']\n",
    "    mean = method_data.mean()\n",
    "    sem = scipy_stats.sem(method_data)  # Standard error of mean\n",
    "    ci = sem * scipy_stats.t.ppf((1 + 0.95) / 2, len(method_data) - 1)\n",
    "    n = len(method_data)\n",
    "    \n",
    "    print(f\"{method:18s}: {mean:7.2f}s Â± {ci:6.2f}s (n={n}, 95% CI)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Success Rate by Method:\")\n",
    "success_rates = df_all_results.groupby('method')['status'].value_counts().unstack(fill_value=0)\n",
    "print(success_rates)\n",
    "print(\"\\nSuccess percentage:\")\n",
    "for method in df_all_results['method'].unique():\n",
    "    method_df = df_all_results[df_all_results['method'] == method]\n",
    "    success_pct = (method_df['status'] == 'âœ…').sum() / len(method_df) * 100\n",
    "    print(f\"  {method:18s}: {success_pct:6.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8ae9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # DEBUG: Print Concurrent Error Details & Endpoint Status\n",
    "# # ============================================================================\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"CONCURRENT BENCHMARK - DIAGNOSTICS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Check endpoint\n",
    "# api_url = str(ee.data._cloud_api_base_url)\n",
    "# print(f\"\\nðŸ“ Current Endpoint: {api_url}\")\n",
    "# print(f\"   Is HIGH-VOLUME: {'YES' if 'highvolume' in api_url.lower() else 'NO'}\")\n",
    "\n",
    "# # Print all errors\n",
    "# errors_found = 0\n",
    "# for i, result in enumerate(benchmark_results_concurrent):\n",
    "#     if result['status'] == 'âŒ':\n",
    "#         errors_found += 1\n",
    "#         print(f\"\\nâŒ Run {i+1}:\")\n",
    "#         print(f\"   Error: {result['error']}\")\n",
    "#         print(f\"   Config: {result['num_polygons']} polygons, {result['num_vertices']} vertices, {result['area_ha']}ha\")\n",
    "\n",
    "# if errors_found == 0:\n",
    "#     print(f\"\\nâœ… No errors found in concurrent benchmark!\")\n",
    "# else:\n",
    "#     print(f\"\\nâš ï¸  Total errors: {errors_found}/{len(benchmark_results_concurrent)}\")\n",
    "#     print(f\"\\nðŸ” Root Cause Analysis:\")\n",
    "#     if not check_ee_endpoint(\"high-volume\"):\n",
    "#         print(f\"   â†’ Using WRONG endpoint (STANDARD instead of HIGH-VOLUME)\")\n",
    "#         print(f\"   â†’ This is likely causing all concurrent tests to fail\")\n",
    "#     else:\n",
    "#         print(f\"   â†’ Endpoint is correct, check function parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c11eebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… High-volume endpoint activated for concurrent benchmark\n"
     ]
    }
   ],
   "source": [
    "# SETUP: Ensure high-volume endpoint is active for concurrent benchmark\n",
    "ee.Reset()\n",
    "ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "print(\"âœ… High-volume endpoint activated for concurrent benchmark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87e9cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSIS 2: PERFORMANCE BY INPUT SIZE\n",
      "(Averages across 5 repetitions)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ˆ Average Time by Polygon Count (Â± Std Dev):\n",
      "\n",
      "Concurrent:\n",
      "  100 polygons:   17.56s Â±   0.75s (n=5)\n",
      "  125 polygons:   17.42s Â±   2.23s (n=5)\n",
      "  150 polygons:   18.18s Â±   0.73s (n=5)\n",
      "  200 polygons:   21.49s Â±   6.02s (n=5)\n",
      "\n",
      "Sequential:\n",
      "  100 polygons:    7.66s Â±   1.00s (n=5)\n",
      "  125 polygons:    8.61s Â±   1.68s (n=5)\n",
      "  150 polygons:   10.71s Â±   1.91s (n=5)\n",
      "  200 polygons:   15.14s Â±   3.21s (n=5)\n",
      "\n",
      "Legacy:\n",
      "  100 polygons:   14.53s Â±   3.19s (n=5)\n",
      "  125 polygons:   18.07s Â±   1.33s (n=5)\n",
      "  150 polygons:   20.33s Â±   1.97s (n=5)\n",
      "  200 polygons:   26.38s Â±   2.76s (n=5)\n",
      "\n",
      "ðŸ“ˆ Average Time by Area Size (Â± Std Dev):\n",
      "\n",
      "Concurrent:\n",
      "   10ha:   18.66s Â±   3.44s (n=20)\n",
      "\n",
      "Sequential:\n",
      "   10ha:   10.53s Â±   3.53s (n=20)\n",
      "\n",
      "Legacy:\n",
      "   10ha:   19.83s Â±   4.95s (n=20)\n",
      "\n",
      "ðŸ“ˆ Average Time by Vertex Complexity (Â± Std Dev):\n",
      "\n",
      "Concurrent:\n",
      "     10 vertices:   18.66s Â±   3.44s (n=20)\n",
      "\n",
      "Sequential:\n",
      "     10 vertices:   10.53s Â±   3.53s (n=20)\n",
      "\n",
      "Legacy:\n",
      "     10 vertices:   19.83s Â±   4.95s (n=20)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 2: Performance by Input Size (Averaging Multiple Runs)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 2: PERFORMANCE BY INPUT SIZE\")\n",
    "print(\"(Averages across {0} repetitions)\".format(benchmark_params['num_repetitions']))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze by number of polygons (complexity indicator)\n",
    "print(\"\\nðŸ“ˆ Average Time by Polygon Count (Â± Std Dev):\")\n",
    "polygon_stats = df_all_results.groupby(['method', 'num_polygons'])['time_seconds'].agg(['mean', 'std', 'count'])\n",
    "for method in df_all_results['method'].unique():\n",
    "    print(f\"\\n{method}:\")\n",
    "    method_data = polygon_stats.loc[method]\n",
    "    for idx in sorted(method_data.index):\n",
    "        mean, std, count = method_data.loc[idx]\n",
    "        print(f\"  {int(idx):3d} polygons: {mean:7.2f}s Â± {std:6.2f}s (n={int(count)})\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Average Time by Area Size (Â± Std Dev):\")\n",
    "area_stats = df_all_results.groupby(['method', 'area_ha'])['time_seconds'].agg(['mean', 'std', 'count'])\n",
    "for method in df_all_results['method'].unique():\n",
    "    print(f\"\\n{method}:\")\n",
    "    method_data = area_stats.loc[method]\n",
    "    for idx in sorted(method_data.index):\n",
    "        mean, std, count = method_data.loc[idx]\n",
    "        print(f\"  {int(idx):3d}ha: {mean:7.2f}s Â± {std:6.2f}s (n={int(count)})\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Average Time by Vertex Complexity (Â± Std Dev):\")\n",
    "vertex_stats = df_all_results.groupby(['method', 'num_vertices'])['time_seconds'].agg(['mean', 'std', 'count'])\n",
    "for method in df_all_results['method'].unique():\n",
    "    print(f\"\\n{method}:\")\n",
    "    method_data = vertex_stats.loc[method]\n",
    "    for idx in sorted(method_data.index):\n",
    "        mean, std, count = method_data.loc[idx]\n",
    "        print(f\"  {int(idx):5d} vertices: {mean:7.2f}s Â± {std:6.2f}s (n={int(count)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c165e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OUTLIER DETECTION & REMOVAL (EXTREMELY CONSERVATIVE - Only CRAZY Outliers)\n",
      "================================================================================\n",
      "\n",
      "Before outlier removal:\n",
      "  Total rows: 60\n",
      "\n",
      "Concurrent:\n",
      "  CRAZY bounds (5x IQR): [10.36s, 25.82s]\n",
      "  CRAZY outliers found: 1\n",
      "    - Row 17: 32.12s (3.9Ïƒ deviation) | 200 polygons Ã— 10 vertices | Rep 3\n",
      "\n",
      "Sequential: No crazy outliers detected\n",
      "\n",
      "Legacy: No crazy outliers detected\n",
      "\n",
      "================================================================================\n",
      "âš ï¸  Removing 1 CRAZY outlier rows (>5x IQR)...\n",
      "After removal:\n",
      "  Total rows: 59\n",
      "  Rows removed: 1\n",
      "\n",
      "================================================================================\n",
      "RECALCULATED STATISTICS (WITHOUT CRAZY OUTLIERS)\n",
      "================================================================================\n",
      "           time_seconds                                rows_processed\n",
      "                  count    mean    std     min     max            sum\n",
      "method                                                               \n",
      "Concurrent           19  17.956  1.371  14.935  20.499           2675\n",
      "Legacy               20  19.830  4.951  11.164  30.128           2875\n",
      "Sequential           20  10.530  3.534   6.600  18.483           2875\n",
      "\n",
      "95% Confidence Intervals (Recalculated):\n",
      "  Concurrent        :   17.96s Â±   0.66s (n=19, 95% CI)\n",
      "  Sequential        :   10.53s Â±   1.65s (n=20, 95% CI)\n",
      "  Legacy            :   19.83s Â±   2.32s (n=20, 95% CI)\n",
      "\n",
      "âœ… Crazy outliers removed and statistics recalculated\n",
      "   Use df_all_results for updated analysis\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OUTLIER DETECTION & REMOVAL (EXTREMELY CONSERVATIVE - Only Extreme Outliers)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OUTLIER DETECTION & REMOVAL (EXTREMELY CONSERVATIVE - Only EXTREME Outliers)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify outliers using EXTREMELY conservative method (5x IQR instead of 1.5x or 3x)\n",
    "# This only catches REALLY EXTREME values - like 5-10x slower than normal\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "print(\"\\nBefore outlier removal:\")\n",
    "print(f\"  Total rows: {len(df_all_results)}\")\n",
    "\n",
    "outliers = []\n",
    "for method in df_all_results['method'].unique():\n",
    "    method_data = df_all_results[df_all_results['method'] == method]['time_seconds']\n",
    "    \n",
    "    Q1 = method_data.quantile(0.25)\n",
    "    Q3 = method_data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # Use 5x IQR (extremely conservative) - only catch extreme outliers\n",
    "    lower_bound = Q1 - 5.0 * IQR\n",
    "    upper_bound = Q3 + 5.0 * IQR\n",
    "    \n",
    "    method_outliers = df_all_results[\n",
    "        (df_all_results['method'] == method) & \n",
    "        ((df_all_results['time_seconds'] < lower_bound) | (df_all_results['time_seconds'] > upper_bound))\n",
    "    ]\n",
    "    \n",
    "    if len(method_outliers) > 0:\n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"  EXTREME bounds (5x IQR): [{lower_bound:.2f}s, {upper_bound:.2f}s]\")\n",
    "        print(f\"  EXTREME outliers found: {len(method_outliers)}\")\n",
    "        for idx, row in method_outliers.iterrows():\n",
    "            deviation = abs(row['time_seconds'] - method_data.mean()) / method_data.std()\n",
    "            print(f\"    - Row {idx}: {row['time_seconds']:.2f}s ({deviation:.1f}Ïƒ deviation) | \" +\n",
    "                  f\"{row['num_polygons']} polygons Ã— {row['num_vertices']} vertices | Rep {row['repetition']}\")\n",
    "            outliers.append(idx)\n",
    "    else:\n",
    "        print(f\"\\n{method}: No extreme outliers detected\")\n",
    "\n",
    "if outliers:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âš ï¸  Removing {len(outliers)} EXTREME outlier rows (>5x IQR)...\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_all_results_clean = df_all_results.drop(outliers).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"After removal:\")\n",
    "    print(f\"  Total rows: {len(df_all_results_clean)}\")\n",
    "    print(f\"  Rows removed: {len(outliers)}\")\n",
    "    \n",
    "    # Recalculate statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RECALCULATED STATISTICS (WITHOUT EXTREME OUTLIERS)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    stats_by_method = df_all_results_clean.groupby('method').agg({\n",
    "        'time_seconds': ['count', 'mean', 'std', 'min', 'max'],\n",
    "        'rows_processed': 'sum'\n",
    "    }).round(3)\n",
    "    print(stats_by_method)\n",
    "    \n",
    "    print(f\"\\n95% Confidence Intervals (Recalculated):\")\n",
    "    for method in df_all_results_clean['method'].unique():\n",
    "        method_data = df_all_results_clean[df_all_results_clean['method'] == method]['time_seconds']\n",
    "        mean = method_data.mean()\n",
    "        sem = scipy_stats.sem(method_data)\n",
    "        ci = sem * scipy_stats.t.ppf((1 + 0.95) / 2, len(method_data) - 1)\n",
    "        n = len(method_data)\n",
    "        print(f\"  {method:18s}: {mean:7.2f}s Â± {ci:6.2f}s (n={n}, 95% CI)\")\n",
    "    \n",
    "    # Update the main dataframe\n",
    "    df_all_results = df_all_results_clean\n",
    "    print(f\"\\nâœ… Extreme outliers removed and statistics recalculated\")\n",
    "    print(f\"   Use df_all_results for updated analysis\")\n",
    "else:\n",
    "    print(f\"\\nâœ… No extreme outliers detected - data is clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b43e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RECALCULATE ANALYSIS WITH CLEAN DATA (After Outlier Removal)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECALCULATING ANALYSIS WITH OUTLIERS REMOVED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Recalculate df_averaged from cleaned df_all_results\n",
    "df_averaged = df_all_results.groupby(['method', 'area_ha', 'num_polygons', 'num_vertices']).agg({\n",
    "    'time_seconds': ['mean', 'std', 'count'],\n",
    "    'time_per_polygon': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "df_averaged.columns = ['_'.join(col).strip() for col in df_averaged.columns.values]\n",
    "df_averaged = df_averaged.reset_index()\n",
    "\n",
    "print(f\"\\nâœ… Recalculated df_averaged with {len(df_averaged)} configurations (cleaned data)\")\n",
    "\n",
    "# Recalculate speedup analysis with cleaned data\n",
    "df_concurrent_avg = df_averaged[df_averaged['method'] == 'Concurrent'].copy()\n",
    "df_sequential_avg = df_averaged[df_averaged['method'] == 'Sequential'].copy()\n",
    "df_legacy_avg = df_averaged[df_averaged['method'] == 'Legacy'].copy()\n",
    "\n",
    "# Set indices for merging\n",
    "df_concurrent_avg = df_concurrent_avg.set_index(['area_ha', 'num_polygons', 'num_vertices'])\n",
    "df_sequential_avg = df_sequential_avg.set_index(['area_ha', 'num_polygons', 'num_vertices'])\n",
    "df_legacy_avg = df_legacy_avg.set_index(['area_ha', 'num_polygons', 'num_vertices'])\n",
    "\n",
    "# Calculate speedup from averaged times (CLEANED)\n",
    "speedup_data = []\n",
    "for idx in df_concurrent_avg.index:\n",
    "    if idx in df_sequential_avg.index and idx in df_legacy_avg.index:\n",
    "        concurrent_time = df_concurrent_avg.loc[idx, 'time_seconds_mean']\n",
    "        sequential_time = df_sequential_avg.loc[idx, 'time_seconds_mean']\n",
    "        legacy_time = df_legacy_avg.loc[idx, 'time_seconds_mean']\n",
    "        \n",
    "        speedup_data.append({\n",
    "            'area_ha': idx[0],\n",
    "            'num_polygons': idx[1],\n",
    "            'num_vertices': idx[2],\n",
    "            'concurrent_mean': concurrent_time,\n",
    "            'sequential_mean': sequential_time,\n",
    "            'legacy_mean': legacy_time,\n",
    "            'speedup_vs_sequential': sequential_time / concurrent_time if concurrent_time > 0 else 0,\n",
    "            'speedup_vs_legacy': legacy_time / concurrent_time if concurrent_time > 0 else 0,\n",
    "        })\n",
    "\n",
    "df_speedup = pd.DataFrame(speedup_data)\n",
    "print(f\"âœ… Recalculated df_speedup with {len(df_speedup)} speedup comparisons (cleaned data)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Speedup Summary (Cleaned Data):\")\n",
    "print(f\"  vs Sequential: {df_speedup['speedup_vs_sequential'].mean():.2f}x average\")\n",
    "print(f\"  vs Legacy:     {df_speedup['speedup_vs_legacy'].mean():.2f}x average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 5: Visualizations (Including Error Bars from Repetitions)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS 5: CREATING VISUALIZATIONS (with Error Bars)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Time vs Polygon Count (with error bars)\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "for method in sorted(df_averaged['method'].unique()):\n",
    "    data = df_averaged[df_averaged['method'] == method].groupby('num_polygons').agg({\n",
    "        'time_seconds_mean': 'mean',\n",
    "        'time_seconds_std': 'mean'\n",
    "    }).reset_index()\n",
    "    ax1.errorbar(data['num_polygons'], data['time_seconds_mean'], \n",
    "                yerr=data['time_seconds_std'], marker='o', label=method, linewidth=2, capsize=5)\n",
    "ax1.set_xlabel('Number of Polygons')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_title('Processing Time vs Polygon Count (with Std Dev)')\n",
    "ax1.set_xscale('log')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Time vs Vertex Complexity (with error bars)\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "for method in sorted(df_averaged['method'].unique()):\n",
    "    data = df_averaged[df_averaged['method'] == method].groupby('num_vertices').agg({\n",
    "        'time_seconds_mean': 'mean',\n",
    "        'time_seconds_std': 'mean'\n",
    "    }).reset_index()\n",
    "    ax2.errorbar(data['num_vertices'], data['time_seconds_mean'], \n",
    "                yerr=data['time_seconds_std'], marker='s', label=method, linewidth=2, capsize=5)\n",
    "ax2.set_xlabel('Number of Vertices per Polygon')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Processing Time vs Vertex Complexity (with Std Dev)')\n",
    "ax2.set_xscale('log')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Time vs Area Size (with error bars)\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "for method in sorted(df_averaged['method'].unique()):\n",
    "    data = df_averaged[df_averaged['method'] == method].groupby('area_ha').agg({\n",
    "        'time_seconds_mean': 'mean',\n",
    "        'time_seconds_std': 'mean'\n",
    "    }).reset_index()\n",
    "    ax3.errorbar(data['area_ha'], data['time_seconds_mean'], \n",
    "                yerr=data['time_seconds_std'], marker='^', label=method, linewidth=2, capsize=5)\n",
    "ax3.set_xlabel('Area (hectares)')\n",
    "ax3.set_ylabel('Time (seconds)')\n",
    "ax3.set_title('Processing Time vs Area Size (with Std Dev)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Variability comparison (coefficient of variation)\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "variability_data = []\n",
    "for method in sorted(df_averaged['method'].unique()):\n",
    "    method_times = df_averaged[df_averaged['method'] == method]['time_seconds_mean']\n",
    "    cv = (method_times.std() / method_times.mean()) * 100\n",
    "    variability_data.append({'Method': method, 'CV (%)': cv})\n",
    "variability_df = pd.DataFrame(variability_data)\n",
    "bars = ax4.bar(variability_df['Method'], variability_df['CV (%)'], color=['steelblue', 'coral', 'green'])\n",
    "ax4.set_ylabel('Coefficient of Variation (%)')\n",
    "ax4.set_title('Consistency: Lower CV = More Predictable')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# 5. Speedup vs Sequential\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "speedup_by_polygons = df_speedup.groupby('num_polygons')['speedup_vs_sequential'].mean()\n",
    "bars = ax5.bar(range(len(speedup_by_polygons)), speedup_by_polygons.values, color='steelblue')\n",
    "ax5.set_xticks(range(len(speedup_by_polygons)))\n",
    "ax5.set_xticklabels(speedup_by_polygons.index)\n",
    "ax5.set_xlabel('Number of Polygons')\n",
    "ax5.set_ylabel('Speedup (x)')\n",
    "ax5.set_title('Concurrent Speedup vs Sequential')\n",
    "ax5.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "# 6. Speedup vs Legacy\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "speedup_by_polygons_legacy = df_speedup.groupby('num_polygons')['speedup_vs_legacy'].mean()\n",
    "bars = ax6.bar(range(len(speedup_by_polygons_legacy)), speedup_by_polygons_legacy.values, color='coral')\n",
    "ax6.set_xticks(range(len(speedup_by_polygons_legacy)))\n",
    "ax6.set_xticklabels(speedup_by_polygons_legacy.index)\n",
    "ax6.set_xlabel('Number of Polygons')\n",
    "ax6.set_ylabel('Speedup (x)')\n",
    "ax6.set_title('Concurrent Speedup vs Legacy')\n",
    "ax6.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "# 7. Distribution of all runs (violin plot)\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "parts = ax7.violinplot([df_all_results[df_all_results['method'] == m]['time_seconds'].values \n",
    "                        for m in sorted(df_all_results['method'].unique())],\n",
    "                       positions=range(len(df_all_results['method'].unique())),\n",
    "                       showmeans=True, showmedians=True)\n",
    "ax7.set_xticks(range(len(df_all_results['method'].unique())))\n",
    "ax7.set_xticklabels(sorted(df_all_results['method'].unique()))\n",
    "ax7.set_ylabel('Time (seconds)')\n",
    "ax7.set_title('Distribution of All Runs (Violin Plot)')\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 8. Box plot by method\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "box_data = [df_all_results[df_all_results['method'] == m]['time_seconds'].values \n",
    "            for m in sorted(df_all_results['method'].unique())]\n",
    "bp = ax8.boxplot(box_data, labels=sorted(df_all_results['method'].unique()), patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['steelblue', 'coral', 'green']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax8.set_ylabel('Time (seconds)')\n",
    "ax8.set_title('Time Distribution by Method (Box Plot)')\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 9. Heatmap: Mean time by Polygons and Method\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "pivot_by_method = df_averaged.pivot_table(\n",
    "    values='time_seconds_mean',\n",
    "    index='num_polygons',\n",
    "    columns='method',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(pivot_by_method, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=ax9, cbar_kws={'label': 'Time (s)'})\n",
    "ax9.set_title('Mean Time by Method and Polygon Count')\n",
    "\n",
    "plt.suptitle('WHISP Stats Processing Benchmark - Statistical Analysis ({} repetitions each)'.format(benchmark_params['num_repetitions']), \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualizations created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38033033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPORT RESULTS (Raw Data + Summary Statistics)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORTING RESULTS TO DOWNLOADS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. Export raw results (all individual runs)\n",
    "raw_df = df_all_results.copy()\n",
    "raw_filename = downloads_path / f\"benchmark_results_raw_{timestamp}.csv\"\n",
    "raw_df.to_csv(raw_filename, index=False)\n",
    "print(f\"\\nðŸ’¾ Raw results (all {len(raw_df)} runs): {raw_filename}\")\n",
    "\n",
    "# 2. Export averaged results (per configuration)\n",
    "avg_export_df = df_averaged.copy()\n",
    "avg_export_df = avg_export_df.sort_values(['method', 'num_polygons', 'num_vertices', 'area_ha'])\n",
    "avg_filename = downloads_path / f\"benchmark_results_averaged_{timestamp}.csv\"\n",
    "avg_export_df.to_csv(avg_filename, index=False)\n",
    "print(f\"ðŸ’¾ Averaged results ({len(avg_export_df)} configurations): {avg_filename}\")\n",
    "\n",
    "# 3. Export speedup analysis\n",
    "speedup_filename = downloads_path / f\"benchmark_speedup_analysis_{timestamp}.csv\"\n",
    "df_speedup.to_csv(speedup_filename, index=False)\n",
    "print(f\"ðŸ’¾ Speedup analysis: {speedup_filename}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nðŸ“Š Results Summary:\")\n",
    "print(f\"  Total raw test runs: {len(raw_df)}\")\n",
    "print(f\"    - Concurrent:   {len(raw_df[raw_df['method'] == 'Concurrent'])}\")\n",
    "print(f\"    - Sequential:   {len(raw_df[raw_df['method'] == 'Sequential'])}\")\n",
    "print(f\"    - Legacy:       {len(raw_df[raw_df['method'] == 'Legacy'])}\")\n",
    "print(f\"  Total configurations: {len(avg_export_df)}\")\n",
    "print(f\"  Repetitions per config: {benchmark_params['num_repetitions']}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Output Directory:\")\n",
    "print(f\"  ðŸ“ {downloads_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Raw Data Preview (first 10 runs):\")\n",
    "print(raw_df[['method', 'area_ha', 'num_polygons', 'num_vertices', 'repetition', 'time_seconds', 'status']].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nðŸ“Š Averaged Data Preview (first 10 configs):\")\n",
    "print(avg_export_df[['method', 'area_ha', 'num_polygons', 'num_vertices', 'time_seconds_mean', 'time_seconds_std', 'time_seconds_count']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f89af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb61a7d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
