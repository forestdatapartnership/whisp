{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### Main changes\n",
    "- Using selfMask() to avoid empty pixels (a lot of bands are sparse)\n",
    "- Skipping the validation step – panderas should be fast but there is some temp schema generation that takes time \n",
    "- Using high volume end point and concurrent processing\n",
    "- Using reduceRegions instead of mapped reduceRegion - a chunk of code for choosing ha or percent etc is based on using reduceRegion and it also allowed to skip\n",
    "- Skipping the use of points to get the admin details (country and level 1 info) and water_flag (should be based on image but was using vector admin still)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Reset Earth Engine completely\n",
    "ee.Reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Engine and Common Libraries\n",
    "import ee\n",
    "from pathlib import Path\n",
    "\n",
    "# Authenticate and initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')  # Try to use existing credentials first\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EE Data Base URL:\", ee.data._cloud_api_base_url)\n",
    "print(\"EE API Base URL:\", ee.data._api_base_url)\n",
    "\n",
    "# Check if using standard endpoint\n",
    "if 'highvolume' in str(ee.data._cloud_api_base_url):\n",
    "    print(\"✅ Using HIGH-VOLUME endpoint\")\n",
    "else:\n",
    "    print(\"❌ Using STANDARD endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --pre openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reducer = ee.Reducer.sum().combine(ee.Reducer.median(),sharedInputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openforis_whisp as whisp\n",
    "import tempfile\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from openforis_whisp.parameters.config_runtime import (\n",
    "    plot_id_column,\n",
    "    admin_1_column, \n",
    "    iso3_country_column, \n",
    "    iso2_country_column, \n",
    "    water_flag,\n",
    "    geometry_type_column, \n",
    "    geometry_area_column,\n",
    "    centroid_x_coord_column, \n",
    "    centroid_y_coord_column\n",
    ")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import the lookup dictionary\n",
    "from src.openforis_whisp.parameters.lookup_gaul1_admin import (\n",
    "    lookup_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging ONCE - avoid duplicate handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING, \n",
    "    format='%(levelname)s: %(message)s',\n",
    "    stream=sys.stdout,\n",
    "    force=True\n",
    "    )\n",
    "logger = logging.getLogger(\"whisp-batch\")\n",
    "\n",
    "logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)\n",
    "logging.getLogger('googleapiclient').setLevel(logging.WARNING)\n",
    "\n",
    "EE_MAX_CONCURRENT = 10\n",
    "EE_FEATURES_PER_BATCH = 25\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def join_admin_codes(df, lookup_dict, id_col='id_col'):\n",
    "    \"\"\"\n",
    "    Join admin names and ISO3 codes to a DataFrame using a lookup dictionary.\n",
    "    Output columns are named using config_runtime.py variables.\n",
    "    \"\"\"\n",
    "    from openforis_whisp.parameters.config_runtime import (\n",
    "        admin_1_column, \n",
    "        iso3_country_column, \n",
    "        iso2_country_column\n",
    "    )\n",
    "    \n",
    "    lookup_df = pd.DataFrame.from_dict(lookup_dict, orient='index')\n",
    "    lookup_df.index.name = 'gaul1_code'\n",
    "    lookup_df = lookup_df.reset_index()\n",
    "    # Ensure gaul1_code is int32 for join\n",
    "    lookup_df['gaul1_code'] = lookup_df['gaul1_code'].fillna(-9999).astype('int32')\n",
    "    # Ensure df[id_col] is int32 for join\n",
    "    df = df.copy()\n",
    "    df['id_col_int'] = df[id_col].fillna(-9999).astype('int32')\n",
    "    merged_df = df.merge(lookup_df, left_on=\"id_col_int\", right_on='gaul1_code', how='left')\n",
    "    merged_df = merged_df.rename(columns={\n",
    "        'gaul1_name': admin_1_column,\n",
    "        'iso3_code': iso3_country_column,\n",
    "        'iso2_code': iso2_country_column\n",
    "    })\n",
    "    merged_df = merged_df.drop(columns=['gaul1_code', \"gaul0_name\", \"id_col_int\",id_col], errors='ignore')\n",
    "    return merged_df\n",
    "\n",
    "# Usage:\n",
    "# result_df = join_admin_codes(example_df, lookup_dict, id_col='id_col')\n",
    "\n",
    "def ee_extract_centroid_and_geomtype(fc, x_col='centroid_x', y_col='centroid_y', type_col='geometry_type',max_error=1):\n",
    "    def add_centroid_and_geomtype(feature):\n",
    "        centroid = feature.geometry().centroid(max_error)\n",
    "        coords = centroid.coordinates()\n",
    "        # Round coordinates to 6 decimal places (Earth Engine's round only takes 1 argument)\n",
    "        x = ee.Number(coords.get(0)).multiply(1e6).round().divide(1e6)\n",
    "        y = ee.Number(coords.get(1)).multiply(1e6).round().divide(1e6)\n",
    "        return feature.set({\n",
    "            x_col: x,\n",
    "            y_col: y,\n",
    "            type_col: feature.geometry().type()\n",
    "        })\n",
    "    return fc.map(add_centroid_and_geomtype)\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract centroid, geometry type, and coordinates from a GeoDataFrame using GeoPandas (faster for local data)\n",
    "\n",
    "def gpd_extract_centroid_and_geomtype(\n",
    "    gdf,\n",
    "    x_col='centroid_x',\n",
    "    y_col='centroid_y',\n",
    "    type_col='geometry_type',\n",
    "    external_id_col=None,\n",
    "    return_attributes_only=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Adds centroid coordinate values and geometry type columns to a GeoDataFrame.\n",
    "    Optionally returns only attributes (no geometry columns) and preserves an external ID column.\n",
    "    Does NOT add a centroid geometry column, only the values for lon/lat/type.\n",
    "    Args:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame.\n",
    "        x_col (str): Name for centroid x column.\n",
    "        y_col (str): Name for centroid y column.\n",
    "        type_col (str): Name for geometry type column.\n",
    "        external_id_col (str, optional): Name of external ID column to preserve in output.\n",
    "        return_attributes_only (bool, optional): If True, returns a pandas DataFrame with only attributes (no geometry columns).\n",
    "    Returns:\n",
    "        GeoDataFrame or DataFrame: Copy with new columns for centroid_x, centroid_y, and geometry_type, optionally only attributes.\n",
    "    \"\"\"\n",
    "    gdf = gdf.copy()\n",
    "    # Calculate centroid coordinates as values only, not as a geometry column\n",
    "    centroid_points = gdf.geometry.centroid\n",
    "    gdf[x_col] = centroid_points.x.round(6)\n",
    "    gdf[y_col] = centroid_points.y.round(6)\n",
    "    gdf[type_col] = gdf.geometry.geom_type\n",
    "    cols = [x_col, y_col, type_col]\n",
    "    if external_id_col and external_id_col in gdf.columns:\n",
    "        cols = [external_id_col] + cols\n",
    "    if return_attributes_only:\n",
    "        df = gdf[cols].reset_index(drop=True)\n",
    "        return df\n",
    "    return gdf\n",
    "\n",
    "# Example usage:\n",
    "# gdf = gpd.read_file(GEOJSON_EXAMPLE_FILEPATH)\n",
    "# gdf_with_centroids = gpd_extract_centroid_and_geomtype(gdf, return_attributes_only=True)\n",
    "# print(gdf_with_centroids[[\"centroid_x\", \"centroid_y\", \"geometry_type\"]].head())\n",
    "\n",
    "def format_stats_dataframe(\n",
    "    df,\n",
    "    area_col='Area_sum',\n",
    "    decimal_places=2,\n",
    "    unit_type='ha',\n",
    "    stats_unit_type_column='Unit',\n",
    "    strip_suffix='_sum',\n",
    "    remove_columns=True,\n",
    "    remove_columns_suffix='_median',\n",
    "    convert_water_flag=True,\n",
    "    water_flag_column='In_waterbody_sum',\n",
    "    water_flag_threshold=0.5,\n",
    "    sort_column=\"plotId\" \n",
    "):\n",
    "    \"\"\"Flexible stats formatting for DataFrame columns.\n",
    "\n",
    "    - Converts columns ending with `strip_suffix` (default '_sum') to hectares or percent.\n",
    "    - Removes columns ending with `remove_columns_suffix` (default '_median') if `remove_columns` is True.\n",
    "    - Optionally converts a water-flag stat into a boolean column based on the threshold compared to `area_col`.\n",
    "    - Strips the `strip_suffix` from produced stat column names (so 'Cocoa_sum' -> 'Cocoa').\n",
    "    - Fills `stats_unit_type_column` with `unit_type` for every row.\n",
    "\n",
    "    Returns a new DataFrame (copy) with conversions applied. Helper sub-functions are used for clarity\n",
    "    and to avoid fragmenting the original DataFrame (we build new columns and concat once).\n",
    "    \"\"\"\n",
    "    # Helper: find stat columns that end with the strip_suffix (and are not the area_col)\n",
    "    def _collect_stat_columns(columns, strip_suffix, area_col):\n",
    "        cols = [c for c in columns if c.endswith(strip_suffix) and c != area_col]\n",
    "        return cols\n",
    "\n",
    "    # Helper: drop columns with a given suffix\n",
    "    def _drop_suffix_columns(df, suffix):\n",
    "        if suffix is None or suffix == '':\n",
    "            return df\n",
    "        return df.loc[:, ~df.columns.str.endswith(suffix)]\n",
    "\n",
    "    # Helper: build converted stats (returns DataFrame of new columns indexed same as df)\n",
    "    def _build_converted_stats(df, stat_cols, area_col, unit_type, decimal_places, strip_suffix):\n",
    "        area = df[area_col].replace(0, float('nan'))\n",
    "        new = {}\n",
    "        for col in stat_cols:\n",
    "            base = col[:-len(strip_suffix)] if strip_suffix and col.endswith(strip_suffix) else col\n",
    "            if unit_type == 'ha':\n",
    "                # value is in whatever units the sum uses (ee outputs square meters) -> convert to hectares\n",
    "                # (user earlier used divide by 10000 pattern)\n",
    "                new[base] = (df[col] / 10000).round(decimal_places)\n",
    "            elif unit_type == 'percent':\n",
    "                new[base] = ((df[col] / area) * 100).round(decimal_places)\n",
    "            else:\n",
    "                # unknown unit type: just copy the raw sums\n",
    "                new[base] = df[col].round(decimal_places)\n",
    "        df[area_col] = (df[area_col]/ 10000).round(decimal_places)\n",
    "        return pd.DataFrame(new, index=df.index)\n",
    "\n",
    "    # Helper: convert water flag stat (if present) into bool by thresholding water_area / total_area\n",
    "    def _apply_water_flag(df, water_flag_column, strip_suffix, area_col, threshold):\n",
    "        # possible names for water stat: exact provided name, name+suffix\n",
    "        candidates = []\n",
    "        if water_flag_column in df.columns:\n",
    "            candidates.append(water_flag_column)\n",
    "        suffixed = water_flag_column + strip_suffix if strip_suffix else None\n",
    "        if suffixed and suffixed in df.columns:\n",
    "            candidates.append(suffixed)\n",
    "        # also check generic 'water' candidates\n",
    "        if 'water' + strip_suffix in df.columns:\n",
    "            candidates.append('water' + strip_suffix)\n",
    "        if not candidates:\n",
    "            # nothing to do\n",
    "            return df\n",
    "        # pick first available candidate\n",
    "        water_col = candidates[0]\n",
    "        total_area = df[area_col].replace(0, float('nan'))\n",
    "        # compute ratio\n",
    "        ratio = df[water_col] / total_area\n",
    "        df[water_flag_column] = (ratio > threshold).astype(bool)\n",
    "        return df\n",
    "\n",
    "    # 1) Work on a shallow copy to avoid mutating caller inplace accidentally\n",
    "    df = df.copy()\n",
    "\n",
    "    # 2) Optionally drop median (or other) columns\n",
    "    if remove_columns and remove_columns_suffix:\n",
    "        df = _drop_suffix_columns(df, remove_columns_suffix)\n",
    "\n",
    "    # 3) Collect stat columns to convert (those ending with strip_suffix and not equal to area_col)\n",
    "    stat_cols = _collect_stat_columns(df.columns, strip_suffix, area_col)\n",
    "\n",
    "    # 4) Build converted stats DataFrame (these will have suffix removed as column names)\n",
    "    if stat_cols:\n",
    "        converted_stats_df = _build_converted_stats(df, stat_cols, area_col, unit_type, decimal_places, strip_suffix)\n",
    "    else:\n",
    "        converted_stats_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 5) Remove original stat columns (the ones with strip_suffix) from df (but keep area_col)\n",
    "    df = df.loc[:, [c for c in df.columns if not (c.endswith(strip_suffix) and c != area_col)]]\n",
    "\n",
    "    # 6) Concatenate converted stats into df in one go to avoid fragmentation\n",
    "    if not converted_stats_df.empty:\n",
    "        df = pd.concat([df, converted_stats_df], axis=1)\n",
    "\n",
    "    # 7) Fill stats unit type column\n",
    "    df[stats_unit_type_column] = unit_type\n",
    "\n",
    "    # 8) Optionally convert water flag to boolean\n",
    "    if convert_water_flag:\n",
    "        df = _apply_water_flag(df, water_flag_column, strip_suffix, area_col, water_flag_threshold)\n",
    "\n",
    "    # 9) rename area_col by stripping suffix from area_col\n",
    "    area_col_stripped = area_col[:-len(strip_suffix)] if area_col.endswith(strip_suffix) else area_col    \n",
    "    df.rename(columns={area_col:area_col_stripped},inplace=True)\n",
    "\n",
    "    # 10) \"\n",
    "    # reorder by plotId column if present\n",
    "    df = df.sort_values(sort_column).reset_index(drop=True) if sort_column in df.columns else df\n",
    "\n",
    "    # 11) Defragment final DataFrame and return\n",
    "    return df.copy()\n",
    "\n",
    "\n",
    "def clean_geodataframe(gdf: gpd.GeoDataFrame, remove_nulls: bool = True, fix_invalid: bool = True) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Validate and optionally clean a GeoDataFrame's geometries.\"\"\"\n",
    "    if remove_nulls:\n",
    "        null_count = gdf.geometry.isna().sum()\n",
    "        if null_count > 0:\n",
    "            print(f\"⚠️  Found {null_count} null geometries - removing...\", flush=True)\n",
    "            gdf = gdf[~gdf.geometry.isna()]\n",
    "    if fix_invalid:\n",
    "        valid_count = gdf.geometry.is_valid.sum()\n",
    "        invalid_count = len(gdf) - valid_count\n",
    "        if invalid_count > 0:\n",
    "            print(f\"⚠️  Found {invalid_count} invalid geometries - fixing...\", flush=True)\n",
    "            from shapely.validation import make_valid\n",
    "            gdf['geometry'] = gdf['geometry'].apply(lambda g: make_valid(g) if g and not g.is_valid else g)\n",
    "    print(f\"✅ Validation complete. {len(gdf):,} geometries ready.\", flush=True)\n",
    "    return gdf\n",
    "\n",
    "def batch_geodataframe(gdf: gpd.GeoDataFrame, batch_size: int) -> List[gpd.GeoDataFrame]:\n",
    "    \"\"\"Split a GeoDataFrame into batches of given size.\"\"\"\n",
    "    return [gdf.iloc[i:i+batch_size] for i in range(0, len(gdf), batch_size)]\n",
    "\n",
    "def convert_batch_to_ee(batch_gdf: gpd.GeoDataFrame) -> ee.FeatureCollection:\n",
    "    \"\"\"Convert a batch GeoDataFrame to an Earth Engine FeatureCollection using whisp.\"\"\"\n",
    "    temp_fd, temp_geojson_path = tempfile.mkstemp(suffix='.geojson', text=True)\n",
    "    try:\n",
    "        os.close(temp_fd)\n",
    "        batch_gdf.to_file(temp_geojson_path, driver='GeoJSON')\n",
    "        fc = whisp.convert_geojson_to_ee(temp_geojson_path)\n",
    "        return fc\n",
    "    finally:\n",
    "        time.sleep(0.1)\n",
    "        if os.path.exists(temp_geojson_path):\n",
    "            try:\n",
    "                os.unlink(temp_geojson_path)\n",
    "            except OSError as cleanup_error:\n",
    "                logger.warning(f\"Could not delete temp file {temp_geojson_path}: {cleanup_error}\")\n",
    "\n",
    "def process_ee_feature_collection(feature_collection: ee.FeatureCollection, whisp_image: ee.Image, reducer: ee.Reducer, batch_idx: int, max_retries: int = MAX_RETRIES) -> pd.DataFrame:\n",
    "    \"\"\"Process an EE FeatureCollection with retry logic and return a DataFrame.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            results = whisp_image.reduceRegions(\n",
    "                collection=feature_collection,\n",
    "                reducer=reducer,\n",
    "                scale=10\n",
    "            )\n",
    "            df_result = whisp.convert_ee_to_df(results)\n",
    "            return df_result\n",
    "        except ee.EEException as e:\n",
    "            error_msg = str(e)\n",
    "            if \"Unable to transform geometry\" in error_msg:\n",
    "                raise Exception(f\"Geometry transformation error in batch {batch_idx + 1}: {error_msg}\")\n",
    "            elif \"Quota\" in error_msg or \"limit\" in error_msg.lower():\n",
    "                if attempt < max_retries - 1:\n",
    "                    backoff = min(30, 2 ** attempt)\n",
    "                    print(f\"⏳ Quota/rate limit hit, waiting {backoff}s before retry...\", flush=True)\n",
    "                    time.sleep(backoff)\n",
    "                else:\n",
    "                    raise Exception(f\"Quota/rate limit exhausted for batch {batch_idx + 1}\")\n",
    "            elif \"timeout\" in error_msg.lower():\n",
    "                if attempt < max_retries - 1:\n",
    "                    backoff = min(15, 2 ** attempt)\n",
    "                    print(f\"⏳ Timeout, retrying in {backoff}s...\", flush=True)\n",
    "                    time.sleep(backoff)\n",
    "                else:\n",
    "                    raise e\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    backoff = min(10, 2 ** attempt)\n",
    "                    time.sleep(backoff)\n",
    "                else:\n",
    "                    raise e\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                backoff = min(5, 2 ** attempt)\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                raise e\n",
    "    raise RuntimeError(f\"Failed to process batch {batch_idx + 1} after {max_retries} attempts\")\n",
    "\n",
    "# def process_geojson_file(\n",
    "#     geojson_path: str,\n",
    "#     whisp_image: ee.Image,\n",
    "#     reducer: ee.Reducer,\n",
    "#     batch_size: int = EE_FEATURES_PER_BATCH,\n",
    "#     max_concurrent: int = EE_MAX_CONCURRENT,\n",
    "#     validate_null_geometries: bool = True,\n",
    "#     validate_invalid_geometries: bool = True,\n",
    "#     max_retries: int = MAX_RETRIES,\n",
    "#     add_metadata_gpd: bool = False,\n",
    "#     add_metadata_ee: bool = True,\n",
    "\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Main function to process a GeoJSON file in batches using Whisp and EE.\"\"\"\n",
    "#     print(f\"🔍 Loading and validating GeoJSON file...\", flush=True)\n",
    "#     gdf = gpd.read_file(geojson_path)\n",
    "#     print(f\"📁 Loaded {len(gdf):,} features from {geojson_path}\", flush=True)\n",
    "#     gdf = clean_geodataframe(gdf, remove_nulls=validate_null_geometries, fix_invalid=validate_invalid_geometries)\n",
    "#     gdf = gpd.read_file(GEOJSON_EXAMPLE_FILEPATH)\n",
    "#     if add_metadata_gpd:\n",
    "#         gdf_reproj = gdf#.to_crs(epsg=6933)  # Reproject to equal area if necessary\n",
    "#         gdf_w_metadata = gpd_extract_centroid_and_geomtype(gdf_reproj, x_col=centroid_x_coord_column, y_col=centroid_y_coord_column, type_col=geometry_type_column)\n",
    "#         gdf_w_metadata_unproj = gdf_w_metadata#.to_crs(epsg=4326) \n",
    "#         batches = batch_geodataframe(gdf_w_metadata_unproj, batch_size)\n",
    "#     else:\n",
    "#         batches = batch_geodataframe(gdf, batch_size)\n",
    "#     print(f\"📊 Processing {len(gdf):,} features in {len(batches)} batches ({batch_size} features/batch)\", flush=True)\n",
    "#     print(f\"🔄 Running {max_concurrent} concurrent requests...\", flush=True)\n",
    "#     results = []\n",
    "    \n",
    "#     def process_one_batch(batch_gdf, batch_idx):\n",
    "#         fc = convert_batch_to_ee(batch_gdf)\n",
    "#         if add_metadata_ee:\n",
    "#             fc = ee_extract_centroid_and_geomtype(fc,x_col=centroid_x_coord_column, y_col=centroid_y_coord_column, type_col=geometry_type_column,max_error=0.1)\n",
    "#         else:\n",
    "#             fc\n",
    "#         return process_ee_feature_collection(fc, whisp_image, reducer, batch_idx, max_retries)\n",
    "#     with ThreadPoolExecutor(max_workers=max_concurrent) as executor:\n",
    "#         future_to_idx = {executor.submit(process_one_batch, batch, i): i for i, batch in enumerate(batches)}\n",
    "#         for future in as_completed(future_to_idx):\n",
    "#             batch_idx = future_to_idx[future]\n",
    "#             try:\n",
    "#                 batch_result = future.result()\n",
    "#                 results.append(batch_result)\n",
    "#                 print(f\"⏳ Progress: Batch {batch_idx + 1} ✓\", flush=True)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"❌ Batch {batch_idx + 1} failed: {str(e)[:80]}...\", flush=True)\n",
    "#     if results:\n",
    "#         combined_df = pd.concat(results, ignore_index=True)\n",
    "#         return combined_df\n",
    "#     else:\n",
    "#         print(\"❌ No results produced - all batches failed\", flush=True)\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Global semaphore to strictly limit concurrent EE calls\n",
    "# (so we can safely use more worker threads than EE concurrency)\n",
    "EE_SEMAPHORE = threading.BoundedSemaphore(EE_MAX_CONCURRENT)\n",
    "\n",
    "def _client_side_metadata(batch_gdf: gpd.GeoDataFrame,\n",
    "                          row_id_col: str,\n",
    "                          x_col: str,\n",
    "                          y_col: str,\n",
    "                          type_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Client-side: compute centroid_x/centroid_y/geom_type using GeoPandas for this batch.\n",
    "    Returns a pandas DataFrame with [row_id_col, x_col, y_col, type_col].\n",
    "    \"\"\"\n",
    "    # Ensure only needed columns are carried across to reduce memory use\n",
    "    meta_df = gpd_extract_centroid_and_geomtype(\n",
    "        batch_gdf,\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        type_col=type_col,\n",
    "        external_id_col=row_id_col,\n",
    "        return_attributes_only=True\n",
    "    )\n",
    "    return meta_df\n",
    "\n",
    "def _server_side_batch(batch_gdf: gpd.GeoDataFrame,\n",
    "                       batch_idx: int,\n",
    "                       whisp_image: ee.Image,\n",
    "                       reducer: ee.Reducer,\n",
    "                       max_retries: int,\n",
    "                       add_metadata_ee: bool,\n",
    "                       x_col: str,\n",
    "                       y_col: str,\n",
    "                       type_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Server-side: convert to EE, (optionally) add centroid/type server-side, run reduceRegions,\n",
    "    return the converted DataFrame for this batch.\n",
    "    \"\"\"\n",
    "    # Limit EE concurrency\n",
    "    with EE_SEMAPHORE:\n",
    "        fc = convert_batch_to_ee(batch_gdf)\n",
    "        if add_metadata_ee:\n",
    "            fc = ee_extract_centroid_and_geomtype(\n",
    "                fc,\n",
    "                x_col=x_col,\n",
    "                y_col=y_col,\n",
    "                type_col=type_col,\n",
    "                max_error=0.1\n",
    "            )\n",
    "        df_result = process_ee_feature_collection(fc, whisp_image, reducer, batch_idx, max_retries)\n",
    "        return df_result\n",
    "\n",
    "def process_geojson_file(\n",
    "    geojson_path: str,\n",
    "    whisp_image: ee.Image,\n",
    "    reducer: ee.Reducer,\n",
    "    batch_size: int = EE_FEATURES_PER_BATCH,\n",
    "    max_concurrent: int = EE_MAX_CONCURRENT,\n",
    "    validate_null_geometries: bool = True,\n",
    "    validate_invalid_geometries: bool = True,\n",
    "    max_retries: int = MAX_RETRIES,\n",
    "    add_metadata_gpd: bool = False,     # kept for compatibility; now client metadata is always computed for merge\n",
    "    add_metadata_ee: bool = False,\n",
    "    row_id_col: str = \"__row_id__\",\n",
    "    x_col: str = None,\n",
    "    y_col: str = None,\n",
    "    type_col: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a GeoJSON in concurrent batches. For each batch, run EE (server-side) and GeoPandas\n",
    "    (client-side) work concurrently, then merge the results.\n",
    "\n",
    "    - EE concurrency is limited by `max_concurrent` via a semaphore.\n",
    "    - Thread pool can exceed `max_concurrent` so client work proceeds even if EE slots are busy.\n",
    "\n",
    "    Args for centroid/type columns:\n",
    "      - If x_col/y_col/type_col are None, they fall back to config variables:\n",
    "        centroid_x_coord_column / centroid_y_coord_column / geometry_type_column\n",
    "    \"\"\"\n",
    "    # Resolve column names from config if not supplied\n",
    "    global centroid_x_coord_column, centroid_y_coord_column, geometry_type_column\n",
    "    x_col = x_col or centroid_x_coord_column\n",
    "    y_col = y_col or centroid_y_coord_column\n",
    "    type_col = type_col or geometry_type_column\n",
    "\n",
    "    print(f\"🔍 Loading and validating GeoJSON file...\", flush=True)\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    print(f\"📁 Loaded {len(gdf):,} features from {geojson_path}\", flush=True)\n",
    "\n",
    "    gdf = clean_geodataframe(\n",
    "        gdf,\n",
    "        remove_nulls=validate_null_geometries,\n",
    "        fix_invalid=validate_invalid_geometries\n",
    "    )\n",
    "\n",
    "    # Stable row id to join server and client results\n",
    "    if row_id_col in gdf.columns:\n",
    "        # If it already exists, ensure uniqueness; otherwise, recreate\n",
    "        if gdf[row_id_col].duplicated().any():\n",
    "            gdf = gdf.reset_index(drop=True)\n",
    "            gdf[row_id_col] = gdf.index.astype(\"int64\")\n",
    "    else:\n",
    "        gdf = gdf.reset_index(drop=True)\n",
    "        gdf[row_id_col] = gdf.index.astype(\"int64\")\n",
    "\n",
    "    # Important: keep row_id property into EE by writing it to GeoJSON; convert_batch_to_ee reads props\n",
    "    # (convert_batch_to_ee writes a temp geojson, so the property is preserved)\n",
    "\n",
    "    batches = batch_geodataframe(gdf, batch_size)\n",
    "    print(f\"📊 Processing {len(gdf):,} features in {len(batches)} batches ({batch_size} features/batch)\", flush=True)\n",
    "\n",
    "    # We can use more workers than EE concurrency because EE calls are gated by the semaphore\n",
    "    # A good heuristic is ~ 2x EE concurrency so client work can overlap fully\n",
    "    pool_workers = max(2 * max_concurrent, max_concurrent + 2)\n",
    "    print(f\"🔄 Running up to {max_concurrent} EE calls concurrently (pool size: {pool_workers})...\", flush=True)\n",
    "\n",
    "    results = []\n",
    "    progress_ok = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=pool_workers) as executor:\n",
    "        # For each batch, submit both server-side and client-side futures, then merge on completion\n",
    "        # We track a tuple of (server_future, client_future, batch_idx)\n",
    "        pair_futures = []\n",
    "        for i, batch in enumerate(batches):\n",
    "            server_fut = executor.submit(\n",
    "                _server_side_batch,\n",
    "                batch, i, whisp_image, reducer, max_retries,\n",
    "                add_metadata_ee, x_col, y_col, type_col\n",
    "            )\n",
    "            client_fut = executor.submit(\n",
    "                _client_side_metadata,\n",
    "                batch, row_id_col, x_col, y_col, type_col\n",
    "            )\n",
    "            pair_futures.append((server_fut, client_fut, i))\n",
    "\n",
    "        # Consume as they complete—but we must wait for both halves of each pair\n",
    "        for server_fut, client_fut, batch_idx in pair_futures:\n",
    "            try:\n",
    "                # Wait for both parts of this batch\n",
    "                server_df = server_fut.result()\n",
    "                client_df = client_fut.result()\n",
    "\n",
    "                # Ensure the join key exists in server_df.\n",
    "                # If whisp.convert_ee_to_df drops the property name, add a safeguard:\n",
    "                if row_id_col not in server_df.columns:\n",
    "                    # Try common fallbacks\n",
    "                    possible_keys = [row_id_col, 'row_id', 'id', 'plotId', 'plot_id']\n",
    "                    found_key = next((k for k in possible_keys if k in server_df.columns), None)\n",
    "                    if found_key and found_key != row_id_col:\n",
    "                        server_df = server_df.rename(columns={found_key: row_id_col})\n",
    "                    else:\n",
    "                        # If no key, inject a temporary index-aligned key (best-effort)\n",
    "                        server_df[row_id_col] = range(len(server_df))\n",
    "\n",
    "                # Merge: server results (left) with client metadata (right)\n",
    "                merged = server_df.merge(client_df, on=row_id_col, how='left', suffixes=('', '_client'))\n",
    "                results.append(merged)\n",
    "                progress_ok += 1\n",
    "                print(f\"⏳ Progress: Batch {batch_idx + 1} ✓\", flush=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Try to surface a helpful failure message but keep going\n",
    "                err = str(e)\n",
    "                print(f\"❌ Batch {batch_idx + 1} failed: {err[:120]}...\", flush=True)\n",
    "\n",
    "    if results:\n",
    "        combined_df = pd.concat(results, ignore_index=True)\n",
    "        print(f\"✅ Done. {progress_ok}/{len(batches)} batches completed.\", flush=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"❌ No results produced - all batches failed\", flush=True)\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose additional national datasets to include (currently three countries: 'co', 'ci', 'br').\n",
    "iso2_codes_list = ['co', 'ci', 'br']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_image = ee.Image(\"projects/ee-whisp/assets/admin_bounds/gaul_2024_level_1_code_500m\").rename(\"admin_code\")\n",
    "\n",
    "water_mask_image = ee.Image(\"projects/ee-whisp/assets/water_mask/water_mask_jrc_usgs\").rename(water_flag)\n",
    "\n",
    "try:\n",
    "    whisp_image = whisp.combine_datasets(national_codes=iso2_codes_list).addBands(admin_image).addBands(water_mask_image)\n",
    "    print(whisp_image.bandNames().getInfo())\n",
    "except:\n",
    "    whisp_image = whisp.combine_datasets(national_codes=iso2_codes_list,validate_bands=True).addBands(admin_image).addBands(water_mask_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = (r\"C:\\Users\\Arnell\\Downloads\\a_processing_tests\")  # Replace with your folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOJSON_EXAMPLE_FILEPATH = folder_path+\"/random_polygons.geojson\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Separate testing of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = (ee.FeatureCollection(\"projects/sat-io/open-datasets/FAO/GAUL/GAUL_2024_L1\")\n",
    "    .filter(ee.Filter.eq('gaul0_name', 'Austria')).geometry().bounds()\n",
    ")\n",
    "\n",
    "# Option 1: Use simple bounds (list)\n",
    "random_geojson = whisp.generate_test_polygons(\n",
    "    bounds=geom, \n",
    "    num_polygons=100, \n",
    "    min_area_ha=100, \n",
    "    max_area_ha=100, \n",
    "    min_number_vert=100,     \n",
    "    max_number_vert=100     \n",
    ")\n",
    "\n",
    "# GEOJSON_EXAMPLE_FILEPATH = folder_path + \"/random_polygons.geojson\"?\n",
    "print(GEOJSON_EXAMPLE_FILEPATH)\n",
    "import json\n",
    "# Save the GeoJSON to a file\n",
    "with open(GEOJSON_EXAMPLE_FILEPATH, 'w') as f:\n",
    "    json.dump(random_geojson, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOJSON_EXAMPLE_FILEPATH = whisp.get_example_data_path(\"geojson_example.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to convert a polygon GeoJSON to a point GeoJSON (centroids)\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "\n",
    "# def convert_polygon_geojson_to_point_geojson(input_geojson_path, output_geojson_path, id_column=None):\n",
    "#     \"\"\"\n",
    "#     Reads a polygon GeoJSON, computes centroids, and writes a new GeoJSON with point geometries.\n",
    "#     Optionally preserves an ID column.\n",
    "#     \"\"\"\n",
    "#     gdf = gpd.read_file(input_geojson_path)\n",
    "#     # Compute centroids\n",
    "#     gdf['geometry'] = gdf.geometry.centroid\n",
    "#     # Optionally keep only id and geometry\n",
    "#     if id_column and id_column in gdf.columns:\n",
    "#         gdf_out = gdf[[id_column, 'geometry']]\n",
    "#     else:\n",
    "#         gdf_out = gdf[['geometry']]\n",
    "#     # Save as GeoJSON\n",
    "#     gdf_out.to_file(output_geojson_path, driver='GeoJSON')\n",
    "#     print(f\"Saved point GeoJSON to: {output_geojson_path}\")\n",
    "\n",
    "# # Example usage:\n",
    "# # convert_polygon_geojson_to_point_geojson(GEOJSON_EXAMPLE_FILEPATH, folder_path+\"/random_points.geojson\", id_column=plot_id_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_polygon_geojson_to_point_geojson(\n",
    "#     GEOJSON_EXAMPLE_FILEPATH,\n",
    "#     folder_path + \"/random_polygons.geojson\",\n",
    "#     id_column=plot_id_column\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEOJSON_EXAMPLE_FILEPATH = folder_path+\"/RSPO-Concessions-Version-10-May-2025.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PER_EE_REQUEST = 10\n",
    "MAX_CONCURRENT_EE_REQUESTS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(whisp_image.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    result_df_raw = process_geojson_file(\n",
    "        geojson_path=GEOJSON_EXAMPLE_FILEPATH,\n",
    "        whisp_image=whisp_image,\n",
    "        reducer=combined_reducer,\n",
    "        batch_size=FEATURES_PER_EE_REQUEST,\n",
    "        max_concurrent=MAX_CONCURRENT_EE_REQUESTS,\n",
    "        validate_null_geometries=True,\n",
    "        validate_invalid_geometries=False,\n",
    "        add_metadata_gpd=False,\n",
    "        add_metadata_ee=False,\n",
    "        max_retries=3,\n",
    "        # ee_version=\"v1\"  # Add this if you implement versioning\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the correct column exists before joining admin codes\n",
    "# print(\"Columns in result_df:\", result_df.columns.tolist())\n",
    "\n",
    "# Use the correct column for admin code joining\n",
    "if \"admin_code_median\" in result_df_raw.columns:\n",
    "    id_col = \"admin_code_median\"\n",
    "    result_df_w_loc = join_admin_codes(df=result_df_raw, lookup_dict=lookup_dict, id_col=id_col)\n",
    "else:\n",
    "    print(\"Column 'admin_code_median' not found. Available columns:\", result_df_raw.columns.tolist())\n",
    "\n",
    "# Robustly add integer index to plot_id_column if missing or empty\n",
    "if plot_id_column not in result_df_w_loc.columns or result_df_w_loc[plot_id_column].isnull().all():\n",
    "    result_df_w_loc[plot_id_column] = range(len(result_df_w_loc))\n",
    "\n",
    "\n",
    "strip_suffix = \"_sum\"\n",
    "\n",
    "area_col = geometry_area_column+strip_suffix\n",
    "\n",
    "result_df_w_loc_formatted = format_stats_dataframe(df=result_df_w_loc, unit_type=\"percent\",area_col=area_col,strip_suffix=strip_suffix)\n",
    "\n",
    "\n",
    "df_stats = whisp.validate_dataframe_using_lookups_flexible(df_stats=result_df_w_loc_formatted,\n",
    "                                                           national_codes=iso2_codes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder \n",
    "# e.g. in running in Sepal this might be: Path.home() / 'module_results/whisp/'\n",
    "out_directory = Path.home() / 'downloads'\n",
    "\n",
    "# Define the output file path for CSV\n",
    "csv_output_file = out_directory / 'whisp_output_table.csv'\n",
    "\n",
    "# Save the CSV file\n",
    "df_stats.to_csv(path_or_buf=csv_output_file, index=False)\n",
    "print(f\"Table with stats saved to: {csv_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path for GeoJSON\n",
    "geojson_output_file = out_directory / 'whisp_output_geo.geojson'\n",
    "\n",
    "# Save the GeoJSON file\n",
    "whisp.convert_df_to_geojson(df_stats, geojson_output_file)  # builds a geojson file containing Whisp columns. Uses the geometry column \"geo\" to create the spatial features.\n",
    "print(f\"GeoJSON file saved to: {geojson_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds risk columns to end of dataframe\n",
    "df_w_risk = whisp.whisp_risk(\n",
    "    df=df_stats,\n",
    "    national_codes=iso2_codes_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Display table with risk columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Export table to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder \n",
    "# e.g. in running in Sepal this might be: Path.home() / 'module_results/whisp/'\n",
    "out_directory = Path.home() / 'downloads'\n",
    "\n",
    "# Define the output file path for CSV\n",
    "csv_output_file = out_directory / 'whisp_output_table_w_risk.csv'\n",
    "\n",
    "# Save the CSV file\n",
    "df_w_risk.to_csv(path_or_buf=csv_output_file, index=False)\n",
    "print(f\"Table with risk columns saved to: {csv_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Export to GeoJSON (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path for GeoJSON\n",
    "geojson_output_file = out_directory / 'whisp_output_geo_w_risk.geojson'\n",
    "\n",
    "# Save the GeoJSON file\n",
    "whisp.convert_df_to_geojson(df_w_risk, geojson_output_file)  # builds a geojson file containing Whisp columns. Uses the geometry column \"geo\" to create the spatial features.\n",
    "print(f\"GeoJSON file saved to: {geojson_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Classic Whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Engine and Common Libraries\n",
    "import ee\n",
    "from pathlib import Path\n",
    "\n",
    "# Authenticate and initialize Earth Engine with STANDARD endpoint\n",
    "# (The concurrent processing section above uses high-volume endpoint)\n",
    "try:\n",
    "    ee.Initialize()  # Standard endpoint (default)\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()  # Standard endpoint (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which endpoint is now active\n",
    "print(\"EE Data Base URL:\", ee.data._cloud_api_base_url)\n",
    "print(\"EE API Base URL:\", ee.data._api_base_url)\n",
    "\n",
    "# Check if using standard endpoint\n",
    "if 'highvolume' in str(ee.data._cloud_api_base_url):\n",
    "    print(\"❌ Still using HIGH-VOLUME endpoint\")\n",
    "else:\n",
    "    print(\"✅ Now using STANDARD endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### whisp = whisp.whisp_formatted_stats_geojson_to_df(GEOJSON_EXAMPLE_FILEPATH)\n",
    "# whisp = whisp.whisp_stats_geojson_to_df(GEOJSON_EXAMPLE_FILEPATH,whisp_image=whisp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n",
    "fc = whisp.convert_geojson_to_ee(GEOJSON_EXAMPLE_FILEPATH)\n",
    "# print(fc.size().getInfo())  # Print number of features in the collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisp_image = whisp.combine_datasets(national_codes=iso2_codes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reducer = ee.Reducer.sum().combine(ee.Reducer.median(),sharedInputs=True)\n",
    "results = whisp_image.reduceRegions(fc, reducer=combined_reducer, scale=10)\n",
    "whisp.convert_ee_to_df(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = whisp.whisp_formatted_stats_geojson_to_df(input_geojson_filepath=GEOJSON_EXAMPLE_FILEPATH,whisp_image=whisp_image,national_codes=iso2_codes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
