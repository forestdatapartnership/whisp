{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1b9eb8",
   "metadata": {},
   "source": [
    "#### Main changes\n",
    "- Using selfMask() to avoid empty pixels (a lot of bands are sparse)\n",
    "- Skipping the validation step – panderas should be fast but there is some temp schema generation that takes time \n",
    "- Using high volume end point and concurrent processing\n",
    "- Using reduceRegions instead of mapped reduceRegion - a chunk of code for choosing ha or percent etc is based on using reduceRegion and it also allowed to skip\n",
    "- Skipping the use of points to get the admin details (country and level 1 info) and water_flag (should be based on image but was using vector admin still)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c612a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Reset Earth Engine completely\n",
    "ee.Reset()\n",
    "\n",
    "# Initialize with standard (normal) endpoint\n",
    "# ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07803221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Engine and Common Libraries\n",
    "import ee\n",
    "from pathlib import Path\n",
    "\n",
    "# Authenticate and initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')  # Try to use existing credentials first\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f70561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --pre openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e43c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reducer = ee.Reducer.sum().combine(ee.Reducer.median(),sharedInputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which endpoint is now active\n",
    "print(\"EE Data Base URL:\", ee.data._cloud_api_base_url)\n",
    "print(\"EE API Base URL:\", ee.data._api_base_url)\n",
    "\n",
    "# Check if using standard endpoint\n",
    "if 'highvolume' in str(ee.data._cloud_api_base_url):\n",
    "    print(\"❌ Still using HIGH-VOLUME endpoint\")\n",
    "else:\n",
    "    print(\"✅ Now using STANDARD endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a889cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.validation import make_valid\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "def generate_random_polygon(\n",
    "    min_lon, min_lat, max_lon, max_lat, min_area_ha=1, max_area_ha=10, vertex_count=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Ultra-fast polygon generation using buffer method with EXACT vertex count control\n",
    "    \"\"\"\n",
    "    target_area_ha = random.uniform(min_area_ha, max_area_ha)\n",
    "    center_lon = random.uniform(min_lon, max_lon)\n",
    "    center_lat = random.uniform(min_lat, max_lat)\n",
    "    \n",
    "    # Estimate radius for target area\n",
    "    radius_degrees = math.sqrt(target_area_ha / math.pi) / 111.32 / 100\n",
    "    \n",
    "    # Create center point\n",
    "    center_point = Point(center_lon, center_lat)\n",
    "    \n",
    "    # METHOD 1: Use buffer with EXACT resolution to control vertices\n",
    "    # Buffer resolution directly controls vertex count\n",
    "    poly = center_point.buffer(radius_degrees, resolution=vertex_count//4)\n",
    "    \n",
    "    # METHOD 2: Manual vertex creation for EXACT count (more control)\n",
    "    if vertex_count > 50:  # For high vertex counts, create manually\n",
    "        # Generate exactly the requested number of vertices\n",
    "        angles = np.linspace(0, 2 * math.pi, vertex_count, endpoint=False)\n",
    "        \n",
    "        # Add controlled randomness to create natural variation\n",
    "        base_radius = radius_degrees\n",
    "        \n",
    "        # Smooth sine wave variations for natural look\n",
    "        freq1 = random.uniform(2, 5)\n",
    "        amp1 = random.uniform(0.08, 0.15)  # Small amplitude for smooth shapes\n",
    "        \n",
    "        freq2 = random.uniform(8, 15)\n",
    "        amp2 = random.uniform(0.03, 0.08)\n",
    "        \n",
    "        # Calculate radius variations\n",
    "        radius_variation = (amp1 * np.sin(freq1 * angles + random.uniform(0, 2*math.pi)) + \n",
    "                           amp2 * np.sin(freq2 * angles + random.uniform(0, 2*math.pi)))\n",
    "        \n",
    "        radii = base_radius * (1.0 + radius_variation)\n",
    "        radii = np.maximum(radii, base_radius * 0.6)  # Ensure reasonable minimum\n",
    "        \n",
    "        # Calculate coordinates\n",
    "        xs = center_lon + radii * np.cos(angles)\n",
    "        ys = center_lat + radii * np.sin(angles)\n",
    "        \n",
    "        # Clip to bounds\n",
    "        xs = np.clip(xs, min_lon, max_lon)\n",
    "        ys = np.clip(ys, min_lat, max_lat)\n",
    "        \n",
    "        # Create vertices list with EXACT count\n",
    "        vertices = list(zip(xs, ys))\n",
    "        vertices.append(vertices[0])  # Close polygon\n",
    "        \n",
    "        # Create polygon\n",
    "        poly = Polygon(vertices)\n",
    "        \n",
    "        # Validate\n",
    "        if not poly.is_valid:\n",
    "            poly = make_valid(poly)\n",
    "            if hasattr(poly, 'geoms'):\n",
    "                poly = max(poly.geoms, key=lambda p: p.area)\n",
    "    \n",
    "    else:\n",
    "        # For smaller vertex counts, add small perturbations to buffered circle\n",
    "        coords = list(poly.exterior.coords)\n",
    "        \n",
    "        # Resample to get exact vertex count\n",
    "        if len(coords) - 1 != vertex_count:  # -1 because last point = first point\n",
    "            # Create new vertices with exact count\n",
    "            angles = np.linspace(0, 2 * math.pi, vertex_count, endpoint=False)\n",
    "            \n",
    "            new_coords = []\n",
    "            for angle in angles:\n",
    "                # Base position on circle\n",
    "                x = center_lon + radius_degrees * math.cos(angle)\n",
    "                y = center_lat + radius_degrees * math.sin(angle)\n",
    "                \n",
    "                # Small random perturbation\n",
    "                dx = random.uniform(-radius_degrees * 0.08, radius_degrees * 0.08)\n",
    "                dy = random.uniform(-radius_degrees * 0.08, radius_degrees * 0.08)\n",
    "                \n",
    "                # Apply bounds\n",
    "                new_x = np.clip(x + dx, min_lon, max_lon)\n",
    "                new_y = np.clip(y + dy, min_lat, max_lat)\n",
    "                new_coords.append((new_x, new_y))\n",
    "            \n",
    "            # Close polygon\n",
    "            new_coords.append(new_coords[0])\n",
    "            poly = Polygon(new_coords)\n",
    "    \n",
    "    # Calculate area\n",
    "    actual_area_ha = poly.area * 111320 * 111320 * math.cos(math.radians(center_lat)) / 10000\n",
    "    \n",
    "    return poly, actual_area_ha\n",
    "\n",
    "\n",
    "def generate_properties(area_ha, index):\n",
    "    \"\"\"Generate properties for features with sequential internal_id\"\"\"\n",
    "    return {\n",
    "        \"internal_id\": index + 1,\n",
    "        \"vertex_count\": None,  # Will be set after polygon creation\n",
    "    }\n",
    "\n",
    "\n",
    "def create_geojson(\n",
    "    bounds,\n",
    "    num_polygons=25,\n",
    "    min_area_ha=1,\n",
    "    max_area_ha=10,\n",
    "    min_number_vert=10,\n",
    "    max_number_vert=20,\n",
    "):\n",
    "    \"\"\"Create a GeoJSON file with EXACT vertex count control\"\"\"\n",
    "    min_lon, min_lat, max_lon, max_lat = bounds\n",
    "\n",
    "    print(f\"🏗️  Generating {num_polygons} polygons with {min_number_vert}-{max_number_vert} vertices...\")\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Pre-generate ALL random values at once\n",
    "    vertex_counts = np.random.randint(min_number_vert, max_number_vert + 1, num_polygons)\n",
    "    center_lons = np.random.uniform(min_lon, max_lon, num_polygons)\n",
    "    center_lats = np.random.uniform(min_lat, max_lat, num_polygons)\n",
    "    target_areas = np.random.uniform(min_area_ha, max_area_ha, num_polygons)\n",
    "    \n",
    "    for i in range(num_polygons):\n",
    "        # Show progress for large batches\n",
    "        if i > 0 and i % 250 == 0:\n",
    "            print(f\"   Generated {i}/{num_polygons} polygons ({i/num_polygons*100:.0f}%)...\")\n",
    "        \n",
    "        # Use pre-calculated values\n",
    "        requested_vertices = vertex_counts[i]\n",
    "        \n",
    "        # Generate polygon with EXACT vertex count\n",
    "        polygon, actual_area = generate_random_polygon(\n",
    "            min_lon, min_lat, max_lon, max_lat,\n",
    "            min_area_ha=target_areas[i] * 0.9,\n",
    "            max_area_ha=target_areas[i] * 1.1,\n",
    "            vertex_count=requested_vertices\n",
    "        )\n",
    "        \n",
    "        # Verify vertex count (excluding closing vertex)\n",
    "        actual_vertex_count = len(list(polygon.exterior.coords)) - 1\n",
    "        \n",
    "        # Create GeoJSON feature with vertex count verification\n",
    "        properties = generate_properties(actual_area, index=i)\n",
    "        properties[\"requested_vertices\"] = int(requested_vertices)\n",
    "        properties[\"actual_vertices\"] = int(actual_vertex_count)\n",
    "        \n",
    "        feature = {\n",
    "            \"type\": \"Feature\", \n",
    "            \"properties\": properties,\n",
    "            \"geometry\": mapping(polygon),\n",
    "        }\n",
    "        \n",
    "        features.append(feature)\n",
    "\n",
    "    print(f\"✅ Generated {num_polygons} polygons!\")\n",
    "    \n",
    "    # Print vertex count summary\n",
    "    actual_counts = [f[\"properties\"][\"actual_vertices\"] for f in features]\n",
    "    requested_counts = [f[\"properties\"][\"requested_vertices\"] for f in features]\n",
    "    \n",
    "    print(f\"📊 Vertex count summary:\")\n",
    "    print(f\"   Requested: {min(requested_counts)}-{max(requested_counts)} vertices\")\n",
    "    print(f\"   Actual: {min(actual_counts)}-{max(actual_counts)} vertices\")\n",
    "    print(f\"   Average match: {sum(1 for i, j in zip(requested_counts, actual_counts) if abs(i-j) <= 2) / len(features) * 100:.1f}%\")\n",
    "    \n",
    "    geojson = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "    return geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whisp_image = whisp.combine_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91ebd320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openforis_whisp as whisp\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Simplified logging setup\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(\"whisp-batch\")\n",
    "\n",
    "# Optimized configuration for EE high-volume processing\n",
    "EE_MAX_CONCURRENT = 10\n",
    "EE_FEATURES_PER_BATCH = 25\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "\n",
    "class OptimizedWhispProcessor:\n",
    "    \"\"\"Optimized processor using whisp.convert_geojson_to_ee() with proper file handling\"\"\"\n",
    "    \n",
    "    def __init__(self, max_concurrent=EE_MAX_CONCURRENT, features_per_batch=EE_FEATURES_PER_BATCH):\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.features_per_batch = features_per_batch\n",
    "        self.semaphore = threading.Semaphore(max_concurrent)\n",
    "        self.results = {}\n",
    "        self.processing_stats = {'completed': 0, 'failed': 0, 'total': 0}\n",
    "        self.failed_batches = []\n",
    "        self.max_consecutive_failures = 3  # Stop if 3 batches fail in a row\n",
    "        \n",
    "    def process_file_optimized(self, geojson_path: str, national_codes: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Process file using whisp.convert_geojson_to_ee() with validation\"\"\"\n",
    "        \n",
    "        print(f\"🔍 Loading and validating GeoJSON file...\")\n",
    "        \n",
    "        # Load and validate the GeoDataFrame first\n",
    "        try:\n",
    "            gdf = gpd.read_file(geojson_path)\n",
    "            print(f\"📁 Loaded {len(gdf):,} features from {geojson_path}\")\n",
    "            \n",
    "            # Basic geometry validation\n",
    "            invalid_geoms = gdf.geometry.isna().sum()\n",
    "            if invalid_geoms > 0:\n",
    "                print(f\"⚠️  Found {invalid_geoms} null geometries - removing...\")\n",
    "                gdf = gdf[~gdf.geometry.isna()]\n",
    "                \n",
    "            # Check for valid geometries\n",
    "            valid_geoms = gdf.geometry.is_valid.sum()\n",
    "            invalid_geom_count = len(gdf) - valid_geoms\n",
    "            if invalid_geom_count > 0:\n",
    "                print(f\"⚠️  Found {invalid_geom_count} invalid geometries - fixing...\")\n",
    "                from shapely.validation import make_valid\n",
    "                gdf['geometry'] = gdf['geometry'].apply(lambda g: make_valid(g) if g and not g.is_valid else g)\n",
    "                \n",
    "            print(f\"✅ Validated {len(gdf):,} geometries\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load/validate GeoJSON: {e}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        total_features = len(gdf)\n",
    "        \n",
    "        # Split into feature batches\n",
    "        feature_batches = []\n",
    "        for i in range(0, total_features, self.features_per_batch):\n",
    "            batch = gdf.iloc[i:i+self.features_per_batch]\n",
    "            feature_batches.append(batch)\n",
    "        \n",
    "        total_batches = len(feature_batches)\n",
    "        print(f\"📊 Processing {total_features:,} features in {total_batches} batches ({self.features_per_batch} features/batch)\")\n",
    "        print(f\"🔄 Running {self.max_concurrent} concurrent requests...\")\n",
    "        print(f\"🛑 Will stop if {self.max_consecutive_failures} consecutive batches fail\")\n",
    "        \n",
    "        # Track progress and failures\n",
    "        completed_batches = 0\n",
    "        failed_batches = 0\n",
    "        consecutive_failures = 0\n",
    "        \n",
    "        # Process batches with early stopping \n",
    "    \n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:\n",
    "            print(f\"🚀 Submitting all {total_batches} batches concurrently...\")\n",
    "            \n",
    "            # Submit ALL batches at once for maximum concurrency\n",
    "            future_to_batch = {\n",
    "                executor.submit(self._process_feature_batch, batch, national_codes, i): i \n",
    "                for i, batch in enumerate(feature_batches)\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ All batches submitted - processing with {self.max_concurrent} concurrent workers...\")\n",
    "            \n",
    "            # Collect results with early stopping on consecutive failures\n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_idx = future_to_batch[future]\n",
    "                try:\n",
    "                    batch_result = future.result()\n",
    "                    results.append(batch_result)\n",
    "                    completed_batches += 1\n",
    "                    consecutive_failures = 0  # Reset failure counter on success\n",
    "                    \n",
    "                    # Show progress every 10 batches or at completion\n",
    "                    if completed_batches % 10 == 0 or completed_batches == total_batches - failed_batches:\n",
    "                        success_rate = completed_batches / (completed_batches + failed_batches) * 100 if (completed_batches + failed_batches) > 0 else 0\n",
    "                        print(f\"✅ Progress: {completed_batches}/{total_batches} batches completed ({success_rate:.1f}% success rate)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed_batches += 1\n",
    "                    consecutive_failures += 1\n",
    "                    error_msg = str(e)\n",
    "                    \n",
    "                    print(f\"❌ Batch {batch_idx + 1} failed: {error_msg[:80]}...\")\n",
    "                    \n",
    "                    # Early stopping only on excessive consecutive failures\n",
    "                    if consecutive_failures >= self.max_consecutive_failures:\n",
    "                        print(f\"🛑 STOPPING: {consecutive_failures} consecutive failures detected\")\n",
    "                        print(f\"💡 This suggests systematic issues - cancelling remaining batches\")\n",
    "                        \n",
    "                        # Cancel remaining futures to free up resources\n",
    "                        for remaining_future in future_to_batch:\n",
    "                            if not remaining_future.done():\n",
    "                                remaining_future.cancel()\n",
    "                        break\n",
    "                    \n",
    "                    self.processing_stats['failed'] += 1\n",
    "                # Final summary\n",
    "                if results:\n",
    "                    combined_df = pd.concat(results, ignore_index=True)\n",
    "                    success_rate = completed_batches / (completed_batches + failed_batches) * 100 if (completed_batches + failed_batches) > 0 else 0\n",
    "                    print(f\"🎉 Successfully processed {len(combined_df):,} features!\")\n",
    "                    print(f\"📈 Success rate: {success_rate:.1f}% ({completed_batches}/{completed_batches + failed_batches} batches)\")\n",
    "                    \n",
    "                    if failed_batches > 0:\n",
    "                        print(f\"⚠️  {failed_batches} batches failed\")\n",
    "                        \n",
    "                    return combined_df\n",
    "                else:\n",
    "                    print(\"❌ No results produced - all batches failed\")\n",
    "                    print(\"💡 Suggestions:\")\n",
    "                    print(\"   - Check if GeoJSON has valid geometries\")\n",
    "                    print(\"   - Try smaller batch sizes (FEATURES_PER_EE_REQUEST)\")\n",
    "                    print(\"   - Verify Earth Engine authentication\")\n",
    "                    print(\"   - Check if features are within valid coordinate ranges\")\n",
    "                    return pd.DataFrame()\n",
    "    \n",
    "    def _process_feature_batch(self, batch_gdf: gpd.GeoDataFrame, national_codes: Optional[List[str]], batch_idx: int) -> pd.DataFrame:\n",
    "        \"\"\"Process a single batch using whisp.convert_geojson_to_ee() with proper file handling\"\"\"\n",
    "        \n",
    "        with self.semaphore:\n",
    "            temp_geojson_path = None\n",
    "            try:\n",
    "                # Create temporary file with delete=False to handle manually\n",
    "                temp_fd, temp_geojson_path = tempfile.mkstemp(suffix='.geojson', text=True)\n",
    "                \n",
    "                try:\n",
    "                    # Close the file descriptor so Windows can access it\n",
    "                    os.close(temp_fd)\n",
    "                    \n",
    "                    # Save batch to temporary GeoJSON file\n",
    "                    batch_gdf.to_file(temp_geojson_path, driver='GeoJSON')\n",
    "                    \n",
    "                    # Use whisp to convert GeoJSON to EE FeatureCollection\n",
    "                    # This handles reprojection and validation automatically\n",
    "                    feature_collection = whisp.convert_geojson_to_ee(temp_geojson_path)\n",
    "                    \n",
    "                    # Process the FeatureCollection\n",
    "                    result_df = self._process_ee_feature_collection(feature_collection, national_codes, batch_idx)\n",
    "                    \n",
    "                    return result_df\n",
    "                    \n",
    "                except Exception as processing_error:\n",
    "                    raise processing_error\n",
    "                    \n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Batch {batch_idx + 1} processing failed: {str(e)}\")\n",
    "                \n",
    "            finally:\n",
    "                # Clean up temporary file with proper error handling\n",
    "                if temp_geojson_path and os.path.exists(temp_geojson_path):\n",
    "                    try:\n",
    "                        # Small delay to ensure file is released\n",
    "                        time.sleep(0.1)\n",
    "                        os.unlink(temp_geojson_path)\n",
    "                    except OSError as cleanup_error:\n",
    "                        # If we can't delete, log it but don't fail\n",
    "                        logger.warning(f\"Could not delete temp file {temp_geojson_path}: {cleanup_error}\")\n",
    "\n",
    "    def _process_ee_feature_collection(self, feature_collection: ee.FeatureCollection, \n",
    "                                 national_codes: Optional[List[str]], batch_idx: int) -> pd.DataFrame:\n",
    "        \"\"\"Process FeatureCollection with enhanced retry logic\"\"\"\n",
    "        \n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                # Use whisp_image.reduceRegions for processing\n",
    "                # results = whisp_image.reduceRegions(\n",
    "                #     collection=feature_collection,\n",
    "                #     reducer=combined_reducer,\n",
    "                #     scale=10\n",
    "                # )\n",
    "                # df_result = whisp.convert_ee_to_df(results)\n",
    "\n",
    "                results = whisp.whisp_stats_ee_to_df(\n",
    "                    feature_collection=feature_collection,\n",
    "                    # national_codes=national_codes,\n",
    "                    whisp_image=whisp_image\n",
    "                )\n",
    "                df_result = results\n",
    "\n",
    "           \n",
    "\n",
    "                return df_result\n",
    "                \n",
    "            except ee.EEException as e:\n",
    "                error_msg = str(e)\n",
    "                \n",
    "                # Check for specific geometry errors\n",
    "                if \"Unable to transform geometry\" in error_msg:\n",
    "                    raise Exception(f\"Geometry transformation error in batch {batch_idx + 1}: {error_msg}\")\n",
    "                elif \"Quota\" in error_msg or \"limit\" in error_msg.lower():\n",
    "                    if attempt < MAX_RETRIES - 1:\n",
    "                        backoff = min(30, 2 ** attempt)\n",
    "                        print(f\"⏳ Quota/rate limit hit, waiting {backoff}s before retry...\")\n",
    "                        time.sleep(backoff)\n",
    "                    else:\n",
    "                        raise Exception(f\"Quota/rate limit exhausted for batch {batch_idx + 1}\")\n",
    "                elif \"timeout\" in error_msg.lower():\n",
    "                    if attempt < MAX_RETRIES - 1:\n",
    "                        backoff = min(15, 2 ** attempt)\n",
    "                        print(f\"⏳ Timeout, retrying in {backoff}s...\")\n",
    "                        time.sleep(backoff)\n",
    "                    else:\n",
    "                        raise e\n",
    "                else:\n",
    "                    if attempt < MAX_RETRIES - 1:\n",
    "                        backoff = min(10, 2 ** attempt)\n",
    "                        time.sleep(backoff)\n",
    "                    else:\n",
    "                        raise e\n",
    "                        \n",
    "            except Exception as e:\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    backoff = min(5, 2 ** attempt)\n",
    "                    time.sleep(backoff)\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        raise RuntimeError(f\"Failed to process batch {batch_idx + 1} after {MAX_RETRIES} attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d7248bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openforis-whisp\n",
      "Version: 2.0.0b1\n",
      "Summary: Whisp (What is in that plot) is an open-source solution which helps to produce relevant forest monitoring information and support compliance with deforestation-related regulations.\n",
      "Home-page: \n",
      "Author: Andy Arnell\n",
      "Author-email: andrew.arnell@fao.org\n",
      "License: MIT\n",
      "Location: c:\\Users\\Arnell\\Documents\\GitHub\\whisp\\.venv\\Lib\\site-packages\n",
      "Editable project location: C:\\Users\\Arnell\\Documents\\GitHub\\whisp\n",
      "Requires: country_converter, earthengine-api, geojson, geopandas, ipykernel, numpy, pandas, pandera, pydantic-core, python-dotenv, rsa, shapely\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a13ee8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = (r\"C:\\Users\\Arnell\\Downloads\\a_processing_tests\")  # Replace with your folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab25a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOJSON_EXAMPLE_FILEPATH = folder_path+\"/random_polygons.geojson\"\n",
    "\n",
    "# Define bounds from the provided Earth Engine geometry\n",
    "# # area in Ghana \n",
    "# bounds = [ \n",
    "#     -3.04548260909834,  # min_lon\n",
    "#     5.253961384163733,  # min_lat\n",
    "#     -1.0179939534016594,  # max_lon\n",
    "#     7.48307210714245    # max_lat\n",
    "# ]\n",
    "\n",
    "# # area in China\n",
    "# bounds = [\n",
    "#     90.44831497309737,  # min_lon\n",
    "#     20.686366665187148,  # min_lat\n",
    "#     114.57868606684737,  # max_lon\n",
    "#     30.79200348254393    # max_lat\n",
    "# ]\n",
    "\n",
    "# Brazil etc\n",
    "bounds = [-81.06002305884182,\n",
    "        -19.332462745930076,\n",
    "        -31.48971055884182,\n",
    "         9.600139384904205\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb89a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️  Generating 1000 polygons with 90-100 vertices...\n",
      "   Generated 250/1000 polygons (25%)...\n",
      "   Generated 500/1000 polygons (50%)...\n",
      "   Generated 750/1000 polygons (75%)...\n",
      "✅ Generated 1000 polygons!\n",
      "📊 Vertex count summary:\n",
      "   Requested: 90-100 vertices\n",
      "   Actual: 90-100 vertices\n",
      "   Average match: 100.0%\n",
      "C:\\Users\\Arnell\\Downloads\\a_processing_tests/random_polygons.geojson\n"
     ]
    }
   ],
   "source": [
    "# random_geojson = whisp.create_geojson(\n",
    "random_geojson = create_geojson(\n",
    "    bounds, \n",
    "    num_polygons=1000, \n",
    "    min_area_ha=5, \n",
    "    max_area_ha=10, \n",
    "    min_number_vert=90, \n",
    "    max_number_vert=100)\n",
    "\n",
    "GEOJSON_EXAMPLE_FILEPATH = folder_path + \"/random_polygons.geojson\"\n",
    "print(GEOJSON_EXAMPLE_FILEPATH)\n",
    "import json\n",
    "# Save the GeoJSON to a file\n",
    "with open(GEOJSON_EXAMPLE_FILEPATH, 'w') as f:\n",
    "    json.dump(random_geojson, f)\n",
    "\n",
    "# Use example Whisp inputs (optional)\n",
    "# GEOJSON_EXAMPLE_FILEPATH = whisp.get_example_data_path(\"geojson_example.geojson\")\n",
    "\n",
    "\n",
    "# Add IDs to your existing GeoJSON file\n",
    "\n",
    "# #Save to a new file (instead of overwriting)\n",
    "# # whisp.reformat_geojson_properties(\n",
    "# whisp.reformat_geojson_properties(\n",
    "    \n",
    "#     geojson_path=GEOJSON_EXAMPLE_FILEPATH, \n",
    "#     id_field=\"internal_id\",\n",
    "#     output_path=folder_path + \"/random_polygons_with_ids.geojson\",\n",
    "#     remove_properties=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96c56521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEOJSON_EXAMPLE_FILEPATH = folder_path+\"/RSPO-Concessions-Version-10-May-2025.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "add125db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:23:52,659 - INFO - Processing with 50 features per Earth Engine request\n",
      "2025-10-14 22:23:52,660 - INFO - Maximum 20 concurrent requests\n",
      "2025-10-14 22:23:53,317 - INFO - Created 50 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Loading and validating GeoJSON file...\n",
      "📁 Loaded 1,000 features from C:\\Users\\Arnell\\Downloads\\a_processing_tests/random_polygons.geojson\n",
      "✅ Validated 1,000 geometries\n",
      "📊 Processing 1,000 features in 20 batches (50 features/batch)\n",
      "🔄 Running 20 concurrent requests...\n",
      "🛑 Will stop if 3 consecutive batches fail\n",
      "🚀 Submitting all 20 batches concurrently...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:23:53,393 - INFO - Created 50 records\n",
      "2025-10-14 22:23:53,541 - INFO - Created 50 records\n",
      "2025-10-14 22:23:53,592 - INFO - Created 50 records\n",
      "2025-10-14 22:23:53,660 - INFO - Created 50 records\n",
      "2025-10-14 22:23:53,708 - INFO - Created 50 records\n",
      "2025-10-14 22:23:53,739 - INFO - Created 50 records\n",
      "2025-10-14 22:23:53,885 - INFO - Created 50 records\n",
      "2025-10-14 22:23:53,885 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,036 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,036 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,143 - INFO - Created 50 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All batches submitted - processing with 20 concurrent workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:23:54,238 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,287 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,302 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,403 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,544 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,568 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,766 - INFO - Created 50 records\n",
      "2025-10-14 22:23:54,975 - INFO - Created 50 records\n",
      "2025-10-14 22:24:23,144 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:23,247 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:23,341 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:23,632 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:23,728 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:23,838 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:23,943 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,080 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,130 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,340 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,396 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,437 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,497 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,590 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,690 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,827 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:24,930 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,047 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,139 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,267 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,315 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,691 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,781 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,821 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,863 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,927 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:25,979 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:26,120 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:26,577 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:26,719 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:27,299 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:27,558 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:27,805 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:28,219 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:28,359 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:28,468 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:28,764 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:29,024 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:29,369 - WARNING - Unknown not found in regex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Successfully processed 50 features!\n",
      "📈 Success rate: 100.0% (1/1 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 22:24:30,524 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:30,865 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:31,012 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:31,269 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:31,813 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:31,942 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:32,119 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:32,352 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:32,859 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:33,000 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:33,165 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:35,412 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:35,620 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:36,468 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:37,007 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:37,182 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:37,519 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:37,938 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:38,254 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:38,571 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:39,224 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:39,427 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:39,921 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:39,954 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:40,278 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:40,764 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:41,969 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:42,114 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:42,436 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:43,018 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:43,205 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:43,206 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:43,526 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:44,225 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:44,377 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:44,745 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:44,988 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:44,991 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:45,596 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:45,917 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:45,959 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:46,231 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:46,702 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:47,500 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:47,686 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:48,176 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-14 22:24:48,283 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:48,563 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:48,626 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:49,427 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:49,527 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:50,577 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:50,597 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:50,753 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:50,853 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:50,909 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:52,406 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:54,175 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:54,388 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:54,735 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:54,752 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:54,885 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:54,885 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:55,434 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:55,667 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:56,051 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:56,134 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:56,194 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:56,455 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,132 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,185 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,351 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,586 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,634 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,797 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,870 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,889 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:57,901 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:58,234 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:58,424 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:58,434 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:58,801 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:58,834 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:59,134 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:59,432 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:59,630 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:59,634 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:59,920 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:24:59,938 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:00,018 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:00,137 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:00,567 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:00,627 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:00,726 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:00,904 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:00,967 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:01,069 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:01,206 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:01,550 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:01,761 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:01,784 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:01,934 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:02,056 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:02,118 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:02,324 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:02,350 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:02,851 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:03,137 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:03,172 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:03,274 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:03,369 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:03,587 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:03,650 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:03,863 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:03,905 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:04,001 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:04,167 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:04,233 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:04,636 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:04,750 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:05,734 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:05,801 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:05,822 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:06,001 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:06,104 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:06,158 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:06,451 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:06,551 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:06,684 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:06,684 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:06,837 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:07,667 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:07,867 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:07,968 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,023 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,034 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,224 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,228 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,270 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,484 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,551 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,615 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:08,755 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:09,037 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:09,054 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:09,301 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:09,550 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:09,825 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:09,882 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:10,139 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:10,217 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:10,417 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:10,734 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:10,772 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:10,939 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:11,254 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:11,653 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:11,674 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:11,734 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:11,884 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:12,138 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:12,238 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:12,253 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:12,487 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:12,619 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:12,754 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:12,906 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:12,950 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:13,028 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:13,633 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:13,667 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:14,100 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:14,533 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:14,550 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:15,134 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:15,167 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:15,529 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:15,826 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:15,871 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:16,000 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:16,470 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:16,767 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:16,950 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:17,337 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:17,382 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:17,534 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:17,684 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:17,783 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:17,783 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:17,833 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:17,900 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:18,287 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:18,754 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:19,085 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:19,154 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:19,301 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:19,487 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:19,534 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:19,667 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:19,735 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:20,404 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:20,454 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:20,653 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:20,755 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:20,793 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:20,834 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:20,957 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:21,033 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:21,155 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:21,382 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:21,600 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:21,634 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:22,233 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:22,400 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:22,567 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:22,724 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:22,990 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:23,325 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:23,455 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:23,475 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:23,522 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:23,816 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:24,370 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:24,434 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:24,554 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:24,803 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:24,842 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:24,857 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:24,857 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:24,999 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:25,000 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:25,078 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:25,159 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:25,271 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:25,527 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:25,654 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:25,748 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:25,855 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:26,057 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:26,505 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:26,738 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:26,830 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:26,839 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:27,008 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:27,234 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:27,390 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:27,690 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:27,750 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:27,990 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:28,300 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:28,500 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:28,500 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:28,654 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:28,870 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:29,299 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:29,400 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:30,070 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:30,199 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:30,277 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:30,833 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:31,092 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:31,237 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:31,483 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:31,995 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:32,100 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:32,148 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:32,529 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:32,607 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:33,150 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:33,239 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:33,488 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:33,604 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:33,737 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:33,989 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:34,063 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:34,323 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:34,428 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:34,624 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:34,886 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:35,033 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:35,137 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:35,267 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:35,383 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:35,553 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:36,100 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:36,283 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:36,322 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:36,603 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:36,839 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:36,937 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:37,133 - WARNING - Unknown not found in regex\n",
      "2025-10-14 22:25:37,354 - INFO - Results saved to optimized_whisp_results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Processed 50 features\n",
      "\n",
      "First 5 rows:\n",
      "                                                 geo Admin_Level_1   Area  \\\n",
      "0  {'type': 'Polygon', 'coordinates': [[[-43.8103...         Piaui  0.058   \n",
      "1  {'type': 'Polygon', 'coordinates': [[[-33.2148...       Unknown  0.074   \n",
      "2  {'type': 'Polygon', 'coordinates': [[[-66.9193...       Bolívar  0.077   \n",
      "3  {'type': 'Polygon', 'coordinates': [[[-79.9223...       Unknown  0.099   \n",
      "4  {'type': 'Polygon', 'coordinates': [[[-53.3352...         Amapa  0.073   \n",
      "\n",
      "   Centroid_lat  Centroid_lon  Cocoa_2023_FDaP  Cocoa_ETH  Cocoa_FDaP  \\\n",
      "0     -9.784622    -43.810237                0          0           0   \n",
      "1     -2.236516    -33.214678                0          0           0   \n",
      "2      6.993892    -66.919179                0          0           0   \n",
      "3    -14.041860    -79.922174                0          0           0   \n",
      "4      1.946154    -53.335117                0          0           0   \n",
      "\n",
      "   Coffee_FDaP  Coffee_FDaP_2023  ... TMF_deg_2023  TMF_deg_2024  \\\n",
      "0            0                 0  ...            0             0   \n",
      "1            0                 0  ...            0             0   \n",
      "2            0                 0  ...            0             0   \n",
      "3            0                 0  ...            0             0   \n",
      "4            0                 0  ...            0             0   \n",
      "\n",
      "   TMF_deg_after_2020  TMF_deg_before_2020  TMF_plant  TMF_regrowth_2023  \\\n",
      "0                   0                  0.0          0                  0   \n",
      "1                   0                  0.0          0                  0   \n",
      "2                   0                  0.0          0                  0   \n",
      "3                   0                  0.0          0                  0   \n",
      "4                   0                  0.0          0                  0   \n",
      "\n",
      "   TMF_undist  Unit  plotId  ProducerCountry  \n",
      "0       0.000    ha       1               BR  \n",
      "1       0.000    ha       2        not found  \n",
      "2       0.000    ha       3               VE  \n",
      "3       0.000    ha       4        not found  \n",
      "4       0.073    ha       5               BR  \n",
      "\n",
      "[5 rows x 180 columns]\n",
      "Processing stats: {'completed': 0, 'failed': 0, 'total': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage with controlled batch sizes\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Configure batch size based on your data characteristics\n",
    "    FEATURES_PER_EE_REQUEST = 50  # Small batches for complex geometries\n",
    "    MAX_CONCURRENT_EE_REQUESTS = 20  # Conservative for quota management\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = OptimizedWhispProcessor(\n",
    "    \n",
    "        max_concurrent=MAX_CONCURRENT_EE_REQUESTS,\n",
    "        features_per_batch=FEATURES_PER_EE_REQUEST\n",
    "    )\n",
    "    \n",
    "    # Process file with controlled batching\n",
    "    try:\n",
    "        # GEOJSON_EXAMPLE_FILEPATH = whisp.get_example_data_path(\"geojson_example.geojson\")\n",
    "        \n",
    "        logger.info(f\"Processing with {FEATURES_PER_EE_REQUEST} features per Earth Engine request\")\n",
    "        logger.info(f\"Maximum {MAX_CONCURRENT_EE_REQUESTS} concurrent requests\")\n",
    "        \n",
    "        result_df = processor.process_file_optimized(\n",
    "            GEOJSON_EXAMPLE_FILEPATH, \n",
    "            # national_codes=[\"br\", \"co\"]\n",
    "        )\n",
    "        \n",
    "        if not result_df.empty:\n",
    "            print(f\"Success! Processed {len(result_df)} features\")\n",
    "            print(\"\\nFirst 5 rows:\")\n",
    "            print(result_df.head())\n",
    "            \n",
    "            # Save results\n",
    "            result_df.to_csv(Path.home() / \"downloads\"/ \"optimized_whisp_results.csv\", index=False)\n",
    "            logger.info(\"Results saved to optimized_whisp_results.csv\")\n",
    "        else:\n",
    "            print(\"No results produced\")\n",
    "            \n",
    "        print(f\"Processing stats: {processor.processing_stats}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df  # Display first few rows of combined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df_no_geo = result_df.drop(columns=['geo'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba90f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_no_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_from_ee = pd.read_csv(Path.home() / 'downloads' / \"whisp_RSPO_Concessions_May_2025_output_table_w_risk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_from_ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701a776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder \n",
    "# e.g. in running in Sepal this might be: Path.home() / 'module_results/whisp/'\n",
    "out_directory = Path.home() / 'downloads'\n",
    "\n",
    "# Define the output file path for CSV\n",
    "csv_output_file = out_directory / 'whisp_output_table.csv'\n",
    "\n",
    "# Save the CSV file\n",
    "result_df.to_csv(path_or_buf=csv_output_file, index=False)\n",
    "print(f\"Table with risk columns saved to: {csv_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path for GeoJSON\n",
    "geojson_output_file = out_directory / 'whisp_output_geo.geojson'\n",
    "\n",
    "# Save the GeoJSON file\n",
    "whisp.convert_df_to_geojson(result_df, geojson_output_file)  # builds a geojson file containing Whisp columns. Uses the geometry column \"geo\" to create the spatial features.\n",
    "print(f\"GeoJSON file saved to: {geojson_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953a1c1",
   "metadata": {},
   "source": [
    "Classic Whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Engine and Common Libraries\n",
    "import ee\n",
    "from pathlib import Path\n",
    "\n",
    "# Authenticate and initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize()  # Try to use existing credentials first\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2330e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a02a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### whisp = whisp.whisp_formatted_stats_geojson_to_df(GEOJSON_EXAMPLE_FILEPATH)\n",
    "# whisp = whisp.whisp_stats_geojson_to_df(GEOJSON_EXAMPLE_FILEPATH,whisp_image=whisp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n",
    "fc = whisp.convert_geojson_to_ee(GEOJSON_EXAMPLE_FILEPATH)\n",
    "print(fc.size().getInfo())  # Print number of features in the collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ab07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisp_image = whisp.combine_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_reducer = ee.Reducer.sum().combine(ee.Reducer.median(),sharedInputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0697a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = whisp_image.reduceRegions(fc.limit(10), reducer=combined_reducer, scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efecacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisp.convert_ee_to_df(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aea4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
