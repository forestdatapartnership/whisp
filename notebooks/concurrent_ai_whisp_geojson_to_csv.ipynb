{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c612a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Reset Earth Engine completely\n",
    "ee.Reset()\n",
    "\n",
    "# Initialize with standard (normal) endpoint\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07803221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Engine and Common Libraries\n",
    "import ee\n",
    "from pathlib import Path\n",
    "\n",
    "# Authenticate and initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')  # Try to use existing credentials first\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f70561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --pre openforis-whisp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fa5fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EE Data Base URL: https://earthengine-highvolume.googleapis.com\n",
      "EE API Base URL: https://earthengine-highvolume.googleapis.com/api\n",
      "‚ùå Still using HIGH-VOLUME endpoint\n"
     ]
    }
   ],
   "source": [
    "# Check which endpoint is now active\n",
    "print(\"EE Data Base URL:\", ee.data._cloud_api_base_url)\n",
    "print(\"EE API Base URL:\", ee.data._api_base_url)\n",
    "\n",
    "# Check if using standard endpoint\n",
    "if 'highvolume' in str(ee.data._cloud_api_base_url):\n",
    "    print(\"‚ùå Still using HIGH-VOLUME endpoint\")\n",
    "else:\n",
    "    print(\"‚úÖ Now using STANDARD endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22bd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import geopandas as gpd  # for random polygon generation in tests\n",
    "import random  # for random polygon generation in tests\n",
    "import math  # for random polygon generation in tests\n",
    "import numpy as np  # for random polygon generation in tests\n",
    "from shapely.geometry import Polygon  # for random polygon generation in tests\n",
    "from shapely.validation import make_valid\n",
    "from shapely.geometry import mapping  # for random polygon generation in tests\n",
    "\n",
    "def generate_random_polygon(\n",
    "    min_lon, min_lat, max_lon, max_lat, min_area_ha=1, max_area_ha=10, vertex_count=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a random polygon within bounds with approximate area in the specified range.\n",
    "    Uses a robust approach that works well with high vertex counts and never falls back to squares.\n",
    "\n",
    "    Args:\n",
    "        min_lon, min_lat, max_lon, max_lat: Boundary coordinates\n",
    "        min_area_ha: Minimum area in hectares\n",
    "        max_area_ha: Maximum area in hectares\n",
    "        vertex_count: Number of vertices for the polygon\n",
    "    \"\"\"\n",
    "    # Initialize variables to ensure they're always defined\n",
    "    poly = None\n",
    "    actual_area_ha = 0\n",
    "\n",
    "    # Simple function to approximate area in hectares (much faster)\n",
    "    def approximate_area_ha(polygon, center_lat):\n",
    "        # Get area in square degrees\n",
    "        area_sq_degrees = polygon.area\n",
    "\n",
    "        # Approximate conversion factor from square degrees to hectares\n",
    "        # This varies with latitude due to the Earth's curvature\n",
    "        lat_factor = 111320  # meters per degree latitude (approximately)\n",
    "        lon_factor = 111320 * math.cos(\n",
    "            math.radians(center_lat)\n",
    "        )  # meters per degree longitude\n",
    "\n",
    "        # Convert to square meters, then to hectares (1 ha = 10,000 sq m)\n",
    "        return area_sq_degrees * lat_factor * lon_factor / 10000\n",
    "\n",
    "    # Target area in hectares\n",
    "    target_area_ha = random.uniform(min_area_ha, max_area_ha)\n",
    "\n",
    "    # Select a center point within the bounds\n",
    "    center_lon = random.uniform(min_lon, max_lon)\n",
    "    center_lat = random.uniform(min_lat, max_lat)\n",
    "\n",
    "    # Initial size estimate (in degrees)\n",
    "    # Rough approximation: 0.01 degrees ~ 1km at equator\n",
    "    initial_radius = math.sqrt(target_area_ha / (math.pi * 100)) * 0.01\n",
    "\n",
    "    # Avoid generating too many points initially - cap vertex count for stability\n",
    "    effective_vertex_count = min(\n",
    "        vertex_count, 100\n",
    "    )  # Cap at 100 to avoid performance issues\n",
    "\n",
    "    # Primary approach: Create polygon using convex hull approach\n",
    "    for attempt in range(5):  # First method gets 5 attempts\n",
    "        try:\n",
    "            # Generate random points in a circle around center with varying distance\n",
    "            thetas = np.linspace(0, 2 * math.pi, effective_vertex_count, endpoint=False)\n",
    "\n",
    "            # Add randomness to angles - smaller randomness for higher vertex counts\n",
    "            angle_randomness = min(0.2, 2.0 / effective_vertex_count)\n",
    "            thetas += np.random.uniform(\n",
    "                -angle_randomness, angle_randomness, size=effective_vertex_count\n",
    "            )\n",
    "\n",
    "            # Randomize distances from center - less extreme for high vertex counts\n",
    "            distance_factor = min(0.3, 3.0 / effective_vertex_count) + 0.7\n",
    "            distances = initial_radius * np.random.uniform(\n",
    "                1.0 - distance_factor / 2,\n",
    "                1.0 + distance_factor / 2,\n",
    "                size=effective_vertex_count,\n",
    "            )\n",
    "\n",
    "            # Convert to cartesian coordinates\n",
    "            xs = center_lon + distances * np.cos(thetas)\n",
    "            ys = center_lat + distances * np.sin(thetas)\n",
    "\n",
    "            # Ensure points are within bounds\n",
    "            xs = np.clip(xs, min_lon, max_lon)\n",
    "            ys = np.clip(ys, min_lat, max_lat)\n",
    "\n",
    "            # Create vertices list\n",
    "            vertices = list(zip(xs, ys))\n",
    "\n",
    "            # Close the polygon\n",
    "            if vertices[0] != vertices[-1]:\n",
    "                vertices.append(vertices[0])\n",
    "\n",
    "            # Create polygon\n",
    "            poly = Polygon(vertices)\n",
    "\n",
    "            # Ensure it's valid\n",
    "            if not poly.is_valid:\n",
    "                poly = make_valid(poly)\n",
    "                if poly.geom_type != \"Polygon\":\n",
    "                    # If not a valid polygon, we'll try again\n",
    "                    continue\n",
    "\n",
    "            # Calculate approximate area\n",
    "            actual_area_ha = approximate_area_ha(poly, center_lat)\n",
    "\n",
    "            # Check if within target range\n",
    "            if min_area_ha * 0.8 <= actual_area_ha <= max_area_ha * 1.2:\n",
    "                return poly, actual_area_ha\n",
    "\n",
    "            # Adjust size for next attempt based on ratio\n",
    "            if actual_area_ha > 0:  # Avoid division by zero\n",
    "                scale_factor = math.sqrt(target_area_ha / actual_area_ha)\n",
    "                initial_radius *= scale_factor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in convex hull method (attempt {attempt+1}): {e}\")\n",
    "\n",
    "    # Second approach: Star-like pattern with controlled randomness\n",
    "    # This is a fallback that will still create an irregular polygon, not a square\n",
    "    for attempt in range(5):  # Second method gets 5 attempts\n",
    "        try:\n",
    "            # Use fewer vertices for stability in the fallback\n",
    "            star_vertex_count = min(15, vertex_count)\n",
    "            vertices = []\n",
    "\n",
    "            # Create a star-like pattern with two radiuses\n",
    "            for i in range(star_vertex_count):\n",
    "                angle = 2 * math.pi * i / star_vertex_count\n",
    "\n",
    "                # Alternate between two distances to create star-like shape\n",
    "                if i % 2 == 0:\n",
    "                    distance = initial_radius * random.uniform(0.7, 0.9)\n",
    "                else:\n",
    "                    distance = initial_radius * random.uniform(0.5, 0.6)\n",
    "\n",
    "                # Add some irregularity to angles\n",
    "                angle += random.uniform(-0.1, 0.1)\n",
    "\n",
    "                # Calculate vertex position\n",
    "                lon = center_lon + distance * math.cos(angle)\n",
    "                lat = center_lat + distance * math.sin(angle)\n",
    "\n",
    "                # Ensure within bounds\n",
    "                lon = min(max(lon, min_lon), max_lon)\n",
    "                lat = min(max(lat, min_lat), max_lat)\n",
    "\n",
    "                vertices.append((lon, lat))\n",
    "\n",
    "            # Close the polygon\n",
    "            vertices.append(vertices[0])\n",
    "\n",
    "            # Create polygon\n",
    "            poly = Polygon(vertices)\n",
    "            if not poly.is_valid:\n",
    "                poly = make_valid(poly)\n",
    "                if poly.geom_type != \"Polygon\":\n",
    "                    continue\n",
    "\n",
    "            actual_area_ha = approximate_area_ha(poly, center_lat)\n",
    "\n",
    "            # We're less picky about size at this point, just return it\n",
    "            if actual_area_ha > 0:\n",
    "                return poly, actual_area_ha\n",
    "\n",
    "            # Still try to adjust if we get another attempt\n",
    "            if actual_area_ha > 0:\n",
    "                scale_factor = math.sqrt(target_area_ha / actual_area_ha)\n",
    "                initial_radius *= scale_factor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in star pattern method (attempt {attempt+1}): {e}\")\n",
    "\n",
    "    # Last resort - create a perturbed circle (never a square)\n",
    "    try:\n",
    "        # Create a circle-like shape with small perturbations\n",
    "        final_vertices = []\n",
    "        perturbed_vertex_count = 8  # Use a modest number for stability\n",
    "\n",
    "        for i in range(perturbed_vertex_count):\n",
    "            angle = 2 * math.pi * i / perturbed_vertex_count\n",
    "            # Small perturbation\n",
    "            distance = initial_radius * random.uniform(0.95, 1.05)\n",
    "\n",
    "            # Calculate vertex position\n",
    "            lon = center_lon + distance * math.cos(angle)\n",
    "            lat = center_lat + distance * math.sin(angle)\n",
    "\n",
    "            # Ensure within bounds\n",
    "            lon = min(max(lon, min_lon), max_lon)\n",
    "            lat = min(max(lat, min_lat), max_lat)\n",
    "\n",
    "            final_vertices.append((lon, lat))\n",
    "\n",
    "        # Close the polygon\n",
    "        final_vertices.append(final_vertices[0])\n",
    "\n",
    "        # Create polygon\n",
    "        poly = Polygon(final_vertices)\n",
    "        if not poly.is_valid:\n",
    "            poly = make_valid(poly)\n",
    "\n",
    "        actual_area_ha = approximate_area_ha(poly, center_lat)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in final fallback method: {e}\")\n",
    "        # If absolutely everything fails, create the simplest valid polygon (triangle)\n",
    "        # This is different from a square and should be more compatible with your code\n",
    "        offset = initial_radius / 2\n",
    "        poly = Polygon(\n",
    "            [\n",
    "                (center_lon, center_lat + offset),\n",
    "                (center_lon + offset, center_lat - offset),\n",
    "                (center_lon - offset, center_lat - offset),\n",
    "                (center_lon, center_lat + offset),\n",
    "            ]\n",
    "        )\n",
    "        actual_area_ha = approximate_area_ha(poly, center_lat)\n",
    "\n",
    "    # Return whatever we've created - never a simple square\n",
    "    return poly, actual_area_ha\n",
    "\n",
    "\n",
    "def generate_properties(area_ha, index):\n",
    "    \"\"\"\n",
    "    Generate properties for features with sequential internal_id\n",
    "\n",
    "    Args:\n",
    "        area_ha: Area in hectares of the polygon\n",
    "        index: Index of the feature to use for sequential ID\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"internal_id\": index + 1,  # Create sequential IDs starting from 1\n",
    "    }\n",
    "\n",
    "\n",
    "def create_geojson(\n",
    "    bounds,\n",
    "    num_polygons=25,\n",
    "    min_area_ha=1,\n",
    "    max_area_ha=10,\n",
    "    min_number_vert=10,\n",
    "    max_number_vert=20,\n",
    "):\n",
    "    \"\"\"Create a GeoJSON file with random polygons within area range\"\"\"\n",
    "    min_lon, min_lat, max_lon, max_lat = bounds\n",
    "    # min_number_vert = 15\n",
    "    # max_number_vert = 20\n",
    "\n",
    "    features = []\n",
    "    for i in range(num_polygons):\n",
    "        # Random vertex count between 4 and 8\n",
    "        # vertices = random.randint(4, 8)\n",
    "        vertices = random.randint(min_number_vert, max_number_vert)\n",
    "\n",
    "        # Generate polygon with area control\n",
    "        polygon, actual_area = generate_random_polygon(\n",
    "            min_lon,\n",
    "            min_lat,\n",
    "            max_lon,\n",
    "            max_lat,\n",
    "            min_area_ha=min_area_ha,\n",
    "            max_area_ha=max_area_ha,\n",
    "            vertex_count=vertices,\n",
    "        )\n",
    "\n",
    "        # Create GeoJSON feature with actual area\n",
    "        properties = generate_properties(actual_area, index=i)\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": properties,\n",
    "            \"geometry\": mapping(polygon),\n",
    "        }\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    # Create the GeoJSON feature collection\n",
    "    geojson = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "    return geojson\n",
    "\n",
    "\n",
    "def reformat_geojson_properties(\n",
    "    geojson_path,\n",
    "    output_path=None,\n",
    "    id_field=\"internal_id\",\n",
    "    start_index=1,\n",
    "    remove_properties=False,\n",
    "    add_uuid=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add numeric IDs to features in an existing GeoJSON file and optionally remove properties.\n",
    "\n",
    "    Args:\n",
    "        geojson_path: Path to input GeoJSON file\n",
    "        output_path: Path to save the output GeoJSON (if None, overwrites input)\n",
    "        id_field: Name of the ID field to add\n",
    "        start_index: Starting index for sequential IDs\n",
    "        remove_properties: Whether to remove all existing properties (default: False)\n",
    "        add_uuid: Whether to also add UUID field\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame with updated features\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the GeoJSON\n",
    "    # print(f\"Reading GeoJSON file: {geojson_path}\")\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "    # Remove existing properties if requested\n",
    "    if remove_properties:\n",
    "        # Keep only the geometry column and drop all other columns\n",
    "        gdf = gdf[[\"geometry\"]].copy()\n",
    "        # print(f\"Removed all existing properties from features\")\n",
    "\n",
    "    # Add sequential numeric IDs\n",
    "    gdf[id_field] = [i + start_index for i in range(len(gdf))]\n",
    "\n",
    "    # Optionally add UUIDs\n",
    "    if add_uuid:\n",
    "        gdf[\"uuid\"] = [str(uuid.uuid4()) for _ in range(len(gdf))]\n",
    "\n",
    "    # Write the GeoJSON with added IDs\n",
    "    output_path = output_path or geojson_path\n",
    "    gdf.to_file(output_path, driver=\"GeoJSON\")\n",
    "    print(f\"Added {id_field} to GeoJSON and saved to {output_path}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ac47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e4d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisp multiband image compiled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "whisp_image = whisp.combine_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8aa3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b1285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Simplified logging setup\n",
    "logging.basicConfig(level=logging.WARNING)  # Reduce default verbosity\n",
    "logger = logging.getLogger(\"whisp-batch\")\n",
    "\n",
    "# # defaults\n",
    "# Optimized configuration for EE high-volume processing\n",
    "EE_MAX_CONCURRENT = 10\n",
    "EE_FEATURES_PER_BATCH = 50  # Features per Earth Engine request\n",
    "MAX_RETRIES = 3\n",
    "THREAD_POOL_SIZE = 4\n",
    "\n",
    "class OptimizedWhispProcessor:\n",
    "    \"\"\"Optimized processor using Earth Engine high-volume patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, max_concurrent=EE_MAX_CONCURRENT, features_per_batch=EE_FEATURES_PER_BATCH):\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.features_per_batch = features_per_batch\n",
    "        self.semaphore = threading.Semaphore(max_concurrent)\n",
    "        self.results = {}\n",
    "        self.processing_stats = {'completed': 0, 'failed': 0, 'total': 0}\n",
    "        \n",
    "    def process_file_optimized(self, geojson_path: str, national_codes: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Process file using optimized Earth Engine batching\"\"\"\n",
    "        \n",
    "        # Load the GeoDataFrame\n",
    "        gdf = gpd.read_file(geojson_path)\n",
    "        total_features = len(gdf)\n",
    "        \n",
    "        # Split into feature batches\n",
    "        feature_batches = []\n",
    "        for i in range(0, total_features, self.features_per_batch):\n",
    "            batch = gdf.iloc[i:i+self.features_per_batch]\n",
    "            feature_batches.append(batch)\n",
    "        \n",
    "        total_batches = len(feature_batches)\n",
    "        print(f\"üìä Processing {total_features:,} features in {total_batches} batches ({self.features_per_batch} features/batch)\")\n",
    "        print(f\"üîÑ Running {self.max_concurrent} concurrent requests...\")\n",
    "        \n",
    "        # Track progress\n",
    "        completed_batches = 0\n",
    "        failed_batches = 0\n",
    "        \n",
    "        # Process batches concurrently using ThreadPoolExecutor\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:\n",
    "            # Submit all batches\n",
    "            future_to_batch = {\n",
    "                executor.submit(self._process_feature_batch, batch, national_codes, i): i \n",
    "                for i, batch in enumerate(feature_batches)\n",
    "            }\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_idx = future_to_batch[future]\n",
    "                try:\n",
    "                    batch_result = future.result()\n",
    "                    results.append(batch_result)\n",
    "                    completed_batches += 1\n",
    "                    \n",
    "                    # Show progress every 10 batches or at completion\n",
    "                    if completed_batches % 10 == 0 or completed_batches == total_batches:\n",
    "                        print(f\"‚úÖ Completed: {completed_batches}/{total_batches} batches ({completed_batches/total_batches*100:.0f}%)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed_batches += 1\n",
    "                    print(f\"‚ùå Batch {batch_idx + 1} failed: {str(e)[:50]}...\")\n",
    "                    self.processing_stats['failed'] += 1\n",
    "        \n",
    "        # Final summary\n",
    "        if results:\n",
    "            combined_df = pd.concat(results, ignore_index=True)\n",
    "            print(f\"üéâ Successfully processed {len(combined_df):,} features!\")\n",
    "            if failed_batches > 0:\n",
    "                print(f\"‚ö†Ô∏è  {failed_batches} batches failed\")\n",
    "            return combined_df\n",
    "        else:\n",
    "            print(\"‚ùå No results produced\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _process_feature_batch(self, batch_gdf: gpd.GeoDataFrame, national_codes: Optional[List[str]], batch_idx: int) -> pd.DataFrame:\n",
    "        \"\"\"Process a single batch of features using Earth Engine\"\"\"\n",
    "        \n",
    "        with self.semaphore:  # Limit concurrent EE requests\n",
    "            # Convert GeoDataFrame to Earth Engine FeatureCollection directly\n",
    "            features_list = []\n",
    "            \n",
    "            for idx, row in batch_gdf.iterrows():\n",
    "                # Convert each feature to EE format\n",
    "                geometry = row.geometry\n",
    "                properties = {k: v for k, v in row.items() if k != 'geometry' and pd.notna(v)}\n",
    "                \n",
    "                # Convert shapely geometry to EE geometry\n",
    "                if geometry is not None:\n",
    "                    ee_geometry = ee.Geometry(geometry.__geo_interface__)\n",
    "                    ee_feature = ee.Feature(ee_geometry, properties)\n",
    "                    features_list.append(ee_feature)\n",
    "            \n",
    "            # Create FeatureCollection from features\n",
    "            feature_collection = ee.FeatureCollection(features_list)\n",
    "            \n",
    "            # Process using optimized Earth Engine parameters\n",
    "            return self._process_ee_feature_collection(feature_collection, national_codes, batch_idx)\n",
    "\n",
    "    def _process_ee_feature_collection(self, feature_collection: ee.FeatureCollection, \n",
    "                                 national_codes: Optional[List[str]], batch_idx: int) -> pd.DataFrame:\n",
    "        \"\"\"Process FeatureCollection using Earth Engine with retry logic\"\"\"\n",
    "        \n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                # Use whisp's existing function that handles all conversions\n",
    "                df_result = whisp.whisp_stats_ee_to_df(\n",
    "                    feature_collection=feature_collection,\n",
    "                    external_id_column=None,\n",
    "                    remove_geom=False,\n",
    "                    national_codes=national_codes,\n",
    "                    unit_type=\"ha\",\n",
    "                    whisp_image=whisp_image,\n",
    "                )\n",
    "                \n",
    "                return df_result\n",
    "                \n",
    "            except ee.EEException as e:\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    backoff = min(2 ** attempt, 30)\n",
    "                    time.sleep(backoff)\n",
    "                else:\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        raise RuntimeError(f\"Failed to process batch {batch_idx + 1} after {MAX_RETRIES} attempts\")\n",
    "\n",
    "# # Example usage with controlled batch sizes\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     # Configure batch size based on your data characteristics\n",
    "#     FEATURES_PER_EE_REQUEST = 25  # Small batches for complex geometries\n",
    "#     MAX_CONCURRENT_EE_REQUESTS = 20  # Conservative for quota management\n",
    "    \n",
    "#     # Initialize processor\n",
    "#     processor = OptimizedWhispProcessor(\n",
    "#         max_concurrent=MAX_CONCURRENT_EE_REQUESTS,\n",
    "#         features_per_batch=FEATURES_PER_EE_REQUEST\n",
    "#     )\n",
    "    \n",
    "#     # Process file with controlled batching\n",
    "#     try:\n",
    "#         result_df = processor.process_file_optimized(\n",
    "#             GEOJSON_EXAMPLE_FILEPATH, \n",
    "#             national_codes=[\"br\", \"co\"]\n",
    "#         )\n",
    "        \n",
    "#         if not result_df.empty:\n",
    "#             print(f\"\\nüìÑ Saving results to CSV...\")\n",
    "#             result_df.to_csv(\"optimized_whisp_results.csv\", index=False)\n",
    "#             print(f\"üíæ Results saved to optimized_whisp_results.csv\")\n",
    "            \n",
    "#             print(f\"\\nüìã First 5 rows:\")\n",
    "#             print(result_df.head())\n",
    "#         else:\n",
    "#             print(\"‚ùå No results to save\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"üí• Processing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7248bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openforis-whisp\n",
      "Version: 2.0.0b1\n",
      "Summary: Whisp (What is in that plot) is an open-source solution which helps to produce relevant forest monitoring information and support compliance with deforestation-related regulations.\n",
      "Home-page: \n",
      "Author: Andy Arnell\n",
      "Author-email: andrew.arnell@fao.org\n",
      "License: MIT\n",
      "Location: c:\\Users\\Arnell\\Documents\\GitHub\\whisp\\.venv\\Lib\\site-packages\n",
      "Editable project location: C:\\Users\\Arnell\\Documents\\GitHub\\whisp\n",
      "Requires: country_converter, earthengine-api, geojson, geopandas, ipykernel, numpy, pandas, pandera, pydantic-core, python-dotenv, rsa, shapely\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13ee8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = (r\"C:\\Users\\Arnell\\Downloads\\processing_tests\")  # Replace with your folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab25a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOJSON_EXAMPLE_FILEPATH = folder_path+\"/random_polygons.geojson\"\n",
    "\n",
    "# Define bounds from the provided Earth Engine geometry\n",
    "# # area in Ghana \n",
    "# bounds = [ \n",
    "#     -3.04548260909834,  # min_lon\n",
    "#     5.253961384163733,  # min_lat\n",
    "#     -1.0179939534016594,  # max_lon\n",
    "#     7.48307210714245    # max_lat\n",
    "# ]\n",
    "\n",
    "# area in China\n",
    "bounds = [\n",
    "    103.44831497309737,  # min_lon\n",
    "    25.686366665187148,  # min_lat\n",
    "    109.57868606684737,  # max_lon\n",
    "    28.79200348254393    # max_lat\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb89a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_geojson = whisp.create_geojson(\n",
    "random_geojson = create_geojson(\n",
    "    bounds, \n",
    "    num_polygons=500, \n",
    "    min_area_ha=5, \n",
    "    max_area_ha=10, \n",
    "    min_number_vert=90, \n",
    "    max_number_vert=100)\n",
    "\n",
    "GEOJSON_EXAMPLE_FILEPATH = folder_path + \"/random_polygons.geojson\"\n",
    "\n",
    "import json\n",
    "# Save the GeoJSON to a file\n",
    "with open(GEOJSON_EXAMPLE_FILEPATH, 'w') as f:\n",
    "    json.dump(random_geojson, f)\n",
    "\n",
    "# Use example Whisp inputs (optional)\n",
    "# GEOJSON_EXAMPLE_FILEPATH = whisp.get_example_data_path(\"geojson_example.geojson\")\n",
    "\n",
    "\n",
    "# Add IDs to your existing GeoJSON file\n",
    "\n",
    "# #Save to a new file (instead of overwriting)\n",
    "# # whisp.reformat_geojson_properties(\n",
    "# whisp.reformat_geojson_properties(\n",
    "    \n",
    "#     geojson_path=GEOJSON_EXAMPLE_FILEPATH, \n",
    "#     id_field=\"internal_id\",\n",
    "#     output_path=folder_path + \"/random_polygons_with_ids.geojson\",\n",
    "#     remove_properties=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "add125db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 10:47:41,423 - INFO - Processing with 25 features per Earth Engine request\n",
      "2025-10-12 10:47:41,424 - INFO - Maximum 20 concurrent requests\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Processing 500 features in 20 batches (25 features/batch)\n",
      "üîÑ Running 20 concurrent requests...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 10:47:52,280 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-12 10:47:52,318 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-12 10:47:52,333 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-12 10:47:52,578 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-12 10:47:52,849 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-12 10:47:53,129 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-12 10:47:53,129 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-12 10:47:53,713 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n",
      "2025-10-12 10:47:53,869 - WARNING - Connection pool is full, discarding connection: earthengine-highvolume.googleapis.com. Connection pool size: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: 10/20 batches (50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 10:48:11,982 - INFO - Results saved to optimized_whisp_results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: 20/20 batches (100%)\n",
      "üéâ Successfully processed 500 features!\n",
      "Success! Processed 500 features\n",
      "\n",
      "First 5 rows:\n",
      "                                                 geo     Admin_Level_1   Area  \\\n",
      "0  {'type': 'Polygon', 'coordinates': [[[104.1493...   Yunnan Province  9.842   \n",
      "1  {'type': 'Polygon', 'coordinates': [[[103.5543...   Yunnan Province  7.038   \n",
      "2  {'type': 'Polygon', 'coordinates': [[[108.4779...  Guizhou Province  5.881   \n",
      "3  {'type': 'Polygon', 'coordinates': [[[103.7600...   Yunnan Province  8.254   \n",
      "4  {'type': 'Polygon', 'coordinates': [[[107.7283...  Guizhou Province  6.436   \n",
      "\n",
      "   Centroid_lat  Centroid_lon  Cocoa_2023_FDaP  Cocoa_ETH  Cocoa_FDaP  \\\n",
      "0     26.224426    104.151540                0          0           0   \n",
      "1     27.029586    103.556195                0          0           0   \n",
      "2     27.984908    108.479651                0          0           0   \n",
      "3     26.781161    103.762100                0          0           0   \n",
      "4     27.514257    107.730136                0          0           0   \n",
      "\n",
      "   Coffee_FDaP  Coffee_FDaP_2023  ... TMF_deg_2023  TMF_deg_2024  \\\n",
      "0            0                 0  ...            0             0   \n",
      "1            0                 0  ...            0             0   \n",
      "2            0                 0  ...            0             0   \n",
      "3            0                 0  ...            0             0   \n",
      "4            0                 0  ...            0             0   \n",
      "\n",
      "   TMF_deg_after_2020  TMF_deg_before_2020  TMF_plant  TMF_regrowth_2023  \\\n",
      "0                   0                    0          0                  0   \n",
      "1                   0                    0          0                  0   \n",
      "2                   0                    0          0                  0   \n",
      "3                   0                    0          0                  0   \n",
      "4                   0                    0          0                  0   \n",
      "\n",
      "   TMF_undist  Unit  plotId  ProducerCountry  \n",
      "0           0    ha       1               CN  \n",
      "1           0    ha       2               CN  \n",
      "2           0    ha       3               CN  \n",
      "3           0    ha       4               CN  \n",
      "4           0    ha       5               CN  \n",
      "\n",
      "[5 rows x 180 columns]\n",
      "Processing stats: {'completed': 0, 'failed': 0, 'total': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage with controlled batch sizes\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Configure batch size based on your data characteristics\n",
    "    FEATURES_PER_EE_REQUEST = 25  # Small batches for complex geometries\n",
    "    MAX_CONCURRENT_EE_REQUESTS = 20  # Conservative for quota management\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = OptimizedWhispProcessor(\n",
    "        max_concurrent=MAX_CONCURRENT_EE_REQUESTS,\n",
    "        features_per_batch=FEATURES_PER_EE_REQUEST\n",
    "    )\n",
    "    \n",
    "    # Process file with controlled batching\n",
    "    try:\n",
    "        # GEOJSON_EXAMPLE_FILEPATH = whisp.get_example_data_path(\"geojson_example.geojson\")\n",
    "        \n",
    "        logger.info(f\"Processing with {FEATURES_PER_EE_REQUEST} features per Earth Engine request\")\n",
    "        logger.info(f\"Maximum {MAX_CONCURRENT_EE_REQUESTS} concurrent requests\")\n",
    "        \n",
    "        result_df = processor.process_file_optimized(\n",
    "            GEOJSON_EXAMPLE_FILEPATH, \n",
    "            # national_codes=[\"br\", \"co\"]\n",
    "        )\n",
    "        \n",
    "        if not result_df.empty:\n",
    "            print(f\"Success! Processed {len(result_df)} features\")\n",
    "            print(\"\\nFirst 5 rows:\")\n",
    "            print(result_df.head())\n",
    "            \n",
    "            # Save results\n",
    "            result_df.to_csv(\"optimized_whisp_results.csv\", index=False)\n",
    "            logger.info(\"Results saved to optimized_whisp_results.csv\")\n",
    "        else:\n",
    "            print(\"No results produced\")\n",
    "            \n",
    "        print(f\"Processing stats: {processor.processing_stats}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af1bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "284e741d-2a11-462f-9a12-ce4dbad28af5",
       "rows": [],
       "shape": {
        "columns": 0,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df  # Display first few rows of combined results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953a1c1",
   "metadata": {},
   "source": [
    "Classic Whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b44e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Engine and Common Libraries\n",
    "import ee\n",
    "from pathlib import Path\n",
    "\n",
    "# Authenticate and initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize()  # Try to use existing credentials first\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad3772b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Switched to STANDARD Earth Engine endpoint\n",
      "EE Data Base URL: https://earthengine.googleapis.com\n",
      "EE API Base URL: https://earthengine.googleapis.com/api\n",
      "Using STANDARD endpoint\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# Reset Earth Engine completely\n",
    "ee.Reset()\n",
    "\n",
    "# Initialize with standard (normal) endpoint\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"‚úÖ Switched to STANDARD Earth Engine endpoint\")\n",
    "\n",
    "# Check the data module's base URL\n",
    "print(\"EE Data Base URL:\", ee.data._cloud_api_base_url)\n",
    "print(\"EE API Base URL:\", ee.data._api_base_url)\n",
    "\n",
    "# Check if high-volume endpoint is configured\n",
    "if 'highvolume' in str(ee.data._cloud_api_base_url):\n",
    "    print(\"Using HIGH-VOLUME endpoint\")\n",
    "else:\n",
    "    print(\"Using STANDARD endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2330e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openforis_whisp as whisp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6a02a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openforis-whisp\n",
      "Version: 2.0.0b1\n",
      "Summary: Whisp (What is in that plot) is an open-source solution which helps to produce relevant forest monitoring information and support compliance with deforestation-related regulations.\n",
      "Home-page: \n",
      "Author: Andy Arnell\n",
      "Author-email: andrew.arnell@fao.org\n",
      "License: MIT\n",
      "Location: c:\\Users\\Arnell\\Documents\\GitHub\\whisp\\.venv\\Lib\\site-packages\n",
      "Editable project location: C:\\Users\\Arnell\\Documents\\GitHub\\whisp\n",
      "Requires: country_converter, earthengine-api, geojson, geopandas, ipykernel, numpy, pandas, pandera, pydantic-core, python-dotenv, rsa, shapely\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show openforis-whisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42efebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading GeoJSON file from: C:\\Users\\Arnell\\Downloads\\processing_tests\\random_polygons.geojson\n"
     ]
    }
   ],
   "source": [
    "# whisp = whisp.whisp_formatted_stats_geojson_to_df(GEOJSON_EXAMPLE_FILEPATH)\n",
    "whisp = whisp.whisp_stats_geojson_to_df(GEOJSON_EXAMPLE_FILEPATH,whisp_image=whisp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e58b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efecacfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b28cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c36dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
